[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/PTB_1.hdf5	Classifier:	nn	Alpha:	1	Eta:	100	Lambda:	1	Minibatch size:	128	Num Epochs:	20	Optimizer:	adagrad	Hidden Layers:	10	Embedding size:	50	K:	10	
Sample dist	1	10001	
nclasses:	10001	nfeatures:	10001	d_win:	1	
Full valid size	 73760
     1
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.01513353115727	4.020439977199	
L1 norm of params:	415783.9351777	
Loss: 	9.2384258943798	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.55578635014837	2.0856579317797	
L1 norm of params:	709601.59602122	
Loss: 	7.5464863362431	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.56142433234421	2.077449331217	
L1 norm of params:	619001.63681929	
Loss: 	8.1906191790561	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.56587537091988	2.5020710807699	
L1 norm of params:	747503.69584503	
Loss: 	10.329965378473	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.56290801186944	2.6644154511564	
L1 norm of params:	926956.1165012	
Loss: 	11.255234906109	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.56320474777448	2.94103275749	
L1 norm of params:	1026077.8530061	
Loss: 	12.115809764963	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.5646884272997	3.2266250859632	
L1 norm of params:	1129269.0992946	
Loss: 	13.025329500563	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.56261127596439	3.4459123412224	
L1 norm of params:	1287644.4619181	
Loss: 	13.660916146922	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.56231454005935	3.5673776295289	
L1 norm of params:	1496421.6644902	
Loss: 	14.017229367796	
