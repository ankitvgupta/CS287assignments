[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/SMALL_5.hdf5	Classifier:	nn	Alpha:	1	Eta:	100	Lambda:	1	Minibatch size:	1024	Num Epochs:	20	Optimizer:	adagrad	Hidden Layers:	10	Embedding size:	100	K:	10	
Sample dist	1	10001	
nclasses:	1000	nfeatures:	1000	d_win:	5	
Full valid size	 73760
     5
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.18961424332344	3.993210420809	
L1 norm of params:	81707.494212776	
Loss: 	6.8971081508006	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.2486646884273	6.845775995631	
L1 norm of params:	85596.292687442	
Loss: 	4.4632053482325	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.28902077151335	6.609839469715	
L1 norm of params:	88449.937859497	
Loss: 	4.326041441099	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.31750741839763	6.3902464219824	
L1 norm of params:	89647.833815821	
Loss: 	4.2412954411292	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.33056379821958	6.3085481169462	
L1 norm of params:	90823.634437957	
Loss: 	4.2522427703448	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.32729970326409	6.3026634250043	
L1 norm of params:	91633.134999117	
Loss: 	4.1788464826774	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.32195845697329	6.3871480692007	
L1 norm of params:	91345.629047812	
Loss: 	4.1543954489081	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.32700296735905	6.4253911435107	
L1 norm of params:	90384.77770029	
Loss: 	4.2263240578695	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.34985163204748	6.2108562754614	
L1 norm of params:	92459.789355285	
Loss: 	4.1414179991391	
