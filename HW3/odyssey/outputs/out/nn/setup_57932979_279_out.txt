[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/SMALL_2.hdf5	Classifier:	nn	Alpha:	1	Eta:	0.01	Lambda:	1	Minibatch size:	32	Num Epochs:	20	Optimizer:	sgd	Hidden Layers:	100	Embedding size:	100	K:	10	
Sample dist	1	10001	
nclasses:	1000	nfeatures:	1000	d_win:	2	
Full valid size	 73760
     2
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.15697329376855	3.8225295407036	
L1 norm of params:	85520.678550562	
Loss: 	7.3854746620036	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.27537091988131	6.7843980959345	
L1 norm of params:	85583.978646359	
Loss: 	2.2045665928396	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.28308605341246	6.7103034925952	
L1 norm of params:	85623.016783833	
Loss: 	2.1929929498026	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.28813056379822	6.6254508617213	
L1 norm of params:	85669.933460105	
Loss: 	2.1759596444792	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.29287833827893	6.5627040948829	
L1 norm of params:	85722.781558161	
Loss: 	2.1450919193883	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.29762611275964	6.513774950682	
L1 norm of params:	85778.519879778	
Loss: 	2.1164550285217	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.3026706231454	6.4757858354325	
L1 norm of params:	85835.775663989	
Loss: 	2.0907852284283	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.30682492581602	6.4448733969388	
L1 norm of params:	85893.334465691	
Loss: 	2.0715687520011	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.30949554896142	6.4153023555011	
L1 norm of params:	85950.830543678	
Loss: 	2.0613492603112	
