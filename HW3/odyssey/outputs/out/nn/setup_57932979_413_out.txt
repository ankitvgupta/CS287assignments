[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/SMALL_2.hdf5	Classifier:	nn	Alpha:	1	Eta:	100	Lambda:	1	Minibatch size:	128	Num Epochs:	20	Optimizer:	adagrad	Hidden Layers:	100	Embedding size:	50	K:	10	
Sample dist	1	10001	
nclasses:	1000	nfeatures:	1000	d_win:	2	
Full valid size	 73760
     2
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.1566765578635	3.9214450105529	
L1 norm of params:	45808.405908509	
Loss: 	6.9392754980985	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.29287833827893	6.6676404807261	
L1 norm of params:	90674.765966691	
Loss: 	3.7401357573571	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.29228486646884	7.9860097222608	
L1 norm of params:	81804.955658433	
Loss: 	4.5299385353355	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.29881305637982	8.414781639449	
L1 norm of params:	111783.22358046	
Loss: 	4.9377958542348	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.29881305637982	8.9462969966181	
L1 norm of params:	148385.81033451	
Loss: 	5.3902292945104	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.30296735905045	9.4973724464874	
L1 norm of params:	178056.06521282	
Loss: 	5.8652033416713	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.30623145400593	9.9663248565851	
L1 norm of params:	206468.55549004	
Loss: 	6.2591943768091	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.30178041543027	10.276884788748	
L1 norm of params:	243619.68946935	
Loss: 	6.4916065441211	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.29970326409496	10.499977540185	
L1 norm of params:	302687.97747598	
Loss: 	6.5979851583176	
