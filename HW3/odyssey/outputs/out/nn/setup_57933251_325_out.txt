[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/PTB_2.hdf5	Classifier:	nn	Alpha:	1	Eta:	1	Lambda:	1	Minibatch size:	32	Num Epochs:	20	Optimizer:	sgd	Hidden Layers:	10	Embedding size:	25	K:	10	
Sample dist	1	10001	
nclasses:	10001	nfeatures:	10001	d_win:	2	
Full valid size	 73760
     2
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.018694362017804	3.9557723895495	
L1 norm of params:	217181.46405916	
Loss: 	9.3287207559968	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.38219584569733	2.8130291045617	
L1 norm of params:	217189.71495597	
Loss: 	8.9780637711163	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.44896142433234	2.5087916388706	
L1 norm of params:	217202.43839687	
Loss: 	9.2066275617909	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.47833827893175	2.3665574328858	
L1 norm of params:	217218.70281238	
Loss: 	9.2807410794676	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.49792284866469	2.2819686107405	
L1 norm of params:	217234.58933436	
Loss: 	9.3318694425031	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.51186943620178	2.2216540406489	
L1 norm of params:	217248.38524568	
Loss: 	9.3738335108064	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.5213649851632	2.1744004383661	
L1 norm of params:	217261.21421427	
Loss: 	9.4114089566655	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.53026706231454	2.1364546844695	
L1 norm of params:	217273.63152645	
Loss: 	9.4435124581975	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.53323442136499	2.1044644398008	
L1 norm of params:	217285.72713252	
Loss: 	9.4748476560155	
