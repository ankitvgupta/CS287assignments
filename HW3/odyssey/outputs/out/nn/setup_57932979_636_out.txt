[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/SMALL_5.hdf5	Classifier:	nn	Alpha:	1	Eta:	100	Lambda:	1	Minibatch size:	1024	Num Epochs:	20	Optimizer:	sgd	Hidden Layers:	50	Embedding size:	100	K:	10	
Sample dist	1	10001	
nclasses:	1000	nfeatures:	1000	d_win:	5	
Full valid size	 73760
     5
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.17388724035608	3.9396132309917	
L1 norm of params:	83762.404593084	
Loss: 	6.964180907942	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.24243323442136	6.4176736339088	
L1 norm of params:	83766.550648214	
Loss: 	5.6343082563277	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.24688427299703	7.1125069473356	
L1 norm of params:	83770.649507302	
Loss: 	5.2512059593164	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.24777448071217	7.1127515009048	
L1 norm of params:	83774.531374276	
Loss: 	5.0459578941569	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.24777448071217	7.0869577654459	
L1 norm of params:	83778.725356567	
Loss: 	4.8966753850037	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.24896142433234	7.0581258000613	
L1 norm of params:	83782.679491275	
Loss: 	4.7908213839112	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.24955489614243	7.0376936953901	
L1 norm of params:	83786.216154911	
Loss: 	4.7171684729593	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.25044510385757	7.0243379752461	
L1 norm of params:	83789.319148508	
Loss: 	4.665981159331	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.25163204747774	7.0140804582238	
L1 norm of params:	83792.025547598	
Loss: 	4.6284120266972	
