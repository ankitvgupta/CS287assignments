[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/PTB_1.hdf5	Classifier:	nn	Alpha:	1	Eta:	0.001	Lambda:	1	Minibatch size:	1024	Num Epochs:	20	Optimizer:	sgd	Hidden Layers:	10	Embedding size:	100	K:	10	
Sample dist	1	10001	
nclasses:	10001	nfeatures:	10001	d_win:	1	
Full valid size	 73760
     1
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.02106824925816	3.9713588079706	
L1 norm of params:	816348.25522539	
Loss: 	9.2736296893517	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.027299703264095	3.9392754463215	
L1 norm of params:	816346.85704029	
Loss: 	9.2437287657862	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.037982195845697	3.9107928316964	
L1 norm of params:	816345.82426891	
Loss: 	9.2166672335438	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.057566765578635	3.88419480494	
L1 norm of params:	816344.99094387	
Loss: 	9.1908594311583	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.071810089020772	3.8589868627379	
L1 norm of params:	816344.36738795	
Loss: 	9.1656924267968	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.083976261127596	3.8340431979035	
L1 norm of params:	816343.84508954	
Loss: 	9.1403233317242	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.098516320474777	3.8093145964974	
L1 norm of params:	816343.40113979	
Loss: 	9.1149225187572	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.10771513353116	3.7837779716853	
L1 norm of params:	816343.04581631	
Loss: 	9.0880314440309	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.12225519287834	3.7573184916731	
L1 norm of params:	816342.80969981	
Loss: 	9.0596845309048	
