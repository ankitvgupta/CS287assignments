[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/PTB_5.hdf5	Classifier:	nn	Alpha:	1	Eta:	0.001	Lambda:	1	Minibatch size:	32	Num Epochs:	20	Optimizer:	sgd	Hidden Layers:	50	Embedding size:	25	K:	10	
Sample dist	1	10001	
nclasses:	10001	nfeatures:	10001	d_win:	5	
Full valid size	 73760
     5
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.016913946587537	3.9715308513519	
L1 norm of params:	235702.6622063	
Loss: 	9.2696455684255	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.45341246290801	2.5210238378995	
L1 norm of params:	235739.78753294	
Loss: 	8.9237208368798	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.51572700296736	2.1994742513154	
L1 norm of params:	235760.27254094	
Loss: 	9.108237173324	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.54272997032641	2.0704094501242	
L1 norm of params:	235785.01473409	
Loss: 	9.2141856036099	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.55163204747774	1.9934897654966	
L1 norm of params:	235814.61929049	
Loss: 	9.2727938590801	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.55816023738872	1.9394635770923	
L1 norm of params:	235847.2746536	
Loss: 	9.3225654611806	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.5620178041543	1.8987311329563	
L1 norm of params:	235881.99131386	
Loss: 	9.3726928849827	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.56646884272997	1.8665140627277	
L1 norm of params:	235917.36670286	
Loss: 	9.4233114793604	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.56943620178042	1.8402628851951	
L1 norm of params:	235952.83001466	
Loss: 	9.4710188489688	
