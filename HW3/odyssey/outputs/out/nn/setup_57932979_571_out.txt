[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW3/SMALL_5.hdf5	Classifier:	nn	Alpha:	1	Eta:	1	Lambda:	1	Minibatch size:	128	Num Epochs:	20	Optimizer:	adagrad	Hidden Layers:	50	Embedding size:	25	K:	10	
Sample dist	1	10001	
nclasses:	1000	nfeatures:	1000	d_win:	5	
Full valid size	 73760
     5
[torch.LongStorage of size 2]

 73760
[torch.LongStorage of size 1]

Making neural network model	
Got params and grads	
Starting predictions	
Initialized output predictions tensor	
Starting Validation accuracy	0.16290801186944	4.0191701552077	
L1 norm of params:	23727.981034133	
Loss: 	6.89726636436	
Starting predictions	
Initialized output predictions tensor	
Epoch 1 Validation accuracy:	0.29020771513353	6.7895758557557	
L1 norm of params:	42600.910701332	
Loss: 	3.7231879447586	
Starting predictions	
Initialized output predictions tensor	
Epoch 2 Validation accuracy:	0.27655786350148	8.0853461968935	
L1 norm of params:	52934.334559214	
Loss: 	4.3303211160768	
Starting predictions	
Initialized output predictions tensor	
Epoch 3 Validation accuracy:	0.29643916913947	8.3087461927925	
L1 norm of params:	81902.523997987	
Loss: 	4.8599449114149	
Starting predictions	
Initialized output predictions tensor	
Epoch 4 Validation accuracy:	0.30148367952522	8.8865876899186	
L1 norm of params:	114939.53724826	
Loss: 	5.3330162416929	
Starting predictions	
Initialized output predictions tensor	
Epoch 5 Validation accuracy:	0.29940652818991	9.4599248955604	
L1 norm of params:	141752.44930244	
Loss: 	5.8346926777155	
Starting predictions	
Initialized output predictions tensor	
Epoch 6 Validation accuracy:	0.30445103857567	9.963641918344	
L1 norm of params:	166546.72235109	
Loss: 	6.2372985338407	
Starting predictions	
Initialized output predictions tensor	
Epoch 7 Validation accuracy:	0.3053412462908	10.290543224226	
L1 norm of params:	190892.69547244	
Loss: 	6.5181319354296	
Starting predictions	
Initialized output predictions tensor	
Epoch 8 Validation accuracy:	0.30385756676558	10.498017679767	
L1 norm of params:	216919.77730633	
Loss: 	6.6480175927986	
