[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	50	Lambda:	10	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1297
 5416
 2089
 3244
 4472
 3410
 4549
 3674
 2640
 5028
  892
 2710
 5731
 2519
 1384
 3011
 1140
 1181
 4975
 2059
 3091
 1660
 1305
 2202
 1171
 5369
 1749
 3518
 2352
 3930
 4126
 2441
  849
 2665
 1244
 1764
 3621
 1326
 5014
 2321
 5529
 3963
 1842
 4818
 2517
[torch.DoubleTensor of size 45]

Validation accuracy:	0.021576839038602	
Grad norm	0	
    Loss 22768603.809863	
    Loss 30287.384948409	
    Loss 13334.024818814	
    Loss 10661.863196714	
    Loss 18140.497671542	
    Loss 11791.030005334	
    Loss 10770.452455966	
    Loss 15699.165892903	
    Loss 14537.972090035	
    Loss 15861.589076051	
    Loss 11014.638432887	
    Loss 13189.348012989	
    Loss 13857.13404158	
    Loss 14541.200541504	
    Loss 10609.551208154	
    Loss 14709.400211003	
    Loss 15346.935189738	
    Loss 9891.1579264626	
    Loss 9996.7491552021	
    Loss 12484.156695665	
    Loss 15589.380508237	
    Loss 11129.755115001	
    Loss 12811.352962409	
    Loss 15677.864331516	
    Loss 14747.53738424	
    Loss 12478.475972289	
    Loss 8272.7418554914	
    Loss 17707.682698298	
    Loss 10350.672644013	
    Loss 16673.032596592	
    Loss 14975.941950154	
    Loss 9452.8233007118	
    Loss 9988.0860609812	
    Loss 14402.48287224	
    Loss 11641.529060492	
    Loss 14390.291305608	
    Loss 11658.969959786	
    Loss 13386.162072212	
    Loss 12768.813619515	
    Loss 11891.836638693	
    Loss 14707.139917017	
    Loss 12143.507314838	
    Loss 17515.682238281	
    Loss 14014.561146792	
    Loss 12262.853735765	
    Loss 14818.057801578	
    Loss 12149.837434568	
    Loss 14172.043066561	
    Loss 10502.448084413	
    Loss 14364.339307785	
    Loss 16178.258148241	
    Loss 14231.166247208	
    Loss 15938.553246863	
    Loss 14014.370347174	
    Loss 12017.652905319	
    Loss 16369.13363534	
    Loss 10341.394287235	
    Loss 10056.822211648	
    Loss 11494.312018036	
    Loss 10394.72626958	
    Loss 13696.628266216	
    Loss 12021.113001248	
    Loss 13501.14904617	
    Loss 9153.2064701362	
    Loss 11933.03974885	
    Loss 14672.131708234	
    Loss 13514.271805432	
    Loss 11120.030036924	
    Loss 14447.531481022	
    Loss 19522.629802063	
    Loss 12203.117403286	
    Loss 15847.70788116	
    Loss 11698.736439141	
    Loss 16706.11767485	
    Loss 15228.121327281	
    Loss 9759.5302159941	
    Loss 10285.600278658	
    Loss 16741.441872304	
    Loss 17836.016742618	
    Loss 11073.085715788	
    Loss 11065.428453744	
    Loss 15683.44263593	
    Loss 12397.324364023	
    Loss 9357.6977927011	
    Loss 16825.888888774	
    Loss 15025.349611311	
    Loss 12836.32581856	
    Loss 14192.246869515	
    Loss 12644.904175292	
    Loss 10967.619305355	
    Loss 13357.533117034	
    Loss 15511.400976741	
    Loss 12108.261222852	
    Loss 11674.798078892	
    Loss 14380.769150486	
    Loss 10212.441895117	
    Loss 11495.463927992	
    Loss 10117.19756048	
    Loss 13670.957222232	
    Loss 13524.205600227	
    Loss 13074.508208414	
    Loss 13174.294734199	
    Loss 12261.502265938	
    Loss 12709.670568099	
    Loss 14108.992603256	
    Loss 12891.074308151	
    Loss 11424.625052748	
    Loss 16767.960708939	
    Loss 12834.967362292	
    Loss 10144.076083501	
    Loss 9528.0750338519	
    Loss 17530.807227311	
    Loss 14662.706400059	
    Loss 10877.394596186	
    Loss 9934.2859910039	
    Loss 10863.541753839	
    Loss 27873.887206819	
    Loss 12796.356008714	
    Loss 11573.028450481	
    Loss 11410.602790388	
    Loss 14414.850329494	
    Loss 15268.143763264	
    Loss 14015.11483599	
    Loss 12670.626989847	
    Loss 14923.315695758	
    Loss 11707.680382843	
    Loss 14871.924349424	
    Loss 11955.247138482	
    Loss 15166.999803756	
    Loss 17351.830330084	
    Loss 15212.678914373	
    Loss 10807.415784478	
    Loss 13125.322985597	
    Loss 15646.983936899	
    Loss 10151.201724989	
    Loss 22675.057637434	
    Loss 10564.938778469	
    Loss 13894.356121534	
    Loss 10626.040172992	
    Loss 11931.72805758	
    Loss 13260.845975797	
    Loss 15048.643893072	
    Loss 16965.282111535	
Epoch 2	
     2
     0
 38391
 27818
     0
   184
   924
     0
     0
     0
 39076
   227
  1085
  2432
  7040
 14537
    68
    24
[torch.DoubleTensor of size 18]

Validation accuracy:	0.043601298858946	
Grad norm	10.273009217907	
    Loss 11597.266098923	
    Loss 13515.086679241	
    Loss 10353.671542206	
    Loss 11593.525135403	
    Loss 11817.345982975	
    Loss 19961.47527246	
    Loss 14109.0115225	
    Loss 14832.373735641	
    Loss 10627.557085744	
    Loss 18458.448175629	
    Loss 11067.069272515	
    Loss 14544.948394479	
    Loss 17386.906783646	
    Loss 11464.298961506	
    Loss 11039.073540127	
    Loss 15006.94234619	
    Loss 16173.941978842	
    Loss 13843.551811979	
    Loss 12884.877587836	
    Loss 13169.155110573	
    Loss 11693.351492294	
    Loss 11248.794987503	
    Loss 11121.494124529	
    Loss 12834.118761351	
    Loss 14742.927720134	
    Loss 12122.500537546	
    Loss 9917.5745836781	
    Loss 9737.6252824552	
    Loss 17350.168041325	
    Loss 17857.514590701	
    Loss 10228.735506362	
    Loss 9881.4556321102	
    Loss 12445.876258844	
    Loss 9115.1120240555	
    Loss 12685.10562667	
    Loss 17977.528564423	
    Loss 15443.134719366	
    Loss 12746.081832621	
    Loss 10734.217708803	
    Loss 10685.372971276	
    Loss 13753.533773646	
    Loss 9389.7068466362	
    Loss 17356.626447572	
    Loss 11827.754462508	
    Loss 14959.712109319	
    Loss 13260.494097335	
    Loss 13073.985579419	
    Loss 14749.917493475	
    Loss 13028.206777252	
    Loss 10425.423789409	
    Loss 14350.412968116	
    Loss 10400.215946161	
    Loss 14960.075515511	
    Loss 12875.314768288	
    Loss 16554.198272603	
    Loss 15145.641245769	
    Loss 17457.033530939	
    Loss 13439.008258978	
    Loss 11865.91294836	
    Loss 13528.508659363	
    Loss 11136.066425687	
    Loss 15096.273898455	
    Loss 11835.938731212	
    Loss 12027.698487605	
    Loss 14129.676523047	
    Loss 14995.145141161	
    Loss 12645.154609326	
    Loss 10454.777889449	
    Loss 13641.857558695	
    Loss 10245.377552776	
    Loss 9602.0167539658	
    Loss 12976.334267246	
    Loss 9934.672151937	
    Loss 10576.769655919	
    Loss 13246.016658654	
    Loss 12315.650788867	
    Loss 11607.289000434	
    Loss 10310.677042454	
    Loss 10825.835914441	
    Loss 9897.6863343969	
    Loss 11668.883520086	
    Loss 9873.3422675956	
    Loss 15413.192245106	
    Loss 13895.717059913	
    Loss 12021.803806607	
    Loss 12115.014685818	
    Loss 12839.530881804	
    Loss 12480.67977447	
    Loss 12312.020804871	
    Loss 10498.596834638	
    Loss 11883.449007851	
    Loss 9374.1335121489	
    Loss 10184.271176272	
    Loss 12058.272290853	
    Loss 15223.199424669	
    Loss 12707.105376571	
    Loss 15172.191015109	
    Loss 14903.06176961	
    Loss 15462.232373349	
    Loss 12970.869337276	
    Loss 12712.070316038	
    Loss 10922.482224751	
    Loss 12588.888077797	
    Loss 13424.219960225	
    Loss 12770.745651561	
    Loss 12677.715487355	
    Loss 10672.095427048	
    Loss 11777.262081543	
    Loss 11010.040408769	
    Loss 11657.207266191	
    Loss 11664.058698525	
    Loss 12192.562030722	
    Loss 15230.527591509	
    Loss 10815.335182156	
    Loss 13068.341359591	
    Loss 14666.447378493	
    Loss 10666.450346625	
    Loss 10648.248067222	
    Loss 11626.935817157	
    Loss 14923.443554503	
    Loss 9060.7697426097	
    Loss 14313.966053634	
    Loss 14799.725271007	
    Loss 10194.552904888	
    Loss 16834.189005288	
    Loss 12182.321579208	
    Loss 11087.735802442	
    Loss 9489.5761746466	
    Loss 14833.099612654	
    Loss 18060.232464995	
    Loss 18804.025704912	
    Loss 12827.87980283	
    Loss 10617.296553448	
    Loss 12465.574313239	
    Loss 12018.966182344	
    Loss 15640.210383579	
    Loss 13145.026235388	
    Loss 10310.703110495	
    Loss 9992.8209491346	
    Loss 11972.708662893	
    Loss 10094.268298731	
    Loss 13316.271310949	
    Loss 15913.67050677	
Epoch 3	
 36096
 89427
   531
  1487
     0
     0
     9
     0
     0
     0
  4019
   232
     0
     1
     2
     0
     4
[torch.DoubleTensor of size 17]

Validation accuracy:	0.017745508618597	
Grad norm	10.75281778047	
    Loss 11214.976014113	
    Loss 15152.993944072	
    Loss 12387.588735013	
    Loss 10629.2216371	
    Loss 13173.509807287	
    Loss 11801.530994796	
    Loss 15927.690680404	
    Loss 15478.414706281	
    Loss 11747.264435026	
    Loss 11059.456023631	
    Loss 16194.839509925	
    Loss 13922.40939746	
    Loss 12664.51316673	
    Loss 14825.969760848	
    Loss 11106.572892201	
    Loss 15985.955746629	
    Loss 11980.25023197	
    Loss 17204.276716366	
    Loss 19399.971319368	
    Loss 11502.748889131	
    Loss 15553.640604779	
    Loss 9988.116621204	
    Loss 13401.221184371	
    Loss 12657.380933759	
    Loss 12145.701729274	
    Loss 15390.707582794	
    Loss 13371.500486924	
    Loss 16617.760001685	
    Loss 11606.170991379	
    Loss 18352.69421058	
    Loss 13726.635235993	
    Loss 14130.95948679	
    Loss 14462.868181929	
    Loss 16492.940745701	
    Loss 18011.619491675	
    Loss 11962.938151794	
    Loss 10707.984633857	
    Loss 12205.528420791	
    Loss 13584.085380994	
    Loss 12615.92702247	
    Loss 13542.730694501	
    Loss 13905.012626333	
    Loss 15091.330741934	
    Loss 11522.89278617	
    Loss 13670.06424425	
    Loss 10517.546214677	
    Loss 14240.833361091	
    Loss 15915.312319955	
    Loss 11148.546565249	
    Loss 12417.826360756	
    Loss 11764.570674457	
    Loss 13641.999221997	
    Loss 12388.745391083	
    Loss 18812.097334132	
    Loss 11962.13107903	
    Loss 10427.647812081	
    Loss 12346.623849816	
    Loss 11557.065024041	
    Loss 9486.4790861705	
    Loss 13898.769201285	
    Loss 10866.91128196	
    Loss 12794.259536993	
    Loss 13913.450460325	
    Loss 10782.583962513	
    Loss 15397.829682286	
    Loss 11374.30270653	
    Loss 13516.611855975	
    Loss 13238.573817057	
    Loss 12205.600004943	
    Loss 15573.675607792	
    Loss 10282.091901087	
    Loss 12620.424046806	
    Loss 13519.042369875	
    Loss 12648.114354143	
    Loss 14269.722121003	
    Loss 10349.854611461	
    Loss 10672.988979767	
    Loss 10664.720319163	
    Loss 13248.01382744	
    Loss 17870.437397343	
    Loss 16618.806876526	
    Loss 11126.120769857	
    Loss 10757.274639682	
    Loss 14153.453440416	
    Loss 10959.087837515	
    Loss 9715.383740106	
    Loss 13925.77112879	
    Loss 12053.29878603	
    Loss 10748.847511397	
    Loss 10461.849633491	
    Loss 12147.357452335	
    Loss 17775.185730817	
    Loss 14874.650805513	
    Loss 11135.278621177	
    Loss 12719.69166531	
    Loss 18474.605214642	
    Loss 11451.645981571	
    Loss 11719.669541698	
    Loss 10607.215338972	
    Loss 16489.225236548	
    Loss 14076.669986281	
    Loss 12063.246957378	
    Loss 8972.6426703242	
    Loss 16612.22058976	
    Loss 11834.260493486	
    Loss 13475.703136105	
    Loss 11668.912911978	
    Loss 12192.464212308	
    Loss 10624.34165829	
    Loss 9092.472614203	
    Loss 8767.592062734	
    Loss 12019.329179345	
    Loss 15835.19698859	
    Loss 8843.7866649981	
    Loss 13998.094826471	
    Loss 10182.977999942	
    Loss 21830.333263397	
    Loss 16150.904420325	
    Loss 10975.449883357	
    Loss 10262.128953342	
    Loss 12583.793908878	
    Loss 14737.600428696	
    Loss 13414.282490181	
    Loss 13412.368286707	
    Loss 13098.618885803	
    Loss 11671.015588585	
    Loss 17497.78040656	
    Loss 10635.423827414	
    Loss 12142.751703539	
    Loss 19962.942146164	
    Loss 12222.739495409	
    Loss 10711.001106339	
    Loss 8359.907507853	
    Loss 11358.507382366	
    Loss 9012.4851268398	
    Loss 18158.751836008	
    Loss 14386.2867057	
    Loss 15785.258789327	
    Loss 11857.594516578	
    Loss 12293.102328561	
    Loss 11897.824511887	
    Loss 14742.738994472	
    Loss 18051.948688134	
Epoch 4	
 19566
     0
     0
    18
     0
     0
     0
 91377
     0
 20847
[torch.DoubleTensor of size 10]

Validation accuracy:	0.068455632435057	
Grad norm	10.889010989253	
    Loss 18049.096048027	
    Loss 13627.938451362	
    Loss 14622.45279887	
    Loss 12982.896620517	
    Loss 15921.979005901	
    Loss 10746.157726628	
    Loss 14368.963994134	
    Loss 14768.28101894	
    Loss 14607.305054174	
    Loss 13264.97571227	
    Loss 9163.0636455415	
    Loss 11881.349581282	
    Loss 11066.964964826	
    Loss 13077.33017017	
    Loss 11028.333147538	
    Loss 17118.167474796	
    Loss 13640.469067959	
    Loss 19603.203458971	
    Loss 10149.449007151	
    Loss 12038.832303721	
    Loss 12073.013428428	
    Loss 10794.97312097	
    Loss 15863.260799406	
    Loss 14038.556295194	
    Loss 15842.773609011	
    Loss 13114.563953966	
    Loss 14945.08543208	
    Loss 11175.956181951	
    Loss 12989.441188867	
    Loss 8926.9885980857	
    Loss 18599.996471118	
    Loss 11118.935617925	
    Loss 15717.490205937	
    Loss 16189.669970336	
    Loss 9720.7125968766	
    Loss 11561.420314207	
    Loss 12684.016014681	
    Loss 9978.4956692725	
    Loss 15490.522065467	
    Loss 12465.724864288	
    Loss 10395.982916421	
    Loss 10298.644122217	
    Loss 13055.217045148	
    Loss 12550.958119658	
    Loss 13447.615588042	
    Loss 13181.544042259	
    Loss 11472.183206045	
    Loss 13530.868429363	
    Loss 13421.048214139	
    Loss 9626.592607726	
    Loss 22239.02751084	
    Loss 11182.537051075	
    Loss 15558.968675356	
    Loss 15000.386035496	
    Loss 18565.787169379	
    Loss 15176.492622929	
    Loss 11871.937394237	
    Loss 14471.789137787	
    Loss 11056.984955541	
    Loss 13462.299475957	
    Loss 12925.98627499	
    Loss 10263.291393869	
    Loss 10546.023098347	
    Loss 11729.979447861	
    Loss 14189.32221623	
    Loss 14255.94075716	
    Loss 10879.124140523	
    Loss 11911.575717368	
    Loss 15849.174343524	
    Loss 13181.196103929	
    Loss 14507.744471233	
    Loss 14305.849818514	
    Loss 11854.596035425	
    Loss 14986.989413313	
    Loss 9966.8520520401	
    Loss 15214.56844634	
    Loss 12799.466390339	
    Loss 12311.242092801	
    Loss 11191.196386087	
    Loss 14502.569852126	
    Loss 11322.973862555	
    Loss 11499.215299695	
    Loss 15065.548244501	
    Loss 13285.467837143	
    Loss 19812.62754474	
    Loss 15426.295463529	
    Loss 11565.823682041	
    Loss 9313.0710504135	
    Loss 15060.536551856	
    Loss 12268.623133955	
    Loss 10057.680620744	
    Loss 12633.928036088	
    Loss 11092.849964226	
    Loss 10797.54269524	
    Loss 10252.847240923	
    Loss 11426.398449142	
    Loss 11660.589068386	
    Loss 11951.862285827	
    Loss 11366.111043582	
    Loss 12608.025972291	
    Loss 20228.705151962	
    Loss 11411.041181047	
    Loss 10218.115706104	
    Loss 11055.340154351	
    Loss 12815.189879922	
    Loss 15511.447906675	
    Loss 12349.220999534	
    Loss 12474.657269968	
    Loss 14727.812539198	
    Loss 9587.4396907923	
    Loss 10553.010780191	
    Loss 10415.444946179	
    Loss 18063.406241094	
    Loss 10626.272793176	
    Loss 9104.9425482512	
    Loss 11247.628387478	
    Loss 20312.534742875	
    Loss 13680.461008271	
    Loss 16832.215207173	
    Loss 10096.119761569	
    Loss 13626.975782829	
    Loss 12663.52749307	
    Loss 9950.4643391672	
    Loss 12606.672532367	
    Loss 15437.611063796	
    Loss 16080.638275676	
    Loss 12827.18206962	
    Loss 11471.680823069	
    Loss 16496.876893382	
    Loss 12641.096433706	
    Loss 14577.227364147	
    Loss 9207.1307080272	
    Loss 17192.050420199	
    Loss 15217.812585383	
    Loss 11512.613943975	
    Loss 19275.155898431	
    Loss 9918.6587110452	
    Loss 10981.715953815	
    Loss 9519.023259003	
    Loss 15547.775831264	
    Loss 13841.68427823	
    Loss 15013.414249837	
    Loss 18318.378080979	
Epoch 5	
 96059
  4615
     0
     4
   384
     0
     5
     0
 26945
     0
  3787
     0
     1
     0
     7
     1
[torch.DoubleTensor of size 16]

Validation accuracy:	0.070344743869871	
Grad norm	11.01535034435	
    Loss 15920.48092114	
    Loss 11160.171123404	
    Loss 12378.307985948	
    Loss 11531.498435372	
    Loss 10029.676819777	
    Loss 12389.39979371	
    Loss 12422.28831874	
    Loss 13045.050845908	
    Loss 13582.212152586	
    Loss 12326.599189471	
    Loss 12018.266420926	
    Loss 17059.606290346	
    Loss 14938.194580236	
    Loss 16754.597933862	
    Loss 10026.80589924	
    Loss 12326.37467288	
    Loss 15965.21128674	
    Loss 17301.284447844	
    Loss 17647.226036306	
    Loss 16546.80911992	
    Loss 10995.678216656	
    Loss 11841.710007378	
    Loss 16232.462302953	
    Loss 12554.156156437	
    Loss 11811.10112204	
    Loss 13539.672883375	
    Loss 13063.614286729	
    Loss 14626.813757604	
    Loss 13012.991411975	
    Loss 10976.786494461	
    Loss 15991.041332222	
    Loss 12394.036232282	
    Loss 10049.128261296	
    Loss 14426.883781348	
    Loss 10635.098926192	
    Loss 11117.025410588	
    Loss 12596.666692114	
    Loss 11103.65309985	
    Loss 12256.985453178	
    Loss 14379.194418652	
    Loss 17909.40770935	
    Loss 14297.584268472	
    Loss 19402.106590384	
    Loss 11467.010519101	
    Loss 12011.248540078	
    Loss 13885.722416712	
    Loss 12845.52177545	
    Loss 14179.043250551	
    Loss 11925.830294982	
    Loss 11677.303950299	
    Loss 17887.987451626	
    Loss 14624.868188333	
    Loss 10944.330279915	
    Loss 11562.032262853	
    Loss 15845.043938387	
    Loss 12303.689714712	
    Loss 9885.3753021019	
    Loss 12561.89041823	
    Loss 10064.127179148	
    Loss 11066.089254541	
    Loss 16457.350831522	
    Loss 15247.441743576	
    Loss 15605.013445996	
    Loss 11557.224199776	
    Loss 14463.379293629	
    Loss 12932.247996245	
    Loss 14767.451729942	
    Loss 11577.074432582	
    Loss 17682.657334497	
    Loss 15279.1914938	
    Loss 13935.627567548	
    Loss 10285.860751156	
    Loss 12095.217817159	
    Loss 15125.848979285	
    Loss 15044.393031791	
    Loss 12069.951135163	
    Loss 15009.453810241	
    Loss 9230.661396971	
    Loss 11638.208494329	
    Loss 10742.619001875	
    Loss 9992.9707906204	
    Loss 10356.933192334	
    Loss 11850.268986055	
    Loss 10866.635032754	
    Loss 9215.2572833743	
    Loss 14815.919471404	
    Loss 13877.527326959	
    Loss 19222.576215418	
    Loss 11277.823422055	
    Loss 10812.475837386	
    Loss 11250.919090017	
    Loss 13233.132743468	
    Loss 9714.9470629252	
    Loss 11149.722267934	
    Loss 10480.328738593	
    Loss 12883.541606849	
    Loss 12927.274248142	
    Loss 14839.725400331	
    Loss 12113.90768304	
    Loss 13311.01300845	
    Loss 10893.223005157	
    Loss 9949.4128785793	
    Loss 12596.615426182	
    Loss 9917.4880317802	
    Loss 10625.736960175	
    Loss 17986.174486233	
    Loss 11309.063586412	
    Loss 16838.266714381	
    Loss 11840.436134365	
    Loss 9572.3552953456	
    Loss 10153.268467115	
    Loss 9749.2413466815	
    Loss 14454.265507025	
    Loss 9437.1037897265	
    Loss 12606.994313311	
    Loss 10269.166295714	
    Loss 17036.518641825	
    Loss 16426.105891846	
    Loss 12871.181617595	
    Loss 11125.877134775	
    Loss 11735.577513652	
