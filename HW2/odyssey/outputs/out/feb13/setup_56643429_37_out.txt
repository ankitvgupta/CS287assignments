[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.25	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4707
  762
 2347
 3836
  978
 5645
 2359
 2193
 4998
 1439
 6045
 1315
 2891
 2462
 2256
 1195
 3594
 2937
 4537
 1809
 4526
 1238
 4307
 5685
 3966
  749
 3307
 1594
 2553
 3107
 2619
 2407
 2169
 2882
 3414
 1567
 1623
 2195
 2564
 4336
 4248
 1933
 3313
 2326
 4875
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02114439184268	
Grad norm	0	
    Loss 11368998.745449	
    Loss 10976985.026581	
    Loss 10598527.531238	
    Loss 10233117.372297	
    Loss 9880312.4076867	
    Loss 9539668.2531995	
    Loss 9210770.6145688	
    Loss 8893215.8469338	
    Loss 8586609.5281443	
    Loss 8290576.1958223	
    Loss 8004747.5534853	
    Loss 7728776.3657463	
    Loss 7462322.247939	
    Loss 7205055.8007871	
    Loss 6956661.1696052	
    Loss 6716826.9012992	
    Loss 6485263.8370047	
    Loss 6261683.4089791	
    Loss 6045808.0273939	
    Loss 5837383.4892154	
    Loss 5636142.7754091	
    Loss 5441839.3694165	
    Loss 5254234.9238331	
    Loss 5073098.4056083	
    Loss 4898208.515918	
    Loss 4729347.7145302	
    Loss 4566310.1391837	
    Loss 4408892.8928282	
    Loss 4256903.4511119	
    Loss 4110150.5925811	
    Loss 3968461.6385088	
    Loss 3831658.4218543	
    Loss 3699570.1013281	
    Loss 3572036.6360351	
    Loss 3448899.0369772	
    Loss 3330005.6251382	
    Loss 3215213.5456007	
    Loss 3104380.3273206	
    Loss 2997367.5311404	
    Loss 2894044.6792822	
    Loss 2794281.5292266	
    Loss 2697959.6597337	
    Loss 2604957.9667457	
    Loss 2515163.9739742	
    Loss 2428464.895016	
    Loss 2344752.1700869	
    Loss 2263928.0913344	
    Loss 2185890.9863055	
    Loss 2110543.3135956	
    Loss 2037795.3805624	
    Loss 1967552.9947079	
    Loss 1899732.8274066	
    Loss 1834250.697841	
    Loss 1771026.2479106	
    Loss 1709981.484499	
    Loss 1651040.466309	
    Loss 1594133.0301351	
    Loss 1539187.5172273	
    Loss 1486135.1874002	
    Loss 1434912.4034358	
    Loss 1385456.5094328	
    Loss 1337704.5259612	
    Loss 1291599.9866126	
    Loss 1247084.5588956	
    Loss 1204102.3941261	
    Loss 1162602.5771424	
    Loss 1122535.1963922	
    Loss 1083848.0702311	
    Loss 1046495.0118849	
    Loss 1010428.1500269	
    Loss 975605.63116522	
    Loss 941983.03287972	
Epoch 2	
 51950
  6849
    23
 10653
  6542
     0
     0
  4570
 39955
 11185
    67
     5
     0
     0
     0
     8
     0
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.10069950230639	
Grad norm	733.62938973069	
    Loss 932124.52774345	
    Loss 900000.86812606	
    Loss 868985.27317284	
    Loss 839038.81381332	
    Loss 810125.73536964	
    Loss 782208.60925668	
    Loss 755253.86839829	
    Loss 729229.38362739	
    Loss 704101.30812097	
    Loss 679839.80220459	
    Loss 656414.09409191	
    Loss 633796.38308318	
    Loss 611958.65298653	
    Loss 590873.32044419	
    Loss 570515.57211326	
    Loss 550859.13513196	
    Loss 531880.49372792	
    Loss 513555.71906156	
    Loss 495861.67680518	
    Loss 478779.53012895	
    Loss 462285.60916625	
    Loss 446360.44262131	
    Loss 430984.35099661	
    Loss 416138.01308304	
    Loss 401803.76567984	
    Loss 387963.33174436	
    Loss 374600.31069439	
    Loss 361698.16328537	
    Loss 349241.23617594	
    Loss 337212.37071355	
    Loss 325599.12909426	
    Loss 314386.40160125	
    Loss 303560.03928256	
    Loss 293107.0323962	
    Loss 283014.30553577	
    Loss 273268.7931346	
    Loss 263859.85179104	
    Loss 254775.40786564	
    Loss 246004.17449615	
    Loss 237535.57824819	
    Loss 229358.28117379	
    Loss 221463.0588279	
    Loss 213839.97105273	
    Loss 206479.72506725	
    Loss 199373.32531029	
    Loss 192511.83679443	
    Loss 185887.15828437	
    Loss 179490.43133649	
    Loss 173314.31513599	
    Loss 167351.22571353	
    Loss 161593.75646338	
    Loss 156034.61198481	
    Loss 150667.11582026	
    Loss 145484.6073082	
    Loss 140480.90131809	
    Loss 135649.19249493	
    Loss 130984.54429983	
    Loss 126481.37003496	
    Loss 122132.81949004	
    Loss 117933.87839258	
    Loss 113880.28658227	
    Loss 109965.94041133	
    Loss 106186.81611824	
    Loss 102537.72492089	
    Loss 99014.473457595	
    Loss 95612.270033054	
    Loss 92327.969375066	
    Loss 89156.877466084	
    Loss 86095.185067683	
    Loss 83138.521065453	
    Loss 80284.085421228	
    Loss 77527.811231991	
Epoch 3	
 87923
    66
     0
   297
    43
     0
     0
     6
 43100
   373
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10409079873756	
Grad norm	213.71846880966	
    Loss 76719.505471878	
    Loss 74086.137729493	
    Loss 71543.955691429	
    Loss 69089.120455855	
    Loss 66719.248099104	
    Loss 64430.95211505	
    Loss 62221.170136634	
    Loss 60087.633141071	
    Loss 58027.871644285	
    Loss 56039.25825597	
    Loss 54118.911270396	
    Loss 52265.064711101	
    Loss 50474.969535163	
    Loss 48746.152688314	
    Loss 47077.358329866	
    Loss 45466.118960747	
    Loss 43910.546329206	
    Loss 42408.318786054	
    Loss 40957.346353277	
    Loss 39557.187802667	
    Loss 38205.010921574	
    Loss 36899.696087121	
    Loss 35639.371283894	
    Loss 34422.287682077	
    Loss 33247.195061913	
    Loss 32112.420286693	
    Loss 31016.813393334	
    Loss 29959.304135736	
    Loss 28938.511276989	
    Loss 27952.247630342	
    Loss 27000.31464681	
    Loss 26081.065816257	
    Loss 25193.686923781	
    Loss 24336.945141072	
    Loss 23509.588763414	
    Loss 22710.37298862	
    Loss 21938.988995555	
    Loss 21194.163982901	
    Loss 20475.144978203	
    Loss 19781.074296077	
    Loss 19110.797809818	
    Loss 18463.454719632	
    Loss 17838.527301835	
    Loss 17234.824210944	
    Loss 16652.201725439	
    Loss 16090.05219958	
    Loss 15547.16589875	
    Loss 15022.513178087	
    Loss 14516.168712441	
    Loss 14027.150294413	
    Loss 13555.270193493	
    Loss 13099.498273614	
    Loss 12659.486467222	
    Loss 12234.60720436	
    Loss 11824.540143976	
    Loss 11428.16493407	
    Loss 11045.702879458	
    Loss 10677.130178187	
    Loss 10320.855631855	
    Loss 9976.4121744879	
    Loss 9644.3127064588	
    Loss 9323.3388460404	
    Loss 9013.4962838544	
    Loss 8714.1623722758	
    Loss 8425.4098320215	
    Loss 8146.1175811798	
    Loss 7876.8626148827	
    Loss 7617.0387732714	
    Loss 7366.1126812275	
    Loss 7123.6127840313	
    Loss 6889.6489070076	
    Loss 6663.6160299613	
Epoch 4	
 107945
      0
      0
      0
      0
      0
      0
      0
  23863
[torch.DoubleTensor of size 9]

Validation accuracy:	0.092839584850692	
Grad norm	64.973148371697	
    Loss 6597.1616378742	
    Loss 6381.2480753055	
    Loss 6173.0602632657	
    Loss 5971.7976700692	
    Loss 5777.6413158849	
    Loss 5590.2414391281	
    Loss 5408.8795184503	
    Loss 5233.6214293609	
    Loss 5064.8251700245	
    Loss 4901.9799815342	
    Loss 4744.4937385908	
    Loss 4592.7107371546	
    Loss 4445.9390989952	
    Loss 4303.8816817034	
    Loss 4167.0324742698	
    Loss 4035.0275092475	
    Loss 3907.6871778264	
    Loss 3784.4899946264	
    Loss 3665.1341025058	
    Loss 3550.446425574	
    Loss 3439.5062390751	
    Loss 3332.6467680453	
    Loss 3229.4369825037	
    Loss 3129.6362216017	
    Loss 3033.269010635	
    Loss 2940.0762709414	
    Loss 2850.0680180102	
    Loss 2763.5100667887	
    Loss 2680.1511276906	
    Loss 2599.2073332909	
    Loss 2521.2566195113	
    Loss 2445.7711205689	
    Loss 2373.1638571533	
    Loss 2303.0840106911	
    Loss 2235.2236832157	
    Loss 2169.435762087	
    Loss 2106.1299985391	
    Loss 2044.9298154728	
    Loss 1985.9915267147	
    Loss 1929.2119393992	
    Loss 1874.3975952654	
    Loss 1821.2273683976	
    Loss 1770.01420623	
    Loss 1720.1689968603	
    Loss 1672.3318408111	
    Loss 1626.6312325832	
    Loss 1582.3190397901	
    Loss 1539.0506938968	
    Loss 1497.5138453407	
    Loss 1457.2400477651	
    Loss 1418.6595760105	
    Loss 1381.2585523633	
    Loss 1345.2148758988	
    Loss 1310.3868002992	
    Loss 1276.9259879584	
    Loss 1244.186672686	
    Loss 1212.7591941957	
    Loss 1183.1173660432	
    Loss 1154.1582312672	
    Loss 1125.7161197272	
    Loss 1098.6797242266	
    Loss 1072.3055868718	
    Loss 1046.8511949951	
    Loss 1022.1228395047	
    Loss 998.55121159651	
    Loss 975.29347234472	
    Loss 953.1987382893	
    Loss 932.04561752126	
    Loss 911.51700568855	
    Loss 891.55352265226	
    Loss 872.42454380752	
    Loss 853.85135553929	
Epoch 5	
 115295
      0
      0
      0
      0
      0
      0
      0
  16513
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094182447195921	
Grad norm	22.488491635153	
    Loss 848.21376091078	
    Loss 830.51328677468	
    Loss 813.6661247958	
    Loss 797.16804084728	
    Loss 781.35738786209	
    Loss 766.20976634398	
    Loss 751.14652866612	
    Loss 736.40332853691	
    Loss 722.63024681849	
    Loss 709.46670219433	
    Loss 696.5063334265	
    Loss 684.25905295837	
    Loss 672.19959082157	
    Loss 660.2424429945	
    Loss 648.9729632418	
    Loss 638.24073693185	
    Loss 627.98994626876	
    Loss 617.85527108239	
    Loss 607.69824961147	
    Loss 598.38538522914	
    Loss 589.20582461005	
    Loss 580.59822192877	
    Loss 572.24651222964	
    Loss 564.05214601922	
    Loss 556.12392710323	
    Loss 548.33296502583	
    Loss 540.76005407552	
    Loss 533.79894578397	
    Loss 527.28084718538	
    Loss 520.58512171396	
    Loss 514.29111918968	
    Loss 507.96592518968	
    Loss 502.16059988974	
    Loss 496.57305931112	
    Loss 490.96660250742	
    Loss 485.31905344332	
    Loss 480.06490146685	
    Loss 474.90262212069	
    Loss 470.07732203185	
    Loss 465.5334792391	
    Loss 461.19339591072	
    Loss 456.74056420298	
    Loss 452.56831637917	
    Loss 448.12501001874	
    Loss 444.12697863102	
    Loss 440.77851518949	
    Loss 437.34237416121	
    Loss 433.54135634061	
    Loss 430.11413006923	
    Loss 426.6282927569	
    Loss 423.57090240166	
    Loss 420.47083544468	
    Loss 417.55217191901	
    Loss 414.71022385759	
    Loss 412.1321861823	
    Loss 409.21416514465	
    Loss 406.56172622599	
    Loss 404.70108788051	
    Loss 402.58602408951	
    Loss 400.05545767001	
    Loss 398.02290061474	
    Loss 395.80623816053	
    Loss 393.66184366758	
    Loss 391.44521285084	
    Loss 389.61880822114	
    Loss 387.3571421643	
    Loss 385.52212290088	
    Loss 383.93766914191	
    Loss 382.28949853057	
    Loss 380.57822931826	
    Loss 379.06423490126	
    Loss 377.50758464897	
Epoch 6	
 116094
      0
      0
      0
      0
      0
      0
      0
  15714
[torch.DoubleTensor of size 9]

Validation accuracy:	0.095730152949745	
Grad norm	10.483096287514	
    Loss 376.85578617691	
    Loss 375.41338301547	
    Loss 374.25274496249	
    Loss 372.90683223031	
    Loss 371.71161904193	
    Loss 370.69152243114	
    Loss 369.26380518857	
    Loss 367.67313215973	
    Loss 366.60891604064	
    Loss 365.71777933714	
    Loss 364.60756451906	
    Loss 363.79948825678	
    Loss 362.78154257654	
    Loss 361.49506144639	
    Loss 360.51808546781	
    Loss 359.72877102788	
    Loss 359.07745414525	
    Loss 358.21408748075	
    Loss 357.01804192656	
    Loss 356.34143942461	
    Loss 355.50490148683	
    Loss 354.95211975382	
    Loss 354.37571855024	
    Loss 353.694017339	
    Loss 353.0169876253	
    Loss 352.23030798543	
    Loss 351.41403938404	
    Loss 350.97728426528	
    Loss 350.75541755156	
    Loss 350.15451037667	
    Loss 349.73680222241	
    Loss 349.07789240058	
    Loss 348.75050221122	
    Loss 348.45006685344	
    Loss 347.94410203537	
    Loss 347.2301416064	
    Loss 346.73561222213	
    Loss 346.16525211886	
    Loss 345.77579207036	
    Loss 345.51101559751	
    Loss 345.31359284254	
    Loss 344.85648147447	
    Loss 344.54218153127	
    Loss 343.81989527872	
    Loss 343.41472087608	
    Loss 343.5415503825	
    Loss 343.45681680422	
    Loss 342.89191695008	
    Loss 342.58978838368	
    Loss 342.11954375437	
    Loss 341.97433096475	
    Loss 341.68665279565	
    Loss 341.48539002802	
    Loss 341.26809956896	
    Loss 341.2238421181	
    Loss 340.75306189956	
    Loss 340.45840246496	
    Loss 340.87364457809	
    Loss 340.96199707163	
    Loss 340.55711686532	
    Loss 340.57252163275	
    Loss 340.33769563891	
    Loss 340.10292802479	
    Loss 339.73140733133	
    Loss 339.6889029827	
    Loss 339.14997656469	
    Loss 338.97485158266	
    Loss 338.99505722986	
    Loss 338.89223910681	
    Loss 338.67939231131	
    Loss 338.6099152153	
    Loss 338.45049524074	
Epoch 7	
 116169
      0
      0
      0
      0
      0
      0
      0
  15639
[torch.DoubleTensor of size 9]

Validation accuracy:	0.09610949259529	
Grad norm	7.2082744557369	
    Loss 338.20735171962	
    Loss 338.09982251783	
    Loss 338.22505021973	
    Loss 338.12254777631	
    Loss 338.1237480513	
    Loss 338.26339670584	
    Loss 337.95391341742	
    Loss 337.43950651006	
    Loss 337.41684044178	
    Loss 337.53201061189	
    Loss 337.39363196479	
    Loss 337.52316552958	
    Loss 337.40958609665	
    Loss 336.99915157829	
    Loss 336.86510248793	
    Loss 336.89092048988	
    Loss 337.02648161479	
    Loss 336.92369643288	
    Loss 336.46457284947	
    Loss 336.49517571388	
    Loss 336.34259766466	
    Loss 336.44998965974	
    Loss 336.51048115358	
    Loss 336.44525976121	
    Loss 336.36278193618	
    Loss 336.15098322115	
    Loss 335.88809325428	
    Loss 335.98576014731	
    Loss 336.27903698268	
    Loss 336.17988415211	
    Loss 336.24435043468	
    Loss 336.04890289226	
    Loss 336.17088953011	
    Loss 336.30377414664	
    Loss 336.21499125074	
    Loss 335.90624807707	
    Loss 335.80197111636	
    Loss 335.60735491512	
    Loss 335.58139528826	
    Loss 335.6663721847	
    Loss 335.80980436764	
    Loss 335.68076701133	
    Loss 335.68302965243	
    Loss 335.26549355126	
    Loss 335.15448756048	
    Loss 335.56701904263	
    Loss 335.75704663462	
    Loss 335.45752949008	
    Loss 335.41175669908	
    Loss 335.18860703284	
    Loss 335.28206805579	
    Loss 335.22498201232	
    Loss 335.24693478669	
    Loss 335.24544957203	
    Loss 335.40940635405	
    Loss 335.13987139047	
    Loss 335.03808826606	
    Loss 335.63938576876	
    Loss 335.90910010351	
    Loss 335.67887753691	
    Loss 335.86158945926	
    Loss 335.78957313489	
    Loss 335.71096679379	
    Loss 335.49055390385	
    Loss 335.59457815835	
    Loss 335.1972512884	
    Loss 335.15787374926	
    Loss 335.30971108412	
    Loss 335.33282949088	
    Loss 335.24336471685	
    Loss 335.29237297383	
    Loss 335.24807012203	
Epoch 8	
 116144
      0
      0
      0
      0
      0
      0
      0
  15664
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096139839766934	
Grad norm	6.3590129316234	
    Loss 335.03839228397	
    Loss 335.04080297173	
    Loss 335.27138220038	
    Loss 335.27113225422	
    Loss 335.36988275383	
    Loss 335.60502598581	
    Loss 335.38729013719	
    Loss 334.96054268693	
    Loss 335.02311204865	
    Loss 335.22081101252	
    Loss 335.16218434716	
    Loss 335.36851111863	
    Loss 335.32881286816	
    Loss 334.99053528433	
    Loss 334.92533517897	
    Loss 335.01793610022	
    Loss 335.21794812121	
    Loss 335.17764247188	
    Loss 334.77955497125	
    Loss 334.86789622392	
    Loss 334.77136081504	
    Loss 334.93280222839	
    Loss 335.04532711582	
    Loss 335.03078170592	
    Loss 334.99704946666	
    Loss 334.83254118064	
    Loss 334.61485209754	
    Loss 334.75618206608	
    Loss 335.09138735684	
    Loss 335.0339422424	
    Loss 335.13805233336	
    Loss 334.98029684736	
    Loss 335.13918850993	
    Loss 335.30754781702	
    Loss 335.2526905513	
    Loss 334.97735617682	
    Loss 334.90507905621	
    Loss 334.74105206257	
    Loss 334.74484934603	
    Loss 334.85818359232	
    Loss 335.02989475147	
    Loss 334.92790236042	
    Loss 334.95617549064	
    Loss 334.56354728443	
    Loss 334.47654500754	
    Loss 334.91270471372	
    Loss 335.12524683381	
    Loss 334.84749978196	
    Loss 334.82277818466	
    Loss 334.61984910001	
    Loss 334.73285055706	
    Loss 334.69466785809	
    Loss 334.73503684454	
    Loss 334.75141939909	
    Loss 334.93257683761	
    Loss 334.67970406838	
    Loss 334.59360531142	
    Loss 335.21000213185	
    Loss 335.49478541943	
    Loss 335.27899513509	
    Loss 335.4752508856	
    Loss 335.41667015339	
    Loss 335.35075859736	
    Loss 335.14268668457	
    Loss 335.25879584887	
    Loss 334.87317643945	
    Loss 334.84482776763	
    Loss 335.0074768937	
    Loss 335.04069984495	
    Loss 334.96149643976	
    Loss 335.02022692559	
    Loss 334.98551556703	
Epoch 9	
 116151
      0
      0
      0
      0
      0
      0
      0
  15657
[torch.DoubleTensor of size 9]

Validation accuracy:	0.09610949259529	
Grad norm	6.1538987347171	
    Loss 334.7785697245	
    Loss 334.79012474419	
    Loss 335.02932031369	
    Loss 335.03753993463	
    Loss 335.14413410899	
    Loss 335.38722321249	
    Loss 335.17702648994	
    Loss 334.75730427523	
    Loss 334.82680833473	
    Loss 335.0312790483	
    Loss 334.97921249654	
    Loss 335.19181356123	
    Loss 335.15809582895	
    Loss 334.8258282845	
    Loss 334.76620106441	
    Loss 334.86425715628	
    Loss 335.06953221828	
    Loss 335.03438348494	
    Loss 334.64147279783	
    Loss 334.73447965773	
    Loss 334.64252691674	
    Loss 334.8083762151	
    Loss 334.92511178353	
    Loss 334.91475468023	
    Loss 334.88501682646	
    Loss 334.72442734631	
    Loss 334.51039614863	
    Loss 334.65526042341	
    Loss 334.99381618626	
    Loss 334.93995012227	
    Loss 335.04734299681	
    Loss 334.89259355272	
    Loss 335.05452649344	
    Loss 335.22577840411	
    Loss 335.17362599946	
    Loss 334.90107955428	
    Loss 334.8314274728	
    Loss 334.66984545911	
    Loss 334.67607166557	
    Loss 334.79163830715	
    Loss 334.96576081613	
    Loss 334.86603126128	
    Loss 334.89644953531	
    Loss 334.50584562344	
    Loss 334.42077925171	
    Loss 334.85893126382	
    Loss 335.07331401226	
    Loss 334.79735427371	
    Loss 334.77436733921	
    Loss 334.57308646309	
    Loss 334.68768176979	
    Loss 334.65104748013	
    Loss 334.69295841697	
    Loss 334.71085585528	
    Loss 334.8934595902	
    Loss 334.6419976622	
    Loss 334.55714701891	
    Loss 335.17473806739	
    Loss 335.46081340633	
    Loss 335.24623987875	
    Loss 335.4435567786	
    Loss 335.38610161165	
    Loss 335.32120197656	
    Loss 335.11412858196	
    Loss 335.23124796007	
    Loss 334.8466159264	
    Loss 334.81914233379	
    Loss 334.98268387557	
    Loss 335.01667198933	
    Loss 334.93835050429	
    Loss 334.99787888688	
    Loss 334.96399626293	
Epoch 10	
 116156
      0
      0
      0
      0
      0
      0
      0
  15652
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096094319009468	
Grad norm	6.1089310502343	
    Loss 334.75727100753	
    Loss 334.76961049753	
    Loss 335.00950673224	
    Loss 335.01844506045	
    Loss 335.12563870234	
    Loss 335.36941272868	
    Loss 335.15983893301	
    Loss 334.74064779341	
    Loss 334.81070337283	
    Loss 335.01573069973	
    Loss 334.96420851103	
    Loss 335.17731832843	
    Loss 335.14406896094	
    Loss 334.81232099036	
    Loss 334.75313103073	
    Loss 334.85162713361	
    Loss 335.05732716227	
    Loss 335.0226112593	
    Loss 334.63017355395	
    Loss 334.72354395287	
    Loss 334.63196305635	
    Loss 334.798166937	
    Loss 334.9152313882	
    Loss 334.90522629674	
    Loss 334.87581500447	
    Loss 334.71555807632	
    Loss 334.50181331557	
    Loss 334.64695408521	
    Loss 334.98575993231	
    Loss 334.9322321877	
    Loss 335.03990374422	
    Loss 334.88537765727	
    Loss 335.04756393577	
    Loss 335.21904835502	
    Loss 335.16709675864	
    Loss 334.89479188673	
    Loss 334.82535566797	
    Loss 334.66395601028	
    Loss 334.67037891483	
    Loss 334.78610187392	
    Loss 334.96044810789	
    Loss 334.86091804119	
    Loss 334.89151506623	
    Loss 334.50107317376	
    Loss 334.4161565705	
    Loss 334.85448696138	
    Loss 335.06901874231	
    Loss 334.79320602224	
    Loss 334.77036370359	
    Loss 334.56921555698	
    Loss 334.68393916014	
    Loss 334.64743131199	
    Loss 334.68947766095	
    Loss 334.70751366512	
    Loss 334.89024610296	
    Loss 334.63891229957	
    Loss 334.55415300464	
    Loss 335.17182974086	
    Loss 335.45802710297	
    Loss 335.24356323462	
    Loss 335.44095322687	
    Loss 335.38359702796	
    Loss 335.31877267788	
    Loss 335.11177750904	
    Loss 335.22898507661	
    Loss 334.84444178597	
    Loss 334.81703154981	
    Loss 334.98064812308	
    Loss 335.01468089024	
    Loss 334.93644314001	
    Loss 334.99603683576	
    Loss 334.96223393325	
Epoch 11	
 116153
      0
      0
      0
      0
      0
      0
      0
  15655
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096094319009468	
Grad norm	6.1000675268957	
    Loss 334.75552579664	
    Loss 334.76793890865	
    Loss 335.00789105558	
    Loss 335.01689518266	
    Loss 335.12412567606	
    Loss 335.36796550472	
    Loss 335.15844420939	
    Loss 334.73928414068	
    Loss 334.80937967567	
    Loss 335.01445303586	
    Loss 334.96297747313	
    Loss 335.17612757983	
    Loss 335.14291035952	
    Loss 334.81121264936	
    Loss 334.75205318369	
    Loss 334.85058293578	
    Loss 335.05631584884	
    Loss 335.02163839064	
    Loss 334.62925315749	
    Loss 334.72264808727	
    Loss 334.63109649703	
    Loss 334.79732752487	
    Loss 334.91441416874	
    Loss 334.90444017167	
    Loss 334.87505533614	
    Loss 334.71482876271	
    Loss 334.50110372284	
    Loss 334.64626332913	
    Loss 334.98508273681	
    Loss 334.93159539161	
    Loss 335.03929259045	
    Loss 334.88477848073	
    Loss 335.04698656286	
    Loss 335.21848870495	
    Loss 335.16654785506	
    Loss 334.89426627707	
    Loss 334.82484801882	
    Loss 334.66345812475	
    Loss 334.66989662068	
    Loss 334.7856247199	
    Loss 334.95999661466	
    Loss 334.86048714639	
    Loss 334.89109948595	
    Loss 334.5006700379	
    Loss 334.41576321756	
    Loss 334.85411236601	
    Loss 335.06865574282	
    Loss 334.79285515819	
    Loss 334.77002536497	
    Loss 334.56888749865	
    Loss 334.68362093285	
    Loss 334.64712329212	
    Loss 334.68918327585	
    Loss 334.70723477413	
    Loss 334.88998061878	
    Loss 334.63866080734	
    Loss 334.55390582234	
    Loss 335.17158618356	
    Loss 335.45779812765	
    Loss 335.24334615324	
    Loss 335.44073822653	
    Loss 335.38339201289	
    Loss 335.31857179193	
    Loss 335.11158204127	
    Loss 335.22879833584	
    Loss 334.84426453632	
    Loss 334.81685708449	
    Loss 334.98048038915	
    Loss 335.01451165963	
    Loss 334.93628400487	
    Loss 334.99588294351	
    Loss 334.96208986352	
Epoch 12	
 116152
      0
      0
      0
      0
      0
      0
      0
  15656
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096094319009468	
Grad norm	6.0986631732252	
    Loss 334.75538285714	
    Loss 334.76780450645	
    Loss 335.00776083365	
    Loss 335.01677229644	
    Loss 335.12400234583	
    Loss 335.36785035322	
    Loss 335.15833379224	
    Loss 334.73917280266	
    Loss 334.80927001678	
    Loss 335.01434726586	
    Loss 334.96287613376	
    Loss 335.17602919464	
    Loss 335.14281283193	
    Loss 334.81112143024	
    Loss 334.75196300403	
    Loss 334.85049473205	
    Loss 335.05622976419	
    Loss 335.02155633297	
    Loss 334.62917926936	
    Loss 334.72257474387	
    Loss 334.63102520521	
    Loss 334.79725792173	
    Loss 334.91434499846	
    Loss 334.9043741304	
    Loss 334.87499136416	
    Loss 334.71476813364	
    Loss 334.50104365805	
    Loss 334.64620370792	
    Loss 334.98502226824	
    Loss 334.93154181276	
    Loss 335.03924192921	
    Loss 334.88472706156	
    Loss 335.04693719888	
    Loss 335.21844041042	
    Loss 335.16649888249	
    Loss 334.8942201547	
    Loss 334.82480346754	
    Loss 334.66341288952	
    Loss 334.66985255105	
    Loss 334.78557887123	
    Loss 334.95995493123	
    Loss 334.86044842895	
    Loss 334.89106217439	
    Loss 334.50063357812	
    Loss 334.41572686244	
    Loss 334.85407869027	
    Loss 335.06862281811	
    Loss 334.79282323639	
    Loss 334.76999465775	
    Loss 334.56885748195	
    Loss 334.68359152268	
    Loss 334.64709467131	
    Loss 334.68915648763	
    Loss 334.70721044494	
    Loss 334.88995819511	
    Loss 334.63864051119	
    Loss 334.55388496008	
    Loss 335.17156467533	
    Loss 335.45777911674	
    Loss 335.24332896274	
    Loss 335.44072010868	
    Loss 335.3833752388	
    Loss 335.31855481174	
    Loss 335.11156523273	
    Loss 335.22878266239	
    Loss 334.84425027217	
    Loss 334.81684235654	
    Loss 334.98046639178	
    Loss 335.0144960692	
    Loss 334.93627015757	
    Loss 334.99586947271	
    Loss 334.96207811743	
Epoch 13	
 116152
      0
      0
      0
      0
      0
      0
      0
  15656
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096079145423647	
Grad norm	6.0985507948342	
    Loss 334.75537112645	
    Loss 334.76779414971	
    Loss 335.00775071373	
    Loss 335.01676332482	
    Loss 335.12399236083	
    Loss 335.3678418418	
    Loss 335.15832579859	
    Loss 334.73916376722	
    Loss 334.80926064025	
    Loss 335.01433824144	
    Loss 334.96286765636	
    Loss 335.17602087553	
    Loss 335.14280407762	
    Loss 334.81111381742	
    Loss 334.75195507746	
    Loss 334.85048672079	
    Loss 335.05622175808	
    Loss 335.02154891572	
    Loss 334.62917361487	
    Loss 334.72256872596	
    Loss 334.63101925238	
    Loss 334.79725195847	
    Loss 334.9143386688	
    Loss 334.90436820763	
    Loss 334.87498558088	
    Loss 334.7147628617	
    Loss 334.5010381374	
    Loss 334.64619790844	
    Loss 334.98501584611	
    Loss 334.93153696736	
    Loss 335.03923755994	
    Loss 334.88472215142	
    Loss 335.04693252642	
    Loss 335.21843571623	
    Loss 335.16649370731	
    Loss 334.89421546754	
    Loss 334.82479894408	
    Loss 334.66340788545	
    Loss 334.6698476202	
    Loss 334.78557316472	
    Loss 334.95995014752	
    Loss 334.86044426991	
    Loss 334.8910581644	
    Loss 334.50062960734	
    Loss 334.41572270498	
    Loss 334.85407506892	
    Loss 335.06861919574	
    Loss 334.79281969425	
    Loss 334.76999126787	
    Loss 334.56885410962	
    Loss 334.68358814199	
    Loss 334.64709134147	
    Loss 334.68915350992	
    Loss 334.70720801038	
    Loss 334.88995614471	
    Loss 334.63863891156	
    Loss 334.55388304829	
    Loss 335.17156244943	
    Loss 335.45777746563	
    Loss 335.24332770553	
    Loss 335.44071846762	
    Loss 335.38337385551	
    Loss 335.31855326611	
    Loss 335.11156362635	
    Loss 335.22878126707	
    Loss 334.84424917239	
    Loss 334.81684102005	
    Loss 334.98046516973	
    Loss 335.01449429751	
    Loss 334.93626878864	
    Loss 334.99586811369	
    Loss 334.96207715751	
Epoch 14	
 116152
      0
      0
      0
      0
      0
      0
      0
  15656
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096079145423647	
Grad norm	6.0985416010288	
    Loss 334.75537014591	
    Loss 334.76779346376	
    Loss 335.0077500195	
    Loss 335.01676287849	
    Loss 335.12399155577	
    Loss 335.36784138904	
    Loss 335.15832542721	
    Loss 334.73916304076	
    Loss 334.80925974352	
    Loss 335.01433738323	
    Loss 334.96286689812	
