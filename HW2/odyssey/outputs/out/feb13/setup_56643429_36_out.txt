[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.1	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4300
 5155
 2842
 3965
 2051
 1408
  893
 1838
 1897
 2892
 1078
 1029
 2239
 1457
 4801
 3668
 1349
 5912
 4955
 1371
 5688
 1591
  960
 4557
 1826
 6094
 2860
 6269
 1851
 3299
 1314
 4159
 1999
 2138
 3964
 2458
 3910
 1658
 5667
 1770
 2286
 3937
 1966
 1787
 2700
[torch.DoubleTensor of size 45]

Validation accuracy:	0.022889354212187	
Grad norm	0	
    Loss 11394156.246683	
    Loss 11235349.861104	
    Loss 11078771.293841	
    Loss 10924381.767276	
    Loss 10772145.220618	
    Loss 10622030.183407	
    Loss 10474006.80049	
    Loss 10328046.395199	
    Loss 10184121.370906	
    Loss 10042202.43314	
    Loss 9902260.2427247	
    Loss 9764268.3182244	
    Loss 9628201.4284576	
    Loss 9494030.7058449	
    Loss 9361729.1118537	
    Loss 9231272.041227	
    Loss 9102632.2263467	
    Loss 8975784.7533774	
    Loss 8850705.5462426	
    Loss 8727370.4398824	
    Loss 8605754.6710519	
    Loss 8485833.1430829	
    Loss 8367582.0116871	
    Loss 8250979.3932057	
    Loss 8136002.2116361	
    Loss 8022627.9943453	
    Loss 7910832.8042016	
    Loss 7800596.3918447	
    Loss 7691896.7390356	
    Loss 7584711.5060053	
    Loss 7479019.9483468	
    Loss 7374801.8383017	
    Loss 7272035.8725895	
    Loss 7170702.2026557	
    Loss 7070780.0736962	
    Loss 6972251.1178696	
    Loss 6875095.4992004	
    Loss 6779294.1750013	
    Loss 6684826.1575971	
    Loss 6591676.5024767	
    Loss 6499824.1458984	
    Loss 6409251.6672602	
    Loss 6319942.3731984	
    Loss 6231877.4743798	
    Loss 6145039.5969184	
    Loss 6059411.3225824	
    Loss 5974977.5120974	
    Loss 5891720.8653306	
    Loss 5809623.8513931	
    Loss 5728670.2970212	
    Loss 5648844.4082882	
    Loss 5570130.7346616	
    Loss 5492514.5563778	
    Loss 5415980.8547409	
    Loss 5340513.4824709	
    Loss 5266097.0344107	
    Loss 5192718.1401336	
    Loss 5120361.0954622	
    Loss 5049012.0264323	
    Loss 4978658.1971714	
    Loss 4909285.7379681	
    Loss 4840878.2422535	
    Loss 4773424.3801208	
    Loss 4706911.8488448	
    Loss 4641324.5598775	
    Loss 4576652.055238	
    Loss 4512881.0861684	
    Loss 4449998.8173538	
    Loss 4387992.5034669	
    Loss 4326849.8361917	
    Loss 4266560.1664154	
    Loss 4207109.0636273	
Epoch 2	
 31663
 12248
   961
 14754
 12321
    16
   101
  9384
 28364
 15794
  1994
  1015
   127
    15
   592
  1068
   455
   443
   441
    21
     9
     7
    14
     0
     1
[torch.DoubleTensor of size 25]

Validation accuracy:	0.084395484340859	
Grad norm	1550.3359847134	
    Loss 4189436.552062	
    Loss 4131061.911363	
    Loss 4073499.7706357	
    Loss 4016740.7514721	
    Loss 3960772.5393861	
    Loss 3905583.7347144	
    Loss 3851164.8778404	
    Loss 3797504.0302068	
    Loss 3744591.022005	
    Loss 3692415.2899549	
    Loss 3640966.4117478	
    Loss 3590233.8782085	
    Loss 3540209.5943697	
    Loss 3490882.6687766	
    Loss 3442242.4210808	
    Loss 3394279.9437473	
    Loss 3346985.5693591	
    Loss 3300350.0537382	
    Loss 3254364.3103999	
    Loss 3209019.9640874	
    Loss 3164307.7818318	
    Loss 3120218.3068366	
    Loss 3076742.60058	
    Loss 3033873.0516988	
    Loss 2991600.9967763	
    Loss 2949918.3492275	
    Loss 2908815.9794093	
    Loss 2868286.8035372	
    Loss 2828322.8178218	
    Loss 2788915.1944531	
    Loss 2750056.8400716	
    Loss 2711740.3803875	
    Loss 2673957.8063534	
    Loss 2636701.3541663	
    Loss 2599963.9683901	
    Loss 2563738.8548882	
    Loss 2528018.4795771	
    Loss 2492796.3111078	
    Loss 2458063.9349163	
    Loss 2423816.3671686	
    Loss 2390045.4355052	
    Loss 2356745.3203689	
    Loss 2323909.6808731	
    Loss 2291531.4314781	
    Loss 2259604.1984714	
    Loss 2228121.7277837	
    Loss 2197078.5353198	
    Loss 2166468.0391522	
    Loss 2136283.8527761	
    Loss 2106519.9670535	
    Loss 2077170.7781337	
    Loss 2048230.2903091	
    Loss 2019693.3485555	
    Loss 1991554.7340302	
    Loss 1963807.9932106	
    Loss 1936447.3778988	
    Loss 1909468.2868056	
    Loss 1882864.9030094	
    Loss 1856632.1217374	
    Loss 1830765.1095327	
    Loss 1805259.1729211	
    Loss 1780107.8846101	
    Loss 1755307.1492671	
    Loss 1730852.5115956	
    Loss 1706738.0462729	
    Loss 1682959.8384981	
    Loss 1659513.2621466	
    Loss 1636393.2944576	
    Loss 1613595.4666801	
    Loss 1591115.0618184	
    Loss 1568948.5134206	
    Loss 1547089.7673166	
Epoch 3	
 43514
  8561
    96
 12307
  9168
     0
     1
  6290
 37606
 13640
   364
    75
     1
     0
    23
    76
    37
    18
    31
[torch.DoubleTensor of size 19]

Validation accuracy:	0.092900279193979	
Grad norm	941.91375094763	
    Loss 1540592.0586894	
    Loss 1519129.5541545	
    Loss 1497965.6260253	
    Loss 1477097.0559396	
    Loss 1456519.1721665	
    Loss 1436227.8833463	
    Loss 1416219.7269405	
    Loss 1396490.0567585	
    Loss 1377035.4832952	
    Loss 1357851.8607187	
    Loss 1338935.5648896	
    Loss 1320282.6442532	
    Loss 1301890.1744165	
    Loss 1283754.0802361	
    Loss 1265870.3951207	
    Loss 1248235.8913643	
    Loss 1230847.0661367	
    Loss 1213700.4558226	
    Loss 1196792.590698	
    Loss 1180120.6495767	
    Loss 1163681.147132	
    Loss 1147470.6530173	
    Loss 1131485.7043841	
    Loss 1115723.6206418	
    Loss 1100181.1211312	
    Loss 1084855.336587	
    Loss 1069742.9010048	
    Loss 1054841.3356298	
    Loss 1040147.7160859	
    Loss 1025658.4247027	
    Loss 1011371.0888587	
    Loss 997283.08924773	
    Loss 983391.41637786	
    Loss 969693.05230281	
    Loss 956185.61496682	
    Loss 942866.46353122	
    Loss 929732.80650829	
    Loss 916782.4871822	
    Loss 904012.10186494	
    Loss 891420.09161439	
    Loss 879003.17396034	
    Loss 866759.43073806	
    Loss 854686.50530727	
    Loss 842781.60486454	
    Loss 831042.56841221	
    Loss 819467.1927121	
    Loss 808053.35807695	
    Loss 796798.46569485	
    Loss 785700.3936943	
    Loss 774756.71871387	
    Loss 763965.71176733	
    Loss 753324.77412132	
    Loss 742832.2880864	
    Loss 732486.39376468	
    Loss 722284.51483337	
    Loss 712224.42672581	
    Loss 702304.66724951	
    Loss 692523.20680238	
    Loss 682877.95691541	
    Loss 673367.02690444	
    Loss 663989.05608274	
    Loss 654741.36102244	
    Loss 645622.51384928	
    Loss 636630.90195377	
    Loss 627764.46103599	
    Loss 619021.49691879	
    Loss 610400.59326564	
    Loss 601899.77851974	
    Loss 593517.43198839	
    Loss 585251.75344677	
    Loss 577101.59351564	
    Loss 569064.32139605	
Epoch 4	
 60341
  4430
     2
  7367
  4594
     0
     0
  2350
 44072
  8639
    12
     1
[torch.DoubleTensor of size 12]

Validation accuracy:	0.1009195193008	
Grad norm	573.10372881133	
    Loss 566675.15720444	
    Loss 558783.83772495	
    Loss 551002.2543597	
    Loss 543329.269343	
    Loss 535763.15610057	
    Loss 528302.48563709	
    Loss 520945.84865249	
    Loss 513691.4547995	
    Loss 506538.36204215	
    Loss 499484.83397046	
    Loss 492529.62232362	
    Loss 485671.28923648	
    Loss 478908.71300117	
    Loss 472240.34247196	
    Loss 465664.78951146	
    Loss 459180.86938968	
    Loss 452787.3101798	
    Loss 446482.78978234	
    Loss 440265.91023728	
    Loss 434135.86702884	
    Loss 428091.28012776	
    Loss 422130.95784765	
    Loss 416253.50061925	
    Loss 410457.97722693	
    Loss 404743.12284003	
    Loss 399107.94601699	
    Loss 393551.2256785	
    Loss 388072.14569736	
    Loss 382669.61930062	
    Loss 377342.06025902	
    Loss 372088.76662625	
    Loss 366908.78951965	
    Loss 361801.03610302	
    Loss 356764.31469853	
    Loss 351797.82394492	
    Loss 346900.4891271	
    Loss 342071.32864371	
    Loss 337309.66386599	
    Loss 332614.09183317	
    Loss 327984.19899785	
    Loss 323418.60568885	
    Loss 318916.69631186	
    Loss 314477.62432872	
    Loss 310100.19478794	
    Loss 305783.81613137	
    Loss 301527.74760522	
    Loss 297331.07181255	
    Loss 293192.69762947	
    Loss 289112.05674341	
    Loss 285088.08287138	
    Loss 281120.410163	
    Loss 277207.770007	
    Loss 273349.77603721	
    Loss 269545.74451856	
    Loss 265794.64928787	
    Loss 262095.54826722	
    Loss 258448.08529261	
    Loss 254851.65613796	
    Loss 251305.24841863	
    Loss 247808.07031775	
    Loss 244359.93617611	
    Loss 240959.60759819	
    Loss 237606.622778	
    Loss 234300.39356053	
    Loss 231040.29611446	
    Loss 227825.43319782	
    Loss 224655.56428601	
    Loss 221529.89282732	
    Loss 218447.78061434	
    Loss 215408.53536578	
    Loss 212411.83862024	
    Loss 209456.47826276	
Epoch 5	
 77325
  1092
     0
  2514
  1008
     0
     0
   332
 46399
  3138
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10745933479	
Grad norm	349.52962792599	
    Loss 208577.93259709	
    Loss 205676.39061582	
    Loss 202815.20164525	
    Loss 199993.92137251	
    Loss 197211.94624848	
    Loss 194468.80783825	
    Loss 191763.82454049	
    Loss 189096.32249097	
    Loss 186466.20575737	
    Loss 183872.66432559	
    Loss 181315.2797447	
    Loss 178793.56395003	
    Loss 176307.03232994	
    Loss 173855.07353259	
    Loss 171437.27049894	
    Loss 169053.18992642	
    Loss 166702.36296151	
    Loss 164384.25063792	
    Loss 162098.23683919	
    Loss 159844.23887848	
    Loss 157621.64878702	
    Loss 155430.10037817	
    Loss 153268.97939839	
    Loss 151137.97374365	
    Loss 149036.58431286	
    Loss 146964.4793965	
    Loss 144921.23273635	
    Loss 142906.62817481	
    Loss 140920.23952555	
    Loss 138961.29641253	
    Loss 137029.6784697	
    Loss 135125.00234169	
    Loss 133246.9238577	
    Loss 131394.95598289	
    Loss 129568.81098376	
    Loss 127768.02383172	
    Loss 125992.30413933	
    Loss 124241.44118901	
    Loss 122514.86170787	
    Loss 120812.49693222	
    Loss 119133.73869379	
    Loss 117478.38179399	
    Loss 115846.15538525	
    Loss 114236.46647595	
    Loss 112649.29228347	
    Loss 111084.4270466	
    Loss 109541.38448518	
    Loss 108019.64691525	
    Loss 106519.20226379	
    Loss 105039.51190507	
    Loss 103580.66448878	
    Loss 102141.94719865	
    Loss 100723.36952493	
    Loss 99324.660690089	
    Loss 97945.427052764	
    Loss 96585.191621679	
    Loss 95243.975955129	
    Loss 93921.71346493	
    Loss 92617.781093373	
    Loss 91331.798625569	
    Loss 90063.97953867	
    Loss 88813.661193135	
    Loss 87580.722474529	
    Loss 86364.93952249	
    Loss 85166.230015711	
    Loss 83983.986863845	
    Loss 82818.396926923	
    Loss 81669.115907016	
    Loss 80535.837919542	
    Loss 79418.294452735	
    Loss 78316.444971886	
    Loss 77229.68630159	
Epoch 6	
 90250
    48
     0
   223
    46
     0
     0
     5
 40898
   338
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10714827628065	
Grad norm	213.9934756908	
    Loss 76906.579951397	
    Loss 75839.701266862	
    Loss 74787.687643454	
    Loss 73750.316899184	
    Loss 72727.414721789	
    Loss 71718.85783783	
    Loss 70724.22158189	
    Loss 69743.277924009	
    Loss 68776.203946858	
    Loss 67822.558723832	
    Loss 66882.204833979	
    Loss 65955.01664829	
    Loss 65040.730510889	
    Loss 64139.08972787	
    Loss 63250.041483939	
    Loss 62373.430733855	
    Loss 61509.077383452	
    Loss 60656.723670108	
    Loss 59816.055804283	
    Loss 58987.244666443	
    Loss 58169.964865145	
    Loss 57364.15467219	
    Loss 56569.502347118	
    Loss 55785.91103156	
    Loss 55013.179740474	
    Loss 54251.198651757	
    Loss 53499.837344903	
    Loss 52759.093022737	
    Loss 52028.774696464	
    Loss 51308.45305069	
    Loss 50598.203254198	
    Loss 49897.820694213	
    Loss 49207.261030915	
    Loss 48526.314272976	
    Loss 47854.845022688	
    Loss 47192.63293315	
    Loss 46539.655150047	
    Loss 45895.827483295	
    Loss 45260.938881724	
    Loss 44634.999529916	
    Loss 44017.730354421	
    Loss 43409.037429572	
    Loss 42808.872542235	
    Loss 42216.874043516	
    Loss 41633.217982384	
    Loss 41057.897124236	
    Loss 40490.579594896	
    Loss 39930.967634169	
    Loss 39379.244309978	
    Loss 38835.082757293	
    Loss 38298.708865727	
    Loss 37769.657162179	
    Loss 37248.046319242	
    Loss 36733.748811616	
    Loss 36226.63822234	
    Loss 35726.405684689	
    Loss 35233.194339593	
    Loss 34747.141331804	
    Loss 34267.766154146	
    Loss 33794.844132882	
    Loss 33328.715004935	
    Loss 32868.954742041	
    Loss 32415.557889875	
    Loss 31968.427026087	
    Loss 31527.686688832	
    Loss 31092.836389954	
    Loss 30664.216216025	
    Loss 30241.657760175	
    Loss 29824.962373375	
    Loss 29414.026462979	
    Loss 29008.896099547	
    Loss 28609.242456374	
Epoch 7	
 100690
      0
      0
      3
      0
      0
      0
      0
  31111
      4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.1033548798252	
Grad norm	131.82836889062	
    Loss 28490.37159963	
    Loss 28098.082673234	
    Loss 27711.299455883	
    Loss 27329.864158284	
    Loss 26953.768150771	
    Loss 26583.011880421	
    Loss 26217.258708558	
    Loss 25856.465129068	
    Loss 25500.884505377	
    Loss 25150.230281497	
    Loss 24804.453220144	
    Loss 24463.56897841	
    Loss 24127.38761064	
    Loss 23795.789012819	
    Loss 23468.857475666	
    Loss 23146.540906437	
    Loss 22828.762726415	
    Loss 22515.363643874	
    Loss 22206.15038976	
    Loss 21901.38024815	
    Loss 21600.830676437	
    Loss 21304.552678265	
    Loss 21012.353306253	
    Loss 20724.207705406	
    Loss 20440.035067509	
    Loss 20159.794087771	
    Loss 19883.465419546	
    Loss 19611.119998106	
    Loss 19342.649207145	
    Loss 19077.773788375	
    Loss 18816.630108088	
    Loss 18559.06158622	
    Loss 18305.148236154	
    Loss 18054.792787537	
    Loss 17807.894977217	
    Loss 17564.336345349	
    Loss 17324.200218717	
    Loss 17087.421322321	
    Loss 16853.956095243	
    Loss 16623.815055348	
    Loss 16396.867889281	
    Loss 16173.037883549	
    Loss 15952.365079853	
    Loss 15734.579264909	
    Loss 15519.922019648	
    Loss 15308.46109611	
    Loss 15099.919622504	
    Loss 14894.085480317	
    Loss 14691.207699793	
    Loss 14491.048260729	
    Loss 14293.864923856	
    Loss 14099.30952099	
    Loss 13907.518834997	
    Loss 13718.413480338	
    Loss 13531.985381162	
    Loss 13347.986106995	
    Loss 13166.593191905	
    Loss 12988.021489868	
    Loss 12811.843593281	
    Loss 12637.894558817	
    Loss 12466.54787517	
    Loss 12297.483322887	
    Loss 12130.732082053	
    Loss 11966.238799375	
    Loss 11804.208990728	
    Loss 11644.181622001	
    Loss 11486.550478153	
    Loss 11331.21685945	
    Loss 11178.013302385	
    Loss 11026.902135086	
    Loss 10877.952169155	
    Loss 10730.965058769	
Epoch 8	
 110791
      0
      0
      0
      0
      0
      0
      0
  21017
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099584243748483	
Grad norm	82.021411519958	
    Loss 10687.193748426	
    Loss 10542.948017834	
    Loss 10400.773613312	
    Loss 10260.526663648	
    Loss 10122.263648925	
    Loss 9986.0291789892	
    Loss 9851.5148761585	
    Loss 9718.7572929512	
    Loss 9588.0231332012	
    Loss 9459.0944338861	
    Loss 9331.9447225606	
    Loss 9206.6465021912	
    Loss 9083.0326411079	
    Loss 8961.0372453546	
    Loss 8840.7966723653	
    Loss 8722.2966806558	
    Loss 8605.4975553648	
    Loss 8490.2730621362	
    Loss 8376.4799990089	
    Loss 8264.4034102067	
    Loss 8153.8578015936	
    Loss 8044.9362315729	
    Loss 7937.493761901	
    Loss 7831.5277129903	
    Loss 7727.0081090804	
    Loss 7623.9106918124	
    Loss 7522.2584712556	
    Loss 7422.1464697909	
    Loss 7323.494082742	
    Loss 7226.0920643692	
    Loss 7130.092925479	
    Loss 7035.3470206902	
    Loss 6941.9882736461	
    Loss 6849.9676439189	
    Loss 6759.1855149819	
    Loss 6669.5678669416	
    Loss 6581.2417105587	
    Loss 6494.1320368714	
    Loss 6408.2773367233	
    Loss 6323.6712437712	
    Loss 6240.254949259	
    Loss 6157.94373578	
    Loss 6076.8139682743	
    Loss 5996.6329039955	
    Loss 5917.6613106582	
    Loss 5839.9959879026	
    Loss 5763.3794179996	
    Loss 5687.6336424189	
    Loss 5613.0283725679	
    Loss 5539.3650288919	
    Loss 5466.9016678496	
    Loss 5395.3492128888	
    Loss 5324.8379268617	
    Loss 5255.30349887	
    Loss 5186.7932693355	
    Loss 5119.0802605343	
    Loss 5052.3485511272	
    Loss 4986.8428844179	
    Loss 4922.1561844058	
    Loss 4858.1470663426	
    Loss 4795.1936152849	
    Loss 4733.0233486294	
    Loss 4671.6773847237	
    Loss 4611.1132748542	
    Loss 4551.5693652973	
    Loss 4492.5993097911	
    Loss 4434.6149753364	
    Loss 4377.5460510144	
    Loss 4321.2318589229	
    Loss 4265.6620587659	
    Loss 4210.9084646642	
    Loss 4156.8381957717	
Epoch 9	
 117692
      0
      0
      0
      0
      0
      0
      0
  14116
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096283988832241	
Grad norm	51.836488395747	
    Loss 4140.6824977633	
    Loss 4087.6408848832	
    Loss 4035.4118701903	
    Loss 3983.8510122525	
    Loss 3933.0411105772	
    Loss 3883.0423927885	
    Loss 3833.5543325594	
    Loss 3784.6501603043	
    Loss 3736.5926330796	
    Loss 3689.196391898	
    Loss 3642.4372139673	
    Loss 3596.4114756899	
    Loss 3550.9579733334	
    Loss 3506.0332678463	
    Loss 3461.7937829514	
    Loss 3418.2389906256	
    Loss 3375.3421529354	
    Loss 3332.9872522217	
    Loss 3291.0530185754	
    Loss 3249.8323119892	
    Loss 3209.152737168	
    Loss 3169.1232409426	
    Loss 3129.6190577386	
    Loss 3090.6430394204	
    Loss 3052.188068638	
    Loss 3014.2310788196	
    Loss 2976.8116741386	
    Loss 2940.031818783	
    Loss 2903.8199143549	
    Loss 2868.001397082	
    Loss 2832.730917213	
    Loss 2797.8548811733	
    Loss 2763.5318040791	
    Loss 2729.7346451383	
    Loss 2696.3567189535	
    Loss 2663.3444705637	
    Loss 2630.8437242973	
    Loss 2598.7658418323	
    Loss 2567.1916687463	
    Loss 2536.0985675355	
    Loss 2505.4641843695	
    Loss 2475.1921626806	
    Loss 2445.3746003109	
    Loss 2415.7947969777	
    Loss 2386.7175375648	
    Loss 2358.2512983934	
    Loss 2330.1452047661	
    Loss 2302.235396369	
    Loss 2274.7969912593	
    Loss 2247.6500699837	
    Loss 2221.0458215607	
    Loss 2194.7265333408	
    Loss 2168.8122018443	
    Loss 2143.242479557	
    Loss 2118.0922067353	
    Loss 2093.1418154242	
    Loss 2068.5742037511	
    Loss 2044.6457205447	
    Loss 2020.9574177021	
    Loss 1997.3775924698	
    Loss 1974.2812061241	
    Loss 1951.4190274683	
    Loss 1928.8337211266	
    Loss 1906.4861086032	
    Loss 1884.6284882049	
    Loss 1862.8200002598	
    Loss 1841.4781424259	
    Loss 1820.5445879176	
    Loss 1799.8584542282	
    Loss 1779.421268563	
    Loss 1759.3027830975	
    Loss 1739.4045564014	
Epoch 10	
 121244
      0
      0
      0
      0
      0
      0
      0
  10564
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094796977421704	
Grad norm	33.555282748456	
    Loss 1733.4039640355	
    Loss 1713.8970977295	
    Loss 1694.7424068804	
    Loss 1675.7918782296	
    Loss 1657.1393877855	
    Loss 1638.8505762702	
    Loss 1620.6262761437	
    Loss 1602.5574889633	
    Loss 1584.9002213006	
    Loss 1567.485672864	
    Loss 1550.286595076	
    Loss 1533.4099403376	
    Loss 1516.6952599109	
    Loss 1500.1089571732	
    Loss 1483.8153589803	
    Loss 1467.8187596161	
    Loss 1452.0968956169	
    Loss 1436.5363338712	
    Loss 1421.026024531	
    Loss 1405.860258749	
    Loss 1390.8711178426	
    Loss 1376.1736745954	
    Loss 1361.6520522009	
