[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	0.1	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 6487
 1226
 1849
 1357
 2451
 1151
 1800
 3317
 1531
 4205
 4101
 3498
 4823
 1889
 3540
 5043
 2271
 6804
 3916
 1391
  867
 3316
 2792
 4973
  711
 3205
 3925
 3585
 4245
 4382
 1433
 1143
 2889
 2561
 1971
 2011
 1095
  943
 4702
 2127
 3913
 1987
 3132
 5684
 1566
[torch.DoubleTensor of size 45]

Validation accuracy:	0.026227543092984	
Grad norm	0	
    Loss 22792366.161935	
    Loss 22161660.75974	
    Loss 21548438.244382	
    Loss 20952196.773782	
    Loss 20372454.773474	
    Loss 19808754.508686	
    Loss 19260650.440635	
    Loss 18727715.610805	
    Loss 18209527.994186	
    Loss 17705676.12151	
    Loss 17215764.551287	
    Loss 16739406.237049	
    Loss 16276233.900668	
    Loss 15825879.212426	
    Loss 15387983.431467	
    Loss 14962204.77254	
    Loss 14548207.582732	
    Loss 14145665.86499	
    Loss 13754261.546478	
    Loss 13373690.032029	
    Loss 13003647.380985	
    Loss 12643845.111422	
    Loss 12293998.344637	
    Loss 11953831.691582	
    Loss 11623077.804495	
    Loss 11301478.57958	
    Loss 10988776.197492	
    Loss 10684725.328574	
    Loss 10389091.699004	
    Loss 10101634.479725	
    Loss 9822134.0343596	
    Loss 9550367.3173119	
    Loss 9286120.4758002	
    Loss 9029184.6187989	
    Loss 8779357.2972159	
    Loss 8536444.1060992	
    Loss 8300254.3387123	
    Loss 8070600.2593262	
    Loss 7847298.7216382	
    Loss 7630174.9245816	
    Loss 7419062.1635386	
    Loss 7213787.5590937	
    Loss 7014195.9801432	
    Loss 6820126.3753548	
    Loss 6631427.0646438	
    Loss 6447948.1092337	
    Loss 6269546.8341582	
    Loss 6096082.4457791	
    Loss 5927417.7773815	
    Loss 5763420.3237891	
    Loss 5603957.8456956	
    Loss 5448909.1947512	
    Loss 5298151.4050749	
    Loss 5151565.6744795	
    Loss 5009035.4146003	
    Loss 4870448.3423127	
    Loss 4735697.8922171	
    Loss 4604674.6984612	
    Loss 4477277.0663921	
    Loss 4353404.915921	
    Loss 4232959.9512807	
    Loss 4115847.8395978	
    Loss 4001975.7765958	
    Loss 3891254.4959661	
    Loss 3783597.620393	
    Loss 3678919.7285832	
    Loss 3577139.0373971	
    Loss 3478174.0943494	
    Loss 3381946.080325	
    Loss 3288381.2769165	
    Loss 3197404.3079156	
    Loss 3108946.0707907	
Epoch 2	
 43012
  9160
   101
 13447
  9130
     0
     4
  6295
 36484
 13397
   382
   147
     1
     0
    14
   149
    22
    33
    30
[torch.DoubleTensor of size 19]

Validation accuracy:	0.099857368293275	
Grad norm	1879.7534036334	
    Loss 3082888.8827777	
    Loss 2997599.9095268	
    Loss 2914670.1051279	
    Loss 2834035.6394141	
    Loss 2755632.2409624	
    Loss 2679397.5375106	
    Loss 2605272.5648558	
    Loss 2533199.2192109	
    Loss 2463120.2726034	
    Loss 2394979.5553613	
    Loss 2328724.2484407	
    Loss 2264301.7153177	
    Loss 2201662.6804358	
    Loss 2140757.4465895	
    Loss 2081536.5515568	
    Loss 2023954.2608994	
    Loss 1967965.5568209	
    Loss 1913525.7224335	
    Loss 1860591.7428938	
    Loss 1809123.5669215	
    Loss 1759078.6367039	
    Loss 1710419.1057778	
    Loss 1663105.6628709	
    Loss 1617101.1513919	
    Loss 1572369.8916104	
    Loss 1528876.7917936	
    Loss 1486586.3498097	
    Loss 1445466.1499982	
    Loss 1405485.0982361	
    Loss 1366608.8409698	
    Loss 1328809.2407578	
    Loss 1292054.9721725	
    Loss 1256318.0169632	
    Loss 1221569.9799488	
    Loss 1187782.7826848	
    Loss 1154930.7457163	
    Loss 1122988.3836752	
    Loss 1091929.789505	
    Loss 1061730.1406385	
    Loss 1032365.6219195	
    Loss 1003814.5418483	
    Loss 976052.51704647	
    Loss 949059.60600536	
    Loss 922813.00909487	
    Loss 897292.91220114	
    Loss 872479.1570344	
    Loss 848352.12505624	
    Loss 824892.31519035	
    Loss 802081.66218937	
    Loss 779902.12759099	
    Loss 758335.80299521	
    Loss 737366.44003394	
    Loss 716977.61491448	
    Loss 697152.92196042	
    Loss 677876.67208933	
    Loss 659133.34583311	
    Loss 640909.23442204	
    Loss 623189.65133024	
    Loss 605960.24866819	
    Loss 589207.25165172	
    Loss 572917.91617375	
    Loss 557079.3538564	
    Loss 541678.62447959	
    Loss 526703.81187876	
    Loss 512144.06977202	
    Loss 497986.54551492	
    Loss 484221.30278054	
    Loss 470836.94737807	
    Loss 457822.45345241	
    Loss 445168.24355502	
    Loss 432863.74548514	
    Loss 420900.12166401	
Epoch 3	
 75353
  1226
     0
  3283
  1058
     0
     0
   366
 47368
  3154
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11342255401797	
Grad norm	694.37101300078	
    Loss 417375.70426521	
    Loss 405840.79448958	
    Loss 394625.01992393	
    Loss 383719.66945678	
    Loss 373116.0562775	
    Loss 362805.82521503	
    Loss 352780.58676403	
    Loss 343032.72612322	
    Loss 333554.95802616	
    Loss 324339.1228742	
    Loss 315378.33516706	
    Loss 306665.50135502	
    Loss 298193.76199438	
    Loss 289956.46552969	
    Loss 281946.90947139	
    Loss 274159.07657473	
    Loss 266586.95674036	
    Loss 259224.12411713	
    Loss 252064.56842818	
    Loss 245103.72599718	
    Loss 238335.06945397	
    Loss 231754.10790325	
    Loss 225355.05425399	
    Loss 219132.96648247	
    Loss 213083.10974423	
    Loss 207200.6854124	
    Loss 201480.7928057	
    Loss 195919.44430712	
    Loss 190512.50565538	
    Loss 185254.50740009	
    Loss 180142.35954525	
    Loss 175171.20034801	
    Loss 170337.83346574	
    Loss 165638.40270317	
    Loss 161068.67347671	
    Loss 156625.32874829	
    Loss 152305.21924732	
    Loss 148104.53985562	
    Loss 144020.10238839	
    Loss 140048.52163939	
    Loss 136187.21377416	
    Loss 132432.3082006	
    Loss 128781.65831595	
    Loss 125231.55990652	
    Loss 121779.90443836	
    Loss 118424.23681821	
    Loss 115161.33947762	
    Loss 111988.24879559	
    Loss 108903.11735846	
    Loss 105903.19715329	
    Loss 102986.41794373	
    Loss 100150.29506795	
    Loss 97392.79270403	
    Loss 94711.515143123	
    Loss 92104.505179803	
    Loss 89569.252250713	
    Loss 87104.386811101	
    Loss 84708.349065552	
    Loss 82378.420182864	
    Loss 80112.471731513	
    Loss 77909.499618089	
    Loss 75767.373569279	
    Loss 73684.292834232	
    Loss 71658.589016018	
    Loss 69689.542919077	
    Loss 67774.292503416	
    Loss 65912.517214573	
    Loss 64102.457136763	
    Loss 62342.228123043	
    Loss 60630.709947803	
    Loss 58966.401258688	
    Loss 57348.237337991	
Epoch 4	
 100563
      0
      0
      9
      0
      0
      0
      0
  31232
      4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10538055353241	
Grad norm	258.62058271901	
    Loss 56871.318093286	
    Loss 55311.193319421	
    Loss 53794.382772195	
    Loss 52319.468454472	
    Loss 50885.396468734	
    Loss 49491.209890368	
    Loss 48135.147906112	
    Loss 46816.489149954	
    Loss 45534.68823259	
    Loss 44288.237983477	
    Loss 43076.263360517	
    Loss 41897.980412205	
    Loss 40752.145492386	
    Loss 39637.869104746	
    Loss 38554.451890444	
    Loss 37501.175745887	
    Loss 36477.206158819	
    Loss 35481.401759056	
    Loss 34512.716833809	
    Loss 33571.267670681	
    Loss 32655.649496774	
    Loss 31765.662573755	
    Loss 30900.150257049	
    Loss 30058.540564169	
    Loss 29240.212136887	
    Loss 28444.468921966	
    Loss 27670.697101618	
    Loss 26918.621374768	
    Loss 26187.617691348	
    Loss 25476.446951971	
    Loss 24785.158653968	
    Loss 24112.606543275	
    Loss 23458.866712283	
    Loss 22823.393059866	
    Loss 22205.280464003	
    Loss 21604.123266186	
    Loss 21019.798994223	
    Loss 20451.515320448	
    Loss 19899.101628284	
    Loss 19361.949724056	
    Loss 18839.87440822	
    Loss 18331.945238721	
    Loss 17838.241076838	
    Loss 17357.795623939	
    Loss 16890.823816053	
    Loss 16437.309849103	
    Loss 15996.226413566	
    Loss 15566.856273877	
    Loss 15149.554000115	
    Loss 14743.631242242	
    Loss 14349.238405213	
    Loss 13965.640925973	
    Loss 13592.7327776	
    Loss 13230.075256454	
    Loss 12877.592676505	
    Loss 12534.508972664	
    Loss 12201.045657227	
    Loss 11877.521984194	
    Loss 11562.738020491	
    Loss 11256.136647075	
    Loss 10958.350306389	
    Loss 10668.63179625	
    Loss 10386.779663725	
    Loss 10112.462020902	
    Loss 9846.2776712277	
    Loss 9586.7916102783	
    Loss 9334.9429047521	
    Loss 9090.3221992831	
    Loss 8852.2869949324	
    Loss 8620.7776725812	
    Loss 8395.6341618221	
    Loss 8176.7013479617	
Epoch 5	
 121133
      0
      0
      0
      0
      0
      0
      0
  10675
[torch.DoubleTensor of size 9]

Validation accuracy:	0.095062515173586	
Grad norm	98.446708452527	
    Loss 8111.9855275701	
    Loss 7900.9404610454	
    Loss 7695.9296683729	
    Loss 7496.4766492264	
    Loss 7302.5847959845	
    Loss 7114.3365866887	
    Loss 6930.7966663543	
    Loss 6752.1980543026	
    Loss 6578.8942694876	
    Loss 6410.3370791489	
    Loss 6246.3974035463	
    Loss 6087.1986126187	
    Loss 5932.2097748689	
    Loss 5781.3026469473	
    Loss 5634.6652421388	
    Loss 5492.2731843981	
    Loss 5353.9548120222	
    Loss 5219.3199814831	
    Loss 5087.9813480299	
    Loss 4960.6470230268	
    Loss 4836.6951047109	
    Loss 4716.4281099196	
    Loss 4599.3455532373	
    Loss 4485.4716717806	
    Loss 4374.7207702242	
    Loss 4266.9488540104	
    Loss 4162.1766594938	
    Loss 4060.581120987	
    Loss 3961.969717619	
    Loss 3865.7867349447	
    Loss 3772.4221356113	
    Loss 3681.2777605475	
    Loss 3592.8445052241	
    Loss 3507.0173231854	
    Loss 3423.379621501	
    Loss 3341.8804897463	
    Loss 3262.8124170283	
    Loss 3185.7934946924	
    Loss 3111.0957849551	
    Loss 3038.4915026102	
    Loss 2968.0533625439	
    Loss 2899.3084081374	
    Loss 2832.5728658052	
    Loss 2767.3050026854	
    Loss 2704.0083049204	
    Loss 2643.0127618021	
    Loss 2583.5805315914	
    Loss 2525.2982942353	
    Loss 2468.8199986205	
    Loss 2413.7364146275	
    Loss 2360.5244755976	
    Loss 2308.6476653577	
    Loss 2258.2557131978	
    Loss 2209.1952470811	
    Loss 2161.6550457001	
    Loss 2115.0844409119	
    Loss 2069.8923632785	
    Loss 2026.6886005483	
    Loss 1984.4600326597	
    Loss 1942.8605330774	
    Loss 1902.7606909836	
    Loss 1863.5767867328	
    Loss 1825.3536107791	
    Loss 1787.9262338534	
    Loss 1752.0507774286	
    Loss 1716.5128384718	
    Loss 1682.4068218823	
    Loss 1649.5229525234	
    Loss 1617.3897632897	
    Loss 1586.060226211	
    Loss 1555.597082941	
    Loss 1525.9179862136	
Epoch 6	
 127642
      0
      0
      0
      0
      0
      0
      0
   4166
[torch.DoubleTensor of size 9]

Validation accuracy:	0.093901735858218	
Grad norm	39.609217540286	
    Loss 1516.9645035229	
    Loss 1488.3822532782	
    Loss 1460.8007736434	
    Loss 1433.8561981212	
    Loss 1407.7019502051	
    Loss 1382.5755545816	
    Loss 1357.6275821022	
    Loss 1333.2288565196	
    Loss 1309.8488606299	
    Loss 1287.0902552402	
    Loss 1264.9015454418	
    Loss 1243.5480350965	
    Loss 1222.5785836586	
    Loss 1201.9613665226	
    Loss 1182.0311723511	
    Loss 1162.8498895497	
    Loss 1144.3218232182	
    Loss 1126.1711840765	
    Loss 1108.0947121776	
    Loss 1090.8663284272	
    Loss 1074.0030095188	
    Loss 1057.8468666427	
    Loss 1041.9962670704	
    Loss 1026.5579697123	
    Loss 1011.5116167472	
    Loss 996.78643489276	
    Loss 982.50956998872	
    Loss 968.90003806248	
    Loss 955.80925852371	
    Loss 942.81298111062	
    Loss 930.31781083514	
    Loss 917.81564144129	
    Loss 905.84389682146	
    Loss 894.35409991553	
    Loss 883.00973420302	
    Loss 871.79386067585	
    Loss 861.05809144552	
    Loss 850.4757924192	
    Loss 840.39160857456	
    Loss 830.63106123115	
    Loss 821.27796314358	
    Loss 811.94238838372	
    Loss 802.9504117007	
    Loss 793.83665322674	
    Loss 785.13465236247	
    Loss 777.22682272092	
    Loss 769.4122679693	
    Loss 761.31856658046	
    Loss 753.64031654693	
    Loss 746.00672681783	
    Loss 738.94818713287	
    Loss 731.94118723795	
    Loss 725.16831945221	
    Loss 718.5216929417	
    Loss 712.23001104896	
    Loss 705.76947472187	
    Loss 699.56458751497	
    Loss 694.27719844355	
    Loss 688.91297254308	
    Loss 683.15369415212	
    Loss 677.90801956041	
    Loss 672.60633779707	
    Loss 667.33691150446	
    Loss 661.95341229191	
    Loss 657.22495712143	
    Loss 651.97701099581	
    Loss 647.31946127421	
    Loss 643.07578086104	
    Loss 638.79888594483	
    Loss 634.54559408815	
    Loss 630.42373280192	
    Loss 626.34301026074	
Epoch 7	
 128352
      0
      0
      0
      0
      0
      0
      0
   3456
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094478332119446	
Grad norm	18.063089157929	
    Loss 624.93528991704	
    Loss 621.0297905642	
    Loss 617.4473956829	
    Loss 613.83448347258	
    Loss 610.36695946679	
    Loss 607.30852786084	
    Loss 603.81204703275	
    Loss 600.27037304694	
    Loss 597.16642063681	
    Loss 594.1308235208	
    Loss 591.11435599909	
    Loss 588.40835968082	
    Loss 585.56671043354	
    Loss 582.56814368331	
    Loss 579.77685631825	
    Loss 577.26389057114	
    Loss 574.93878376426	
    Loss 572.54572057495	
    Loss 569.79090607256	
    Loss 567.452187257	
    Loss 565.07746993812	
    Loss 563.00383470918	
    Loss 560.84654673187	
    Loss 558.72420292107	
    Loss 556.62275423676	
    Loss 554.4807518105	
    Loss 552.44743488574	
    Loss 550.74025091509	
    Loss 549.21165450046	
    Loss 547.46901224397	
    Loss 545.9107869762	
    Loss 544.04669955451	
    Loss 542.41710411099	
    Loss 540.97987875063	
    Loss 539.41446100289	
    Loss 537.70344702356	
    Loss 536.2078141292	
    Loss 534.60794291357	
    Loss 533.26294938893	
    Loss 532.00628611686	
    Loss 530.91474981973	
    Loss 529.61713566239	
    Loss 528.4330177946	
    Loss 526.91397688037	
    Loss 525.59449173988	
    Loss 524.86609048804	
    Loss 524.03207933186	
    Loss 522.72551018792	
    Loss 521.64638857356	
    Loss 520.42950122979	
    Loss 519.61578254857	
    Loss 518.67854710628	
    Loss 517.80425343781	
    Loss 516.89348241715	
    Loss 516.1820281557	
    Loss 515.14829724516	
    Loss 514.21547027182	
    Loss 514.05691809974	
    Loss 513.67857874438	
    Loss 512.76577535067	
    Loss 512.23434872699	
    Loss 511.51385594006	
    Loss 510.70194829276	
    Loss 509.65367967217	
    Loss 509.13666719303	
    Loss 507.98544781271	
    Loss 507.30996414477	
    Loss 506.94051163604	
    Loss 506.4337738238	
    Loss 505.84290267118	
    Loss 505.28722556241	
    Loss 504.66906185386	
Epoch 8	
 128387
      0
      0
      0
      0
      0
      0
      0
   3421
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094713522699684	
Grad norm	10.258183204861	
    Loss 504.28321371111	
    Loss 503.71440591523	
    Loss 503.37817815712	
    Loss 502.92057599345	
    Loss 502.52126110401	
    Loss 502.44933638892	
    Loss 501.8545840467	
    Loss 501.13413404994	
    Loss 500.77184879546	
    Loss 500.40484747896	
    Loss 499.98129382405	
    Loss 499.79848756045	
    Loss 499.40898178431	
    Loss 498.79199103419	
    Loss 498.319114856	
    Loss 498.06179317002	
    Loss 497.92841270229	
    Loss 497.6674690721	
    Loss 496.98571423205	
    Loss 496.65977696758	
    Loss 496.24622019002	
    Loss 496.07762376555	
    Loss 495.77278712915	
    Loss 495.4521525011	
    Loss 495.10161478531	
    Loss 494.66077022334	
    Loss 494.28471379038	
    Loss 494.18793751738	
    Loss 494.2213464042	
    Loss 494.0015199045	
    Loss 493.9221171125	
    Loss 493.49729252268	
    Loss 493.26660139724	
    Loss 493.18826194831	
    Loss 492.94574215198	
    Loss 492.51974352193	
    Loss 492.27309086474	
    Loss 491.88683166283	
    Loss 491.72374068494	
    Loss 491.618622967	
    Loss 491.64426894009	
    Loss 491.43467228661	
    Loss 491.3057529039	
    Loss 490.81372245478	
    Loss 490.49209342041	
    Loss 490.73435494768	
    Loss 490.84408475073	
    Loss 490.45507436588	
    Loss 490.2679743527	
    Loss 489.9185064428	
    Loss 489.95021172449	
    Loss 489.83420587075	
    Loss 489.75727514404	
    Loss 489.6219321565	
    Loss 489.66560567328	
    Loss 489.36643979735	
    Loss 489.14623402532	
    Loss 489.68149351532	
    Loss 489.97739292936	
    Loss 489.71965242746	
    Loss 489.82578971048	
    Loss 489.72430775115	
    Loss 489.51534797348	
    Loss 489.05383839328	
    Loss 489.10592031052	
    Loss 488.50871582873	
    Loss 488.37142264814	
    Loss 488.52612264084	
    Loss 488.5301805251	
    Loss 488.43471762752	
    Loss 488.36256976433	
    Loss 488.21280764277	
Epoch 9	
 128363
      0
      0
      0
      0
      0
      0
      0
   3445
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094781803835882	
Grad norm	7.5091751061439	
    Loss 487.96562016264	
    Loss 487.84772623878	
    Loss 487.95057791368	
    Loss 487.91957847285	
    Loss 487.93513844076	
    Loss 488.26775175507	
    Loss 488.06555677833	
    Loss 487.72674827706	
    Loss 487.73498809943	
    Loss 487.72923939407	
    Loss 487.65627428076	
    Loss 487.81506748092	
    Loss 487.75729977485	
    Loss 487.46188718937	
    Loss 487.30266110469	
    Loss 487.35083302816	
    Loss 487.5139491221	
    Loss 487.54166732474	
    Loss 487.14053229571	
    Loss 487.08639037905	
    Loss 486.93862311612	
    Loss 487.02779465252	
    Loss 486.97362754419	
    Loss 486.89690259231	
    Loss 486.78317751963	
    Loss 486.57211246518	
    Loss 486.42065799133	
    Loss 486.54187578838	
    Loss 486.78587420744	
    Loss 486.77224873766	
    Loss 486.89268188102	
    Loss 486.66265726353	
    Loss 486.62119371166	
    Loss 486.72635304365	
    Loss 486.66283206636	
    Loss 486.41041168823	
    Loss 486.33240352563	
    Loss 486.1098071425	
    Loss 486.10651353373	
    Loss 486.15761912117	
    Loss 486.3342719894	
    Loss 486.27212268127	
    Loss 486.28560831987	
    Loss 485.9323943691	
    Loss 485.74547787297	
    Loss 486.11887646202	
    Loss 486.35609189934	
    Loss 486.0910253363	
    Loss 486.02436410447	
    Loss 485.79203708071	
    Loss 485.93835759935	
    Loss 485.93350881886	
    Loss 485.96425504498	
    Loss 485.93363739706	
    Loss 486.07957215722	
    Loss 485.87994924892	
    Loss 485.75596002286	
    Loss 486.38508916	
    Loss 486.77212328924	
    Loss 486.60281027987	
    Loss 486.79513823603	
    Loss 486.77714329158	
    Loss 486.649730143	
    Loss 486.26772794656	
    Loss 486.39658449828	
    Loss 485.87427446953	
    Loss 485.80961742826	
    Loss 486.03525666277	
    Loss 486.10871099749	
    Loss 486.08026563525	
    Loss 486.0739373004	
    Loss 485.98755370652	
Epoch 10	
 128346
      0
      0
      0
      0
      0
      0
      0
   3462
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094796977421704	
Grad norm	6.5744395157759	
    Loss 485.75928007954	
    Loss 485.70222393982	
    Loss 485.86446071835	
    Loss 485.8910976852	
    Loss 485.96272949784	
    Loss 486.3502732654	
    Loss 486.20119964609	
    Loss 485.91402888262	
    Loss 485.97227282841	
    Loss 486.01549048467	
    Loss 485.98989506457	
    Loss 486.19499845636	
    Loss 486.18212297322	
    Loss 485.93000538635	
    Loss 485.81322207878	
    Loss 485.9028585142	
    Loss 486.10609468574	
    Loss 486.17295547814	
    Loss 485.80984988149	
    Loss 485.79229962827	
    Loss 485.68067413216	
    Loss 485.8047466774	
    Loss 485.78452381228	
    Loss 485.74086492533	
    Loss 485.65916191218	
    Loss 485.47905667492	
    Loss 485.35814400402	
    Loss 485.50890672502	
    Loss 485.78113989254	
    Loss 485.79548393041	
    Loss 485.94287627132	
    Loss 485.7392448537	
    Loss 485.72338141606	
    Loss 485.85324917361	
    Loss 485.81396059975	
    Loss 485.58493077213	
    Loss 485.52963148036	
    Loss 485.32899426226	
    Loss 485.34728779139	
    Loss 485.41968998025	
    Loss 485.61672961721	
    Loss 485.5746238631	
    Loss 485.60725564003	
    Loss 485.27278126974	
    Loss 485.10399201218	
    Loss 485.49507240185	
    Loss 485.74947095587	
    Loss 485.50111107847	
    Loss 485.45066002759	
    Loss 485.23410930395	
    Loss 485.39602574462	
    Loss 485.40623825517	
    Loss 485.45148553379	
    Loss 485.43497127168	
    Loss 485.59478172856	
    Loss 485.40868769316	
    Loss 485.29765058992	
    Loss 485.93948388458	
    Loss 486.33881963579	
    Loss 486.18140194356	
    Loss 486.38536648018	
    Loss 486.37857439312	
    Loss 486.26218320881	
    Loss 485.89098931157	
    Loss 486.0301563576	
    Loss 485.51796082373	
    Loss 485.46306904511	
    Loss 485.69831830416	
    Loss 485.7812704984	
    Loss 485.76189273829	
    Loss 485.76461701081	
    Loss 485.68681510327	
Epoch 11	
 128346
      0
      0
      0
      0
      0
      0
      0
   3462
[torch.DoubleTensor of size 9]

Validation accuracy:	0.094865258557902	
Grad norm	6.269384449823	
    Loss 485.46115682046	
    Loss 485.41227274314	
    Loss 485.58253863132	
    Loss 485.61694802913	
    Loss 485.6961494896	
    Loss 486.09120098598	
    Loss 485.94932127367	
    Loss 485.66914184943	
    Loss 485.73410869273	
    Loss 485.78398572993	
    Loss 485.76477797466	
    Loss 485.97618118773	
    Loss 485.96938567225	
    Loss 485.7230509881	
    Loss 485.61201416768	
    Loss 485.70731143603	
    Loss 485.91598017962	
    Loss 485.98817122057	
    Loss 485.63023410637	
    Loss 485.61756895943	
    Loss 485.51090208376	
    Loss 485.63970898443	
    Loss 485.62409158421	
    Loss 485.58493210716	
    Loss 485.50755593328	
    Loss 485.33159345126	
    Loss 485.2148716326	
    Loss 485.36965141151	
    Loss 485.64561302659	
    Loss 485.66376817666	
    Loss 485.81478088632	
    Loss 485.61473553126	
    Loss 485.60233710339	
    Loss 485.73550621404	
    Loss 485.69950253677	
    Loss 485.47360390766	
