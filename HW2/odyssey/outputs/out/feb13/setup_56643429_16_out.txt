[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.25	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
  574
  949
 2652
 2246
 3463
 4035
 4078
 4233
 1710
 2188
 3743
 3871
 1108
 4345
 4002
 1772
 2627
 2784
 1616
 2916
 5311
  800
 1553
 1473
 5042
 2228
 5413
  917
 4318
 3634
 4319
 4378
 3209
 1520
 2987
 3151
 1851
 2522
 4805
 2640
 4134
 2831
 2372
 3273
 2215
[torch.DoubleTensor of size 45]

Validation accuracy:	0.016455753823744	
Grad norm	0	
    Loss 2278467.3579474	
    Loss 2262489.0725262	
    Loss 2246632.7483931	
    Loss 2230888.4087227	
    Loss 2215254.3000673	
    Loss 2199729.5669218	
    Loss 2184315.2275221	
    Loss 2169009.4901975	
    Loss 2153810.9862619	
    Loss 2138718.7258283	
    Loss 2123733.5777439	
    Loss 2108852.9971644	
    Loss 2094077.3836311	
    Loss 2079406.6131957	
    Loss 2064837.9992661	
    Loss 2050371.277686	
    Loss 2036007.2941569	
    Loss 2021742.7673634	
    Loss 2007578.8877595	
    Loss 1993515.0052026	
    Loss 1979549.3463871	
    Loss 1965681.549969	
    Loss 1951911.0007388	
    Loss 1938236.9960682	
    Loss 1924659.053548	
    Loss 1911177.5862671	
    Loss 1897790.0368361	
    Loss 1884495.7946581	
    Loss 1871294.4150841	
    Loss 1858185.6445969	
    Loss 1845170.4153682	
    Loss 1832245.0642584	
    Loss 1819411.2121261	
    Loss 1806667.0867173	
    Loss 1794011.8998134	
    Loss 1781445.4277182	
    Loss 1768967.6404714	
    Loss 1756577.9163456	
    Loss 1744274.3353098	
    Loss 1732057.4009008	
    Loss 1719926.6876093	
    Loss 1707880.4983866	
    Loss 1695918.5984317	
    Loss 1684041.0516543	
    Loss 1672246.3018123	
    Loss 1660533.3647843	
    Loss 1648903.5624962	
    Loss 1637355.2695426	
    Loss 1625887.1931336	
    Loss 1614500.0226329	
    Loss 1603192.1605755	
    Loss 1591963.5087982	
    Loss 1580814.3773091	
    Loss 1569742.6642268	
    Loss 1558749.3251683	
    Loss 1547832.557662	
    Loss 1536993.2961876	
    Loss 1526228.7249519	
    Loss 1515539.3054914	
    Loss 1504926.0258758	
    Loss 1494386.0940763	
    Loss 1483920.4245408	
    Loss 1473528.936977	
    Loss 1463208.8796166	
    Loss 1452961.0511769	
    Loss 1442786.133038	
    Loss 1432682.8069602	
    Loss 1422649.8136468	
    Loss 1412687.0055432	
    Loss 1402794.0542407	
    Loss 1392969.7511076	
    Loss 1383215.3591602	
Epoch 2	
 30405
 11943
  1463
 14924
 11910
    47
   355
  9988
 27364
 14482
  2149
  1499
   348
    51
   851
  1435
   674
   744
   901
    80
    65
    44
    86
[torch.DoubleTensor of size 23]

Validation accuracy:	0.089258618596747	
Grad norm	401.93288292948	
    Loss 1380302.7433525	
    Loss 1370636.7982929	
    Loss 1361038.0203943	
    Loss 1351507.3510756	
    Loss 1342042.6881198	
    Loss 1332644.1583908	
    Loss 1323312.2471851	
    Loss 1314045.9029013	
    Loss 1304843.8255918	
    Loss 1295705.9809379	
    Loss 1286632.4921917	
    Loss 1277622.2390654	
    Loss 1268675.4322548	
    Loss 1259792.2459066	
    Loss 1250970.669172	
    Loss 1242210.4138512	
    Loss 1233512.2392289	
    Loss 1224873.9971291	
    Loss 1216296.7279005	
    Loss 1207779.8719524	
    Loss 1199322.4289361	
    Loss 1190924.0515989	
    Loss 1182584.4008335	
    Loss 1174303.1384474	
    Loss 1166080.1786361	
    Loss 1157915.690469	
    Loss 1149807.730859	
    Loss 1141756.1417598	
    Loss 1133760.590702	
    Loss 1125821.1102249	
    Loss 1117938.5019514	
    Loss 1110110.1627469	
    Loss 1102337.1734018	
    Loss 1094618.4593838	
    Loss 1086953.3991775	
    Loss 1079342.3702431	
    Loss 1071784.9371802	
    Loss 1064280.6952455	
    Loss 1056828.6308118	
    Loss 1049428.8282515	
    Loss 1042081.2830802	
    Loss 1034784.9068842	
    Loss 1027539.664373	
    Loss 1020345.6088234	
    Loss 1013201.4607623	
    Loss 1006106.5788042	
    Loss 999062.44048339	
    Loss 992067.55629377	
    Loss 985121.06670313	
    Loss 978223.76720118	
    Loss 971374.26838136	
    Loss 964572.63473906	
    Loss 957819.25268431	
    Loss 951112.67265352	
    Loss 944453.58689303	
    Loss 937840.95670749	
    Loss 931275.1513598	
    Loss 924754.55468738	
    Loss 918279.47840991	
    Loss 911850.5608846	
    Loss 905465.96003075	
    Loss 899126.39628567	
    Loss 892831.88700375	
    Loss 886580.32726516	
    Loss 880372.56238288	
    Loss 874209.036055	
    Loss 868088.8647859	
    Loss 862011.22457097	
    Loss 855976.08821163	
    Loss 849983.23587702	
    Loss 844031.98643442	
    Loss 838123.10574336	
Epoch 3	
 38806
 10796
   698
 14150
 10690
     6
    69
  8521
 30727
 13739
  1107
   641
    95
     6
   300
   619
   269
   236
   302
    15
     4
     3
     9
[torch.DoubleTensor of size 23]

Validation accuracy:	0.094660415149308	
Grad norm	313.94947948798	
    Loss 836358.73755386	
    Loss 830503.3358494	
    Loss 824688.54392169	
    Loss 818915.18315474	
    Loss 813181.57584921	
    Loss 807488.16715639	
    Loss 801835.04716864	
    Loss 796221.65960285	
    Loss 790647.0826458	
    Loss 785111.45976604	
    Loss 779614.81523691	
    Loss 774156.44789888	
    Loss 768736.51926387	
    Loss 763355.20396099	
    Loss 758011.21425688	
    Loss 752704.24645347	
    Loss 747435.0213038	
    Loss 742201.87274367	
    Loss 737005.72844819	
    Loss 731846.1548183	
    Loss 726722.60231092	
    Loss 721634.79988863	
    Loss 716582.56080609	
    Loss 711565.63858507	
    Loss 706584.10196821	
    Loss 701638.07253158	
    Loss 696726.21019395	
    Loss 691848.42916635	
    Loss 687004.53693881	
    Loss 682194.60700852	
    Loss 677419.2409131	
    Loss 672676.64072567	
    Loss 667967.63321012	
    Loss 663291.5001277	
    Loss 658647.79469336	
    Loss 654036.93622218	
    Loss 649458.47108985	
    Loss 644912.22630097	
    Loss 640397.60004271	
    Loss 635914.61609272	
    Loss 631463.3249846	
    Loss 627042.96143303	
    Loss 622653.61051045	
    Loss 618295.32823263	
    Loss 613967.22119438	
    Loss 609668.88958787	
    Loss 605401.41587336	
    Loss 601163.70626543	
    Loss 596955.25887598	
    Loss 592776.66973976	
    Loss 588627.00755552	
    Loss 584506.28264563	
    Loss 580414.86239263	
    Loss 576351.71751776	
    Loss 572317.38641544	
    Loss 568311.22865966	
    Loss 564333.41345667	
    Loss 560382.97848541	
    Loss 556460.13830515	
    Loss 552565.24139769	
    Loss 548697.15932583	
    Loss 544856.36933145	
    Loss 541042.96989242	
    Loss 537255.39558146	
    Loss 533494.43000094	
    Loss 529760.26935593	
    Loss 526052.35958864	
    Loss 522370.23483921	
    Loss 518713.85561105	
    Loss 515083.09168379	
    Loss 511477.51406857	
    Loss 507897.65966729	
Epoch 4	
 46871
  9372
   205
 12554
  9101
     0
     9
  6688
 33916
 11969
   472
   211
    12
     0
    67
   179
    50
    55
    74
     3
[torch.DoubleTensor of size 20]

Validation accuracy:	0.098817977664482	
Grad norm	245.49800534159	
    Loss 506828.69886433	
    Loss 503281.17930573	
    Loss 499758.24696903	
    Loss 496260.46935397	
    Loss 492786.69066976	
    Loss 489337.32938773	
    Loss 485912.3184438	
    Loss 482511.38116251	
    Loss 479133.92202038	
    Loss 475780.08856965	
    Loss 472449.88993592	
    Loss 469142.85636967	
    Loss 465859.10673831	
    Loss 462598.78689244	
    Loss 459361.09581733	
    Loss 456145.77297425	
    Loss 452953.4206604	
    Loss 449782.75935781	
    Loss 446634.56889657	
    Loss 443508.51569492	
    Loss 440404.30692223	
    Loss 437321.7394722	
    Loss 434260.73064453	
    Loss 431221.07467164	
    Loss 428202.89020226	
    Loss 425206.28146088	
    Loss 422230.3340369	
    Loss 419275.00316933	
    Loss 416340.16686813	
    Loss 413425.89367449	
    Loss 410532.64400674	
    Loss 407659.16566622	
    Loss 404806.0960568	
    Loss 401972.94948606	
    Loss 399159.40013726	
    Loss 396365.80426297	
    Loss 393591.78102151	
    Loss 390837.29431314	
    Loss 388101.97056268	
    Loss 385385.82427893	
    Loss 382688.91626121	
    Loss 380010.68188485	
    Loss 377351.26194728	
    Loss 374710.68158743	
    Loss 372088.38286255	
    Loss 369484.0994242	
    Loss 366898.54889329	
    Loss 364330.97631097	
    Loss 361781.11018621	
    Loss 359249.3568673	
    Loss 356735.14252166	
    Loss 354238.41477487	
    Loss 351759.50504212	
    Loss 349297.65604117	
    Loss 346853.31167318	
    Loss 344426.03090507	
    Loss 342015.92849453	
    Loss 339622.41803381	
    Loss 337245.62950418	
    Loss 334885.74904266	
    Loss 332542.10268777	
    Loss 330214.98374806	
    Loss 327904.53962857	
    Loss 325609.60865694	
    Loss 323330.86420577	
    Loss 321068.34531488	
    Loss 318821.72232081	
    Loss 316590.7527875	
    Loss 314375.38092805	
    Loss 312175.53638714	
    Loss 309990.91729928	
    Loss 307821.94058763	
Epoch 5	
 54907
  7244
    43
 10616
  6955
     0
     0
  4580
 37413
  9816
   131
    35
     2
     0
    11
    35
     3
     8
     9
[torch.DoubleTensor of size 19]

Validation accuracy:	0.100221534353	
Grad norm	192.2352096463	
    Loss 307174.24079171	
    Loss 305024.81078973	
    Loss 302890.28010522	
    Loss 300771.00164128	
    Loss 298666.22802764	
    Loss 296576.27536929	
    Loss 294501.03023047	
    Loss 292440.36678489	
    Loss 290393.92563616	
    Loss 288361.81740476	
    Loss 286344.04067488	
    Loss 284340.29065608	
    Loss 282350.63786918	
    Loss 280375.19593142	
    Loss 278413.48291611	
    Loss 276465.29037471	
    Loss 274531.09183602	
    Loss 272609.91061027	
    Loss 270702.38352865	
    Loss 268808.26031712	
    Loss 266927.38631445	
    Loss 265059.61089261	
    Loss 263204.91235919	
    Loss 261363.11686565	
    Loss 259534.34936696	
    Loss 257718.69756789	
    Loss 255915.53813278	
    Loss 254124.85818223	
    Loss 252346.57155714	
    Loss 250580.7275897	
    Loss 248827.68632429	
    Loss 247086.56754334	
    Loss 245357.85904626	
    Loss 243641.22955664	
    Loss 241936.43636463	
    Loss 240243.75858085	
    Loss 238562.90273564	
    Loss 236893.89730484	
    Loss 235236.51268707	
    Loss 233590.75614137	
    Loss 231956.69205653	
    Loss 230333.88849657	
    Loss 228722.50595262	
    Loss 227122.52989814	
    Loss 225533.65155079	
    Loss 223955.68246767	
    Loss 222389.07441189	
    Loss 220833.31275974	
    Loss 219288.27185645	
    Loss 217754.21391431	
    Loss 216230.80764648	
    Loss 214717.95948421	
    Loss 213215.96128838	
    Loss 211724.23956773	
    Loss 210243.16869442	
    Loss 208772.41527067	
    Loss 207312.08574128	
    Loss 205861.82302711	
    Loss 204421.68289645	
    Loss 202991.76860307	
    Loss 201571.6874502	
    Loss 200161.60821496	
    Loss 198761.69658124	
    Loss 197371.08841602	
    Loss 195990.33911705	
    Loss 194619.40059278	
    Loss 193258.08929155	
    Loss 191906.29275012	
    Loss 190563.94857669	
    Loss 189231.02092301	
    Loss 187907.28382304	
    Loss 186593.07560613	
Epoch 6	
 62148
  5094
     5
  8276
  4820
     0
     0
  2828
 40953
  7652
    19
     5
     0
     0
     0
     6
     0
     1
     1
[torch.DoubleTensor of size 19]

Validation accuracy:	0.1027251760136	
Grad norm	150.78872389135	
    Loss 186200.59514096	
    Loss 184898.19810899	
    Loss 183604.83800657	
    Loss 182320.71265622	
    Loss 181045.36386994	
    Loss 179779.003587	
    Loss 178521.51724065	
    Loss 177272.85919717	
    Loss 176032.83063053	
    Loss 174801.50165472	
    Loss 173578.85960207	
    Loss 172364.71635841	
    Loss 171159.10342319	
    Loss 169962.1046194	
    Loss 168773.44294067	
    Loss 167592.95684492	
    Loss 166421.00762178	
    Loss 165256.84758006	
    Loss 164100.98999023	
    Loss 162953.2534159	
    Loss 161813.55412197	
    Loss 160681.78097685	
    Loss 159557.94497482	
    Loss 158441.89916601	
    Loss 157333.75885576	
    Loss 156233.59384654	
    Loss 155140.97852445	
    Loss 154055.92416115	
    Loss 152978.36334149	
    Loss 151908.32446418	
    Loss 150846.09431899	
    Loss 149791.0472977	
    Loss 148743.5557685	
    Loss 147703.39119394	
    Loss 146670.36782548	
    Loss 145644.69613387	
    Loss 144626.16178174	
    Loss 143614.8170254	
    Loss 142610.52601648	
    Loss 141613.28836413	
    Loss 140623.16836556	
    Loss 139639.82638104	
    Loss 138663.42415017	
    Loss 137693.91098613	
    Loss 136731.15156986	
    Loss 135775.00564322	
    Loss 134825.74526085	
    Loss 133883.01478025	
    Loss 132946.78131186	
    Loss 132017.20359275	
    Loss 131094.10682444	
    Loss 130177.37733133	
    Loss 129267.26746268	
    Loss 128363.33307778	
    Loss 127465.89231825	
    Loss 126574.67542379	
    Loss 125689.79168927	
    Loss 124811.02964078	
    Loss 123938.38480046	
    Loss 123071.91800729	
    Loss 122211.41253142	
    Loss 121356.95441114	
    Loss 120508.70805086	
    Loss 119666.02824231	
    Loss 118829.36276911	
    Loss 117998.61981906	
    Loss 117173.70932276	
    Loss 116354.59103983	
    Loss 115541.20295021	
    Loss 114733.5272836	
    Loss 113931.3917088	
    Loss 113135.07213914	
Epoch 7	
 69850
  3071
     0
  5672
  2775
     0
     0
  1369
 43635
  5432
     4
[torch.DoubleTensor of size 11]

Validation accuracy:	0.10651098567613	
Grad norm	118.53641316026	
    Loss 112897.22709363	
    Loss 112108.03973732	
    Loss 111324.34069005	
    Loss 110546.2220247	
    Loss 109773.42168158	
    Loss 109006.06713391	
    Loss 108244.05712013	
    Loss 107487.38776219	
    Loss 106735.96720609	
    Loss 105989.83134225	
    Loss 105248.95671847	
    Loss 104513.23552228	
    Loss 103782.67224731	
    Loss 103057.32525211	
    Loss 102337.0481945	
    Loss 101621.71707526	
    Loss 100911.60356774	
    Loss 100206.13042779	
    Loss 99505.708807888	
    Loss 98810.213481483	
    Loss 98119.594423524	
    Loss 97433.770323561	
    Loss 96752.768393039	
    Loss 96076.462662248	
    Loss 95404.955186283	
    Loss 94738.298195495	
    Loss 94076.199868665	
    Loss 93418.689572906	
    Loss 92765.709189347	
    Loss 92117.269580757	
    Loss 91473.602894458	
    Loss 90834.258864582	
    Loss 90199.52200277	
    Loss 89569.233056029	
    Loss 88943.245179124	
    Loss 88321.713018902	
    Loss 87704.488067415	
    Loss 87091.625344814	
    Loss 86483.050228703	
    Loss 85878.755893142	
    Loss 85278.80329117	
    Loss 84682.92002078	
    Loss 84091.260002333	
    Loss 83503.743472634	
    Loss 82920.352626443	
    Loss 82340.977842838	
    Loss 81765.773544658	
    Loss 81194.486415952	
    Loss 80627.14262225	
    Loss 80063.828009184	
    Loss 79504.46638627	
    Loss 78948.938440662	
    Loss 78397.45953821	
    Loss 77849.67946067	
    Loss 77305.869308846	
    Loss 76765.800488313	
    Loss 76229.585471924	
    Loss 75697.107733875	
    Loss 75168.316598881	
    Loss 74643.250147022	
    Loss 74121.804154548	
    Loss 73604.009225389	
    Loss 73090.018442462	
    Loss 72579.347828367	
    Loss 72072.354111216	
    Loss 71568.926574994	
    Loss 71069.034496954	
    Loss 70572.675728441	
    Loss 70079.791996936	
    Loss 69590.374134066	
    Loss 69104.291912301	
    Loss 68621.76743361	
Epoch 8	
 76580
  1498
     0
  3407
  1351
     0
     0
   535
 45090
  3347
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10653374605487	
Grad norm	93.438069440324	
    Loss 68477.62030462	
    Loss 67999.398266841	
    Loss 67524.516140386	
    Loss 67052.995996675	
    Loss 66584.705626839	
    Loss 66119.710377581	
    Loss 65657.924299041	
    Loss 65199.364074828	
    Loss 64744.009004904	
    Loss 64291.868666454	
    Loss 63842.909801823	
    Loss 63397.079197838	
    Loss 62954.364060589	
    Loss 62514.801743026	
    Loss 62078.329595704	
    Loss 61644.851696742	
    Loss 61214.571039833	
    Loss 60787.036639626	
    Loss 60362.577099117	
    Loss 59941.112620809	
    Loss 59522.606670125	
    Loss 59107.00070735	
    Loss 58694.329113825	
    Loss 58284.481251076	
    Loss 57877.546951071	
    Loss 57473.562539726	
    Loss 57072.326759612	
    Loss 56673.882463858	
    Loss 56278.175451856	
    Loss 55885.203614412	
    Loss 55495.157582537	
    Loss 55107.708989978	
    Loss 54723.07575859	
    Loss 54341.14622974	
    Loss 53961.8001237	
    Loss 53585.148236455	
    Loss 53211.095035663	
    Loss 52839.68826945	
    Loss 52470.893343504	
    Loss 52104.698729442	
    Loss 51741.159800589	
    Loss 51380.05483099	
    Loss 51021.526105303	
    Loss 50665.471790463	
    Loss 50311.95285231	
    Loss 49960.879339225	
    Loss 49612.3298315	
    Loss 49266.116288377	
    Loss 48922.30240269	
    Loss 48580.922873074	
    Loss 48241.962955917	
    Loss 47905.305977584	
    Loss 47571.134388196	
    Loss 47239.167725111	
    Loss 46909.637520794	
    Loss 46582.343908806	
    Loss 46257.400427029	
    Loss 45934.753705239	
    Loss 45614.318763322	
    Loss 45296.120617015	
    Loss 44980.128664167	
    Loss 44666.335740354	
    Loss 44354.879663038	
    Loss 44045.395497597	
    Loss 43738.165987167	
    Loss 43433.074926688	
    Loss 43130.129077814	
    Loss 42829.344913385	
    Loss 42530.668990261	
    Loss 42234.095549413	
    Loss 41939.529013844	
    Loss 41647.144525758	
Epoch 9	
 81947
   608
     0
  1681
   464
     0
     0
   159
 45272
  1677
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10755796309784	
Grad norm	73.906409150464	
    Loss 41559.774344825	
    Loss 41269.981230382	
    Loss 40982.227250656	
    Loss 40696.488902438	
    Loss 40412.718767223	
    Loss 40130.938788614	
    Loss 39851.076179348	
    Loss 39573.156481963	
    Loss 39297.206675708	
    Loss 39023.21706127	
    Loss 38751.145619772	
    Loss 38480.976297351	
    Loss 38212.685732924	
    Loss 37946.294273426	
    Loss 37681.793503974	
    Loss 37419.107603915	
    Loss 37158.388018324	
    Loss 36899.277294965	
    Loss 36642.0381797	
    Loss 36386.627318417	
    Loss 36133.010996545	
    Loss 35881.148277015	
    Loss 35631.0759509	
    Loss 35382.694116566	
    Loss 35136.08267113	
    Loss 34891.264288853	
    Loss 34648.099291869	
    Loss 34406.640539315	
    Loss 34166.834760362	
    Loss 33928.670942968	
    Loss 33692.307760258	
    Loss 33457.502531426	
    Loss 33224.422762069	
    Loss 32992.990079995	
    Loss 32763.102417798	
    Loss 32534.836622869	
    Loss 32308.139177225	
    Loss 32083.046455175	
    Loss 31859.550137667	
    Loss 31637.635537717	
    Loss 31417.351370422	
    Loss 31198.514914682	
    Loss 30981.255904884	
    Loss 30765.456257821	
    Loss 30551.22959195	
    Loss 30338.498642284	
    Loss 30127.293417494	
    Loss 29917.467506076	
    Loss 29709.108428746	
    Loss 29502.214844301	
    Loss 29296.810158107	
    Loss 29092.784295983	
    Loss 28890.291476017	
    Loss 28689.103853695	
    Loss 28489.419909029	
    Loss 28291.060948181	
    Loss 28094.13959984	
    Loss 27898.645205356	
    Loss 27704.467716586	
    Loss 27511.624106124	
    Loss 27320.1330142	
    Loss 27129.961053387	
    Loss 26941.230259375	
    Loss 26753.663901594	
    Loss 26567.48600745	
    Loss 26382.581650903	
    Loss 26198.981150849	
    Loss 26016.708911109	
    Loss 25835.716229262	
    Loss 25655.998015684	
    Loss 25477.487598387	
    Loss 25300.319687239	
Epoch 10	
 86269
   183
     0
   621
   104
     0
     0
    35
 43890
   706
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10659444039815	
Grad norm	58.70727783422	
    Loss 25247.355358183	
    Loss 25071.744471275	
    Loss 24897.385143911	
    Loss 24724.224451977	
    Loss 24552.269959925	
    Loss 24381.514159268	
    Loss 24211.894426809	
    Loss 24043.438833554	
    Loss 23876.206679002	
    Loss 23710.174554087	
    Loss 23545.292695729	
    Loss 23381.570566813	
    Loss 23218.978552576	
    Loss 23057.523395548	
    Loss 22897.232366454	
    Loss 22738.043627682	
    Loss 22580.070352978	
    Loss 22423.024516657	
    Loss 22267.116795802	
    Loss 22112.332903689	
    Loss 21958.636634426	
    Loss 21806.000795516	
    Loss 21654.462217408	
    Loss 21503.928312486	
    Loss 21354.471337828	
    Loss 21206.102638587	
    Loss 21058.724736048	
    Loss 20912.397887303	
    Loss 20767.068192183	
    Loss 20622.71884285	
    Loss 20479.483729714	
    Loss 20337.181074516	
    Loss 20195.94023405	
    Loss 20055.706473396	
    Loss 19916.390294034	
    Loss 19778.042425394	
    Loss 19640.642437874	
    Loss 19504.214355067	
    Loss 19368.767220622	
    Loss 19234.284344672	
    Loss 19100.807629134	
    Loss 18968.184709734	
    Loss 18836.53336066	
    Loss 18705.723743976	
    Loss 18575.904714048	
    Loss 18447.007232908	
    Loss 18319.030263898	
    Loss 18191.854061537	
    Loss 18065.581295447	
    Loss 17940.185011863	
    Loss 17815.712337111	
    Loss 17692.06114209	
    Loss 17569.36206471	
    Loss 17447.427364317	
    Loss 17326.428218782	
    Loss 17206.202078605	
    Loss 17086.859402121	
    Loss 16968.41884802	
    Loss 16850.752174792	
    Loss 16733.871085801	
    Loss 16617.827820883	
    Loss 16502.570447933	
    Loss 16388.20641924	
    Loss 16274.524608808	
    Loss 16161.703114561	
    Loss 16049.631318092	
    Loss 15938.354257823	
    Loss 15827.89867229	
    Loss 15718.220016552	
    Loss 15609.312506911	
    Loss 15501.133540816	
    Loss 15393.783663668	
Epoch 11	
 89732
    35
     0
   173
    14
     0
     0
     5
 41633
   216
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10594197620782	
Grad norm	46.880759610081	
    Loss 15361.66945339	
    Loss 15255.251553585	
    Loss 15149.608394122	
    Loss 15044.668114067	
    Loss 14940.47386246	
    Loss 14836.998467184	
    Loss 14734.186774365	
    Loss 14632.066122604	
    Loss 14530.718015348	
    Loss 14430.109486473	
    Loss 14330.184013459	
    Loss 14230.968808596	
    Loss 14132.430655654	
    Loss 14034.565513636	
    Loss 13937.424532806	
    Loss 13840.955415233	
    Loss 13745.243071564	
    Loss 13650.051153858	
    Loss 13555.549430327	
    Loss 13461.746703235	
    Loss 13368.601937629	
    Loss 13276.098743501	
    Loss 13184.272922493	
    Loss 13093.036779187	
    Loss 13002.456843828	
    Loss 12912.535266136	
    Loss 12823.203778913	
    Loss 12734.52809642	
    Loss 12646.452970042	
    Loss 12558.957980075	
    Loss 12472.157738647	
    Loss 12385.914204227	
    Loss 12300.327889246	
    Loss 12215.361051598	
    Loss 12130.932936377	
    Loss 12047.074234381	
    Loss 11963.79047161	
    Loss 11881.093914159	
    Loss 11799.005171547	
    Loss 11717.506344081	
    Loss 11636.632952441	
    Loss 11556.25637691	
    Loss 11476.483708628	
    Loss 11397.176697316	
    Loss 11318.507863733	
    Loss 11240.413548154	
    Loss 11162.87284637	
    Loss 11085.783218372	
    Loss 11009.257007735	
    Loss 10933.248859537	
    Loss 10857.820732757	
    Loss 10782.878368786	
    Loss 10708.533081633	
    Loss 10634.62790326	
    Loss 10561.311721494	
    Loss 10488.43453157	
    Loss 10416.104014339	
    Loss 10344.359231152	
    Loss 10273.058760074	
    Loss 10202.210675784	
    Loss 10131.890558314	
    Loss 10062.033140813	
    Loss 9992.7331999915	
    Loss 9923.828587287	
    Loss 9855.4618046197	
    Loss 9787.5275417629	
    Loss 9720.0799734433	
    Loss 9653.1454435575	
    Loss 9586.682840773	
    Loss 9520.685089754	
    Loss 9455.1294764751	
    Loss 9390.0881851641	
Epoch 12	
 92507
     3
     0
    45
     0
     0
     0
     1
 39193
    59
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10525916484584	
Grad norm	37.680025873623	
    Loss 9370.6096660309	
    Loss 9306.1228200566	
    Loss 9242.1220727941	
    Loss 9178.5233606951	
    Loss 9115.392907324	
    Loss 9052.6905150187	
    Loss 8990.3663931225	
    Loss 8928.4456294115	
    Loss 8867.0251960933	
    Loss 8806.0655618607	
    Loss 8745.5043859567	
    Loss 8685.3814070722	
    Loss 8625.6613907	
    Loss 8566.331810658	
    Loss 8507.4600750335	
    Loss 8449.0003066543	
    Loss 8391.0165883416	
    Loss 8333.3111550229	
    Loss 8276.0217819095	
    Loss 8219.1755511903	
    Loss 8162.7258747209	
    Loss 8106.6648835321	
    Loss 8051.0269684134	
    Loss 7995.7276631724	
    Loss 7940.8292586461	
    Loss 7886.3265474433	
    Loss 7832.1717166865	
    Loss 7778.4345041182	
    Loss 7725.0580660422	
    Loss 7672.0198282233	
    Loss 7619.4194025616	
    Loss 7567.1504462422	
    Loss 7515.2916808687	
    Loss 7463.8176995748	
    Loss 7412.6538819538	
    Loss 7361.815562851	
    Loss 7311.3285750437	
    Loss 7261.1947211595	
    Loss 7211.4423848743	
    Loss 7162.0529703418	
    Loss 7113.0563513645	
    Loss 7064.3425081313	
    Loss 7016.0093557487	
    Loss 6967.9126622146	
    Loss 6920.2409309288	
    Loss 6872.9340561539	
    Loss 6825.9583710653	
    Loss 6779.2225413551	
    Loss 6732.8451353522	
    Loss 6686.7674254168	
    Loss 6641.0607309509	
    Loss 6595.6379093855	
    Loss 6550.5946526381	
    Loss 6505.797711768	
    Loss 6461.3779667656	
    Loss 6417.1952498734	
    Loss 6373.3543173369	
    Loss 6329.9086100306	
    Loss 6286.7071513797	
    Loss 6243.7554199534	
    Loss 6201.1454697926	
    Loss 6158.8023071959	
    Loss 6116.8100981206	
    Loss 6075.0433848724	
    Loss 6033.6172497849	
    Loss 5992.4316558905	
    Loss 5951.5464035573	
    Loss 5910.9862717259	
    Loss 5870.7128861775	
    Loss 5830.7176989643	
    Loss 5790.9942767967	
    Loss 5751.5916089524	
Epoch 13	
 94858
     1
     0
     7
     0
     0
     0
     0
 36931
    11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10383284777859	
Grad norm	30.524371848551	
    Loss 5739.7708614928	
    Loss 5700.694748397	
    Loss 5661.930041634	
    Loss 5623.3847777398	
    Loss 5585.1402214431	
    Loss 5547.1475620954	
    Loss 5509.3608249979	
    Loss 5471.8022993986	
    Loss 5434.5798824514	
    Loss 5397.6495827273	
    Loss 5360.9441240502	
    Loss 5324.5122108104	
    Loss 5288.317514447	
    Loss 5252.3408719379	
    Loss 5216.6609484315	
    Loss 5181.2361470735	
    Loss 5146.1152027795	
    Loss 5111.1292454793	
    Loss 5076.3911132741	
    Loss 5041.9423239168	
    Loss 5007.7308457102	
    Loss 4973.7555401032	
    Loss 4940.049351319	
    Loss 4906.5299226196	
    Loss 4873.2564225913	
    Loss 4840.2178141457	
    Loss 4807.3807811772	
    Loss 4774.81802829	
    Loss 4742.4711256234	
    Loss 4710.316277489	
    Loss 4678.4414404272	
    Loss 4646.763429138	
    Loss 4615.3445167393	
    Loss 4584.168319229	
    Loss 4553.164555303	
    Loss 4522.3368006191	
    Loss 4491.7267418875	
    Loss 4461.3272400783	
    Loss 4431.1719524208	
    Loss 4401.2419067257	
    Loss 4371.5621685173	
    Loss 4342.037283514	
    Loss 4312.757484602	
    Loss 4283.5743025467	
    Loss 4254.6871065047	
    Loss 4226.0381709299	
    Loss 4197.5855813981	
    Loss 4169.2453837364	
    Loss 4141.1401933787	
    Loss 4113.2014557544	
    Loss 4085.5064711022	
    Loss 4057.9742067718	
    Loss 4030.6878474764	
    Loss 4003.5324822745	
    Loss 3976.6245724907	
    Loss 3949.8316897432	
    Loss 3923.255795876	
    Loss 3896.960585397	
    Loss 3870.7881415167	
    Loss 3844.7420339161	
    Loss 3818.926108755	
    Loss 3793.2581891145	
    Loss 3767.8137807154	
    Loss 3742.4949948319	
    Loss 3717.3958791884	
    Loss 3692.4211588234	
    Loss 3667.634062754	
    Loss 3643.0574484316	
    Loss 3618.6550744199	
    Loss 3594.4169558652	
    Loss 3570.349866179	
    Loss 3546.4840527443	
Epoch 14	
 96642
     1
     0
     0
     0
     0
     0
     0
 35160
     5
[torch.DoubleTensor of size 10]

Validation accuracy:	0.1026568948774	
Grad norm	24.96152924567	
    Loss 3539.3043721605	
    Loss 3515.6276615575	
    Loss 3492.1566611765	
    Loss 3468.7942542216	
    Loss 3445.6315883271	
    Loss 3422.6141424219	
    Loss 3399.6986535938	
    Loss 3376.904583692	
    Loss 3354.3476442041	
    Loss 3331.9808418232	
    Loss 3309.7327692828	
    Loss 3287.6586330547	
    Loss 3265.7215378086	
    Loss 3243.8971545079	
    Loss 3222.2720680768	
    Loss 3200.8075145194	
    Loss 3179.5409062524	
    Loss 3158.3250764561	
    Loss 3137.2534719342	
    Loss 3116.3789878444	
    Loss 3095.6446515446	
    Loss 3075.054639297	
    Loss 3054.6405735542	
    Loss 3034.3214616277	
    Loss 3014.1541769681	
    Loss 2994.1230917887	
    Loss 2974.2052892836	
    Loss 2954.4756649127	
    Loss 2934.8743365819	
    Loss 2915.376863023	
    Loss 2896.062164026	
    Loss 2876.8640874835	
    Loss 2857.8324996641	
    Loss 2838.9576561118	
    Loss 2820.1723680826	
    Loss 2801.4712101609	
    Loss 2782.9082249887	
    Loss 2764.4688336079	
    Loss 2746.1903027446	
    Loss 2728.0534761786	
    Loss 2710.079434944	
    Loss 2692.1841183682	
    Loss 2674.4514395696	
    Loss 2656.7299344231	
    Loss 2639.2264163197	
    Loss 2621.8847768955	
    Loss 2604.6577304803	
    Loss 2587.4662444466	
    Loss 2570.4356005624	
    Loss 2553.4899486581	
    Loss 2536.7104704555	
    Loss 2520.0212113152	
    Loss 2503.4954129828	
    Loss 2487.0323722886	
    Loss 2470.7370897362	
    Loss 2454.4832277486	
    Loss 2438.3702278574	
    Loss 2422.4691739399	
    Loss 2406.6170959075	
    Loss 2390.8161320703	
    Loss 2375.1786722737	
    Loss 2359.6171807678	
    Loss 2344.2002189714	
    Loss 2328.8506671579	
    Loss 2313.6464802052	
    Loss 2298.4964628545	
    Loss 2283.4659058372	
    Loss 2268.5757841816	
    Loss 2253.7914940074	
    Loss 2239.1017517945	
    Loss 2224.5239574049	
    Loss 2210.0733591577	
Epoch 15	
 98102
     0
     0
     0
     0
     0
     0
     0
 33705
     1
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10206512503035	
Grad norm	20.639122981546	
    Loss 2205.7065158962	
    Loss 2191.3622021048	
    Loss 2177.1596748067	
    Loss 2162.9984204487	
    Loss 2148.9761480748	
    Loss 2135.0345263424	
    Loss 2121.1322265283	
    Loss 2107.286109143	
    Loss 2093.6175400079	
    Loss 2080.0775221144	
    Loss 2066.5912309731	
    Loss 2053.2186433052	
    Loss 2039.9225903776	
    Loss 2026.6747382126	
    Loss 2013.5673253149	
    Loss 2000.5634521828	
    Loss 1987.6921615865	
    Loss 1974.8226340593	
    Loss 1962.0332629668	
    Loss 1949.3858964059	
    Loss 1936.8192717363	
    Loss 1924.3417576692	
    Loss 1911.9838403939	
    Loss 1899.6653990616	
    Loss 1887.4416203027	
    Loss 1875.2932765594	
    Loss 1863.2049976992	
    Loss 1851.253299664	
    Loss 1839.3770334137	
    Loss 1827.5514540448	
    Loss 1815.8484920135	
    Loss 1804.2145965509	
    Loss 1792.6902798785	
    Loss 1781.2707072819	
    Loss 1769.8909090637	
    Loss 1758.5385720992	
    Loss 1747.2773540352	
    Loss 1736.086599618	
    Loss 1725.0060672774	
    Loss 1714.0165284696	
    Loss 1703.1358260429	
    Loss 1692.2887996407	
    Loss 1681.5542285003	
    Loss 1670.7785336107	
    Loss 1660.1736711626	
    Loss 1649.6846487002	
    Loss 1639.2606490514	
    Loss 1628.825966972	
    Loss 1618.507667776	
