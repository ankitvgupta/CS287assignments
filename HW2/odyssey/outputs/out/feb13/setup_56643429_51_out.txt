[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	0.25	Lambda:	10	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3261
  932
 3189
 2331
 3185
 3373
 1794
 4344
 3405
 1963
 4844
 4707
 2402
 3036
 1187
 1038
 1375
  966
 1931
 2841
 4316
 1179
 2068
 1795
 4566
  799
 6934
 4324
 2562
 2559
 1700
 2774
 2683
 2900
 2761
 1819
 2693
 4853
 9840
 3241
 5069
 2543
 2119
 2018
 1589
[torch.DoubleTensor of size 45]

Validation accuracy:	0.020507101238165	
Grad norm	0	
    Loss 22775601.353754	
    Loss 21990354.280989	
    Loss 21232252.136315	
    Loss 20500268.083895	
    Loss 19793520.472854	
    Loss 19111141.308216	
    Loss 18452292.015829	
    Loss 17816168.929618	
    Loss 17201966.10628	
    Loss 16608941.868208	
    Loss 16036371.608944	
    Loss 15483539.318388	
    Loss 14949767.64526	
    Loss 14434407.682209	
    Loss 13936804.236331	
    Loss 13456358.036449	
    Loss 12992471.691154	
    Loss 12544582.140649	
    Loss 12112134.766692	
    Loss 11694595.77819	
    Loss 11291443.779445	
    Loss 10902191.723644	
    Loss 10526362.267998	
    Loss 10163492.897258	
    Loss 9813135.1079171	
    Loss 9474853.4820536	
    Loss 9148237.0720193	
    Loss 8832882.5837876	
    Loss 8528397.0090024	
    Loss 8234409.0580593	
    Loss 7950559.8458054	
    Loss 7676492.0954932	
    Loss 7411871.403098	
    Loss 7156370.1224205	
    Loss 6909677.8214769	
    Loss 6671492.9946618	
    Loss 6441517.6513093	
    Loss 6219475.0953096	
    Loss 6005087.4302082	
    Loss 5798090.1109406	
    Loss 5598229.9749524	
    Loss 5405257.8136027	
    Loss 5218938.170616	
    Loss 5039047.0112273	
    Loss 4865353.3571685	
    Loss 4697646.4643065	
    Loss 4535727.0606339	
    Loss 4379389.2322071	
    Loss 4228440.937408	
    Loss 4082695.6887279	
    Loss 3941974.9101833	
    Loss 3806103.411371	
    Loss 3674919.2787827	
    Loss 3548253.6054724	
    Loss 3425957.9092206	
    Loss 3307875.7687054	
    Loss 3193864.6860211	
    Loss 3083783.8839099	
    Loss 2977501.4735414	
    Loss 2874881.3255783	
    Loss 2775801.0599809	
    Loss 2680136.7374517	
    Loss 2587768.2226297	
    Loss 2498580.8776891	
    Loss 2412470.8173615	
    Loss 2329329.8320244	
    Loss 2249054.188387	
    Loss 2171549.1046611	
    Loss 2096713.1478746	
    Loss 2024456.6594786	
    Loss 1954692.9552266	
    Loss 1887335.8677711	
    Loss 1822299.6982085	
    Loss 1759503.1453072	
    Loss 1698873.7169289	
    Loss 1640335.0492297	
    Loss 1583812.6983516	
    Loss 1529238.2893639	
    Loss 1476545.4140643	
    Loss 1425672.4554889	
    Loss 1376550.0535085	
    Loss 1329122.4659991	
    Loss 1283330.6289437	
    Loss 1239116.1347154	
    Loss 1196427.2341305	
    Loss 1155208.9944826	
    Loss 1115412.1542576	
    Loss 1076985.5613249	
    Loss 1039884.2798173	
    Loss 1004063.0910315	
    Loss 969476.63231664	
    Loss 936081.2534211	
    Loss 903838.56392893	
    Loss 872708.02323849	
    Loss 842647.92691709	
    Loss 813624.84410714	
    Loss 785604.14669453	
    Loss 758549.01068673	
    Loss 732425.81251093	
    Loss 707201.83950435	
    Loss 682848.13003175	
    Loss 659334.98236109	
    Loss 636631.76048947	
    Loss 614711.25631566	
    Loss 593547.42260808	
    Loss 573112.49438299	
    Loss 553381.75979544	
    Loss 534331.41444996	
    Loss 515937.74809591	
    Loss 498176.25921041	
    Loss 481028.69556375	
    Loss 464471.55935859	
    Loss 448487.7380765	
    Loss 433055.06885258	
    Loss 418153.91747875	
    Loss 403765.37324735	
    Loss 389872.75992725	
    Loss 376458.8098062	
    Loss 363508.2500367	
    Loss 351002.46385589	
    Loss 338929.50765325	
    Loss 327272.24865855	
    Loss 316015.89298036	
    Loss 305148.08863564	
    Loss 294655.15262892	
    Loss 284524.05868802	
    Loss 274740.66276596	
    Loss 265295.41277923	
    Loss 256176.90888201	
    Loss 247371.51281678	
    Loss 238869.40587799	
    Loss 230660.56413669	
    Loss 222735.45816855	
    Loss 215083.73675896	
    Loss 207695.35239677	
    Loss 200562.35984422	
    Loss 193674.29786683	
    Loss 187023.0127723	
    Loss 180601.56437743	
    Loss 174403.04498835	
    Loss 168416.25287394	
    Loss 162636.94811728	
    Loss 157056.22749167	
Epoch 2	
 131550
      0
      0
      0
      0
      0
      0
      0
    258
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099993930565671	
Grad norm	214.07175097687	
    Loss 153800.66008401	
    Loss 148524.91574314	
    Loss 143432.05400678	
    Loss 138513.98753334	
    Loss 133764.83174561	
    Loss 129179.18092054	
    Loss 124752.11108651	
    Loss 120477.41631167	
    Loss 116350.87623429	
    Loss 112365.91696119	
    Loss 108519.37782199	
    Loss 104803.8848579	
    Loss 101216.40865916	
    Loss 97753.618067993	
    Loss 94409.107405415	
    Loss 91181.036327929	
    Loss 88063.619391597	
    Loss 85054.517762662	
    Loss 82148.789136027	
    Loss 79342.705441215	
    Loss 76633.512197594	
    Loss 74018.231062657	
    Loss 71493.238029301	
    Loss 69055.08549693	
    Loss 66700.449112991	
    Loss 64426.342718727	
    Loss 62231.530291326	
    Loss 60112.077451742	
    Loss 58065.896192125	
    Loss 56090.283622716	
    Loss 54183.622646845	
    Loss 52342.070798705	
    Loss 50563.74993939	
    Loss 48846.255755435	
    Loss 47188.115407526	
    Loss 45587.025620617	
    Loss 44040.755813773	
    Loss 42547.831268622	
    Loss 41107.568887391	
    Loss 39715.993599334	
    Loss 38372.7025989	
    Loss 37076.140412903	
    Loss 35823.400073847	
    Loss 34614.295308855	
    Loss 33447.379964422	
    Loss 32319.606480688	
    Loss 31231.774307109	
    Loss 30181.201387562	
    Loss 29166.690546163	
    Loss 28187.072300355	
    Loss 27240.760911246	
    Loss 26328.309585863	
    Loss 25445.162940825	
    Loss 24594.003769922	
    Loss 23772.69961325	
    Loss 22979.861245183	
    Loss 22213.880499972	
    Loss 21473.712366542	
    Loss 20759.89678917	
    Loss 20071.22344126	
    Loss 19405.373068506	
    Loss 18762.336597356	
    Loss 18141.458438261	
    Loss 17541.169618277	
    Loss 16962.658745578	
    Loss 16403.946565014	
    Loss 15864.700486411	
    Loss 15344.00769732	
    Loss 14841.153135126	
    Loss 14355.072954059	
    Loss 13886.048912545	
    Loss 13433.656934997	
    Loss 12995.895491128	
    Loss 12573.255077569	
    Loss 12165.621697018	
    Loss 11772.961874062	
    Loss 11392.934873575	
    Loss 11026.509816618	
    Loss 10672.3421167	
    Loss 10331.238940762	
    Loss 10000.671053575	
    Loss 9682.259103268	
    Loss 9374.2530928759	
    Loss 9077.1780457483	
    Loss 8790.5953208559	
    Loss 8513.0537381141	
    Loss 8244.3891484484	
    Loss 7986.125804	
    Loss 7736.3046080069	
    Loss 7495.8483258086	
    Loss 7264.3645079819	
    Loss 7039.1829001212	
    Loss 6823.5023909201	
    Loss 6614.6193235461	
    Loss 6411.2676983157	
    Loss 6215.883069081	
    Loss 6027.941365803	
    Loss 5845.721871492	
    Loss 5670.3055504143	
    Loss 5500.7580024305	
    Loss 5337.18907046	
    Loss 5179.3222961367	
    Loss 5026.8047313543	
    Loss 4879.8044545352	
    Loss 4737.6797488954	
    Loss 4600.5370945297	
    Loss 4467.9555849506	
    Loss 4340.0892618664	
    Loss 4216.5999061161	
    Loss 4095.6464303498	
    Loss 3980.5562032386	
    Loss 3868.6710280502	
    Loss 3761.7582561687	
    Loss 3659.1958907462	
    Loss 3560.2252277448	
    Loss 3464.2294928581	
    Loss 3370.731931601	
    Loss 3280.0276134397	
    Loss 3193.1710569268	
    Loss 3108.2045957311	
    Loss 3027.9921267176	
    Loss 2950.0758815338	
    Loss 2874.2032733569	
    Loss 2801.1698852781	
    Loss 2730.3595727984	
    Loss 2662.08824471	
    Loss 2595.4440922282	
    Loss 2531.5475824719	
    Loss 2470.8789040395	
    Loss 2411.1800432994	
    Loss 2353.3287093506	
    Loss 2297.8842237291	
    Loss 2244.7940719892	
    Loss 2193.3743858896	
    Loss 2144.3429774545	
    Loss 2096.6358838978	
    Loss 2050.1862154704	
    Loss 2004.9565246063	
    Loss 1961.3439839653	
    Loss 1920.3771230547	
    Loss 1879.7921165411	
    Loss 1841.2884830895	
    Loss 1803.6888840678	
Epoch 3	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	22.506853858488	
    Loss 1781.4226762805	
    Loss 1745.4448821667	
    Loss 1711.8738623973	
    Loss 1679.3942907407	
    Loss 1647.123910127	
    Loss 1616.1409335082	
    Loss 1586.4103460536	
    Loss 1557.3139767369	
    Loss 1530.1749470944	
    Loss 1503.5397949214	
    Loss 1478.3218413388	
    Loss 1452.6832713726	
    Loss 1427.9637227054	
    Loss 1404.6109134161	
    Loss 1381.7214041248	
    Loss 1360.3188931169	
    Loss 1339.4871001944	
    Loss 1319.778574353	
    Loss 1300.6299786395	
    Loss 1281.4643886792	
    Loss 1263.3164422158	
    Loss 1246.3889053081	
    Loss 1229.908817153	
    Loss 1213.7521866166	
    Loss 1197.7493799477	
    Loss 1181.9179743267	
    Loss 1167.1770722185	
    Loss 1152.4674339365	
    Loss 1138.6322863812	
    Loss 1125.486356329	
    Loss 1113.3907188936	
    Loss 1101.1633427887	
    Loss 1089.2068798985	
    Loss 1077.5148512999	
    Loss 1066.3275180624	
    Loss 1055.1170491866	
    Loss 1044.1170246776	
    Loss 1033.4823897893	
    Loss 1024.3156611593	
    Loss 1014.4751996632	
    Loss 1005.3007461153	
    Loss 997.00622669815	
    Loss 988.08575901379	
    Loss 979.60841490309	
    Loss 972.04370082176	
    Loss 963.94133245366	
    Loss 956.81605016189	
    Loss 949.83216342547	
    Loss 942.91256943836	
    Loss 936.19246529486	
    Loss 929.14755763167	
    Loss 923.7217835061	
    Loss 916.3113851465	
    Loss 910.80126114781	
    Loss 905.86084894848	
    Loss 901.34392770242	
    Loss 896.55884302151	
    Loss 891.2666905252	
    Loss 886.91563060772	
    Loss 883.25090442209	
    Loss 878.82688730324	
    Loss 874.35134687698	
    Loss 870.09431543372	
    Loss 865.42357266654	
    Loss 861.79229904895	
    Loss 858.15014285929	
    Loss 854.83110868308	
    Loss 851.454206468	
    Loss 848.18793636937	
    Loss 844.49595525146	
    Loss 841.21140822657	
    Loss 838.42977056108	
    Loss 834.79556195397	
    Loss 831.47476112942	
    Loss 828.61202195815	
    Loss 826.68835745009	
    Loss 824.04203269009	
    Loss 822.03937134262	
    Loss 819.70767434603	
    Loss 818.07449598319	
    Loss 815.49942364055	
    Loss 813.70027074537	
    Loss 811.35832734862	
    Loss 809.4671928614	
    Loss 807.82782968175	
    Loss 805.4265751835	
    Loss 802.39003638717	
    Loss 800.74453197412	
    Loss 798.5876919319	
    Loss 797.20806534968	
    Loss 796.62454372999	
    Loss 794.41836784445	
    Loss 794.03738346515	
    Loss 792.96403312384	
    Loss 790.33044405349	
    Loss 788.72928122391	
    Loss 787.79312066702	
    Loss 786.1709208995	
    Loss 785.1666276604	
    Loss 784.08562215158	
    Loss 783.11565556417	
    Loss 782.21366030885	
    Loss 781.24473708419	
    Loss 780.61527891879	
    Loss 779.72086024366	
    Loss 779.02996351289	
    Loss 778.18571185029	
    Loss 777.5164386066	
    Loss 776.8327674754	
    Loss 774.48860232361	
    Loss 773.88689173133	
    Loss 772.57318920054	
    Loss 772.3009917635	
    Loss 772.72195114973	
    Loss 773.2431898201	
    Loss 773.35878251576	
    Loss 772.61748022127	
    Loss 771.44357496436	
    Loss 771.02673521674	
    Loss 769.55760286687	
    Loss 769.95075752687	
    Loss 769.87169271574	
    Loss 769.18304951868	
    Loss 768.72136400571	
    Loss 767.94129927418	
    Loss 767.29253560444	
    Loss 765.9961954157	
    Loss 765.14842715957	
    Loss 765.33708067654	
    Loss 764.44293734943	
    Loss 763.36560268698	
    Loss 762.74807181345	
    Loss 762.57410285643	
    Loss 762.20552972497	
    Loss 762.55884049619	
    Loss 762.4404057378	
    Loss 761.99192240254	
    Loss 761.20390946375	
    Loss 760.45043085405	
    Loss 760.83568221258	
    Loss 760.26207960625	
    Loss 760.34995217604	
    Loss 760.02577211926	
Epoch 4	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	7.1841935835768	
    Loss 759.48332711411	
    Loss 758.69103503828	
    Loss 759.1007223864	
    Loss 759.46998576503	
    Loss 758.90537231965	
    Loss 758.56018403718	
    Loss 758.39597691114	
    Loss 757.83644563635	
    Loss 758.25277102397	
    Loss 758.24430842785	
    Loss 758.69521247341	
    Loss 757.86497703415	
    Loss 757.10375529986	
    Loss 756.85511260371	
    Loss 756.32280513342	
    Loss 756.46867155568	
    Loss 756.47726108714	
    Loss 756.85345385504	
    Loss 757.11711085669	
    Loss 756.68354317924	
    Loss 756.63367255195	
    Loss 757.18793017001	
    Loss 757.56531514325	
    Loss 757.68323595018	
    Loss 757.40687685888	
    Loss 756.78101466307	
    Loss 756.69244400946	
    Loss 756.1162859757	
    Loss 755.93970226616	
    Loss 755.99225105118	
    Loss 756.6207184489	
    Loss 756.6847057817	
    Loss 756.60404461904	
    Loss 756.40249663065	
    Loss 756.30493399912	
    Loss 755.78212837677	
    Loss 755.11454470518	
    Loss 754.4549343625	
    Loss 754.90996546252	
    Loss 754.35798186586	
    Loss 754.15392327613	
    Loss 754.52998118345	
    Loss 753.97671510279	
    Loss 753.55650865448	
    Loss 753.77655872343	
    Loss 753.21501093891	
    Loss 753.34213338878	
    Loss 753.37293958565	
    Loss 753.22227151528	
    Loss 753.04087516169	
    Loss 752.3015802395	
    Loss 752.97324995185	
    Loss 751.45305128455	
    Loss 751.63439437071	
    Loss 752.17878345083	
    Loss 752.96456669418	
    Loss 753.30475105335	
    Loss 752.95376190113	
    Loss 753.36937952958	
    Loss 754.3035797759	
    Loss 754.32668842713	
    Loss 754.13870459153	
    Loss 754.02620399853	
    Loss 753.37524423485	
    Loss 753.61024365786	
    Loss 753.70161557157	
    Loss 753.98603252442	
    Loss 754.08026734438	
    Loss 754.17131900183	
    Loss 753.72261686055	
    Loss 753.56966143498	
    Loss 753.8057433752	
    Loss 753.08689209284	
    Loss 752.59105695074	
    Loss 752.45011857611	
    Loss 753.14737138603	
    Loss 753.04086523633	
    Loss 753.493582017	
    Loss 753.53125739881	
    Loss 754.16670630621	
    Loss 753.79978444851	
    Loss 754.12774326516	
    Loss 753.83698975307	
    Loss 753.92972716609	
    Loss 754.20067813607	
    Loss 753.64637665193	
    Loss 752.39105009612	
    Loss 752.47594272938	
    Loss 751.98167886931	
    Loss 752.20318540776	
    Loss 753.1704325273	
    Loss 752.46503726365	
    Loss 753.53360296487	
    Loss 753.85434298642	
    Loss 752.57249309233	
    Loss 752.27522691113	
    Loss 752.59027960969	
    Loss 752.17966771756	
    Loss 752.34763302472	
    Loss 752.40358238791	
    Loss 752.52661875613	
    Loss 752.67739316195	
    Loss 752.72409100868	
    Loss 753.07943565014	
    Loss 753.12863931574	
    Loss 753.35572429307	
    Loss 753.39719160567	
    Loss 753.58302563213	
    Loss 753.72462491162	
    Loss 752.18026389454	
    Loss 752.34734199587	
    Loss 751.77934232339	
    Loss 752.21723894569	
    Loss 753.32543091143	
    Loss 754.51535351493	
    Loss 755.28019535898	
    Loss 755.16268994717	
    Loss 754.5881242679	
    Loss 754.75016093667	
    Loss 753.84264315617	
    Loss 754.77681670389	
    Loss 755.22070991725	
    Loss 755.03936897399	
    Loss 755.06588843471	
    Loss 754.75436388401	
    Loss 754.55859709292	
    Loss 753.70374256951	
    Loss 753.27877030378	
    Loss 753.87370686316	
    Loss 753.37633227056	
    Loss 752.68135027757	
    Loss 752.43374894135	
    Loss 752.61535898714	
    Loss 752.58632194885	
    Loss 753.27533548708	
    Loss 753.47356558928	
    Loss 753.33498729657	
    Loss 752.84846028735	
    Loss 752.38171208986	
    Loss 753.04167792667	
    Loss 752.74027104688	
    Loss 753.08757868061	
    Loss 753.01484317101	
Epoch 5	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.095777560264	
    Loss 752.61751024347	
    Loss 752.05867585537	
    Loss 752.6944817421	
    Loss 753.28477873153	
    Loss 752.93304724983	
    Loss 752.79516174242	
    Loss 752.83019049459	
    Loss 752.46235165639	
    Loss 753.06360087842	
    Loss 753.23513432661	
    Loss 753.85704890541	
    Loss 753.19383503954	
    Loss 752.59425282571	
    Loss 752.4997851113	
    Loss 752.11985978399	
    Loss 752.40986854128	
    Loss 752.56029539133	
    Loss 753.0705065236	
    Loss 753.46508281494	
    Loss 753.15703109142	
    Loss 753.2294300313	
    Loss 753.90234916873	
    Loss 754.39252297302	
    Loss 754.6192606704	
    Loss 754.44898272402	
    Loss 753.92721643513	
    Loss 753.9368418988	
    Loss 753.45447805726	
    Loss 753.36944448009	
    Loss 753.51101617805	
    Loss 754.22396552964	
    Loss 754.37008505942	
    Loss 754.36935109849	
    Loss 754.24677003135	
    Loss 754.22506566209	
    Loss 753.77420183198	
    Loss 753.17692052052	
    Loss 752.58517291763	
    Loss 753.10493392294	
    Loss 752.61533670643	
    Loss 752.47178752221	
    Loss 752.90692950445	
    Loss 752.41028047751	
    Loss 752.04309892334	
    Loss 752.31476449046	
    Loss 751.80499905919	
    Loss 751.97990051057	
    Loss 752.05781562543	
    Loss 751.95224104933	
    Loss 751.8146402952	
    Loss 751.11698952013	
    Loss 751.82957372223	
    Loss 750.3492141448	
    Loss 750.56928033732	
    Loss 751.15022474447	
    Loss 751.97186624976	
    Loss 752.34713381123	
    Loss 752.02940513513	
    Loss 752.47687964334	
    Loss 753.44143862615	
    Loss 753.49452208443	
    Loss 753.33492873697	
    Loss 753.25021295985	
    Loss 752.62738539869	
    Loss 752.88847430029	
    Loss 753.00504133657	
    Loss 753.31367300555	
    Loss 753.43077476665	
    Loss 753.54425433799	
    Loss 753.11737638126	
    Loss 752.98552873578	
    Loss 753.24149087363	
    Loss 752.54200022138	
    Loss 752.0655499493	
    Loss 751.94295557984	
    Loss 752.65735371224	
    Loss 752.56814646021	
    Loss 753.03776584014	
    Loss 753.09168779685	
    Loss 753.74131074449	
    Loss 753.38943795415	
    Loss 753.73160913026	
    Loss 753.45435081508	
    Loss 753.56036135699	
    Loss 753.84370000454	
    Loss 753.30162158861	
    Loss 752.05791606471	
    Loss 752.15484364365	
    Loss 751.671569155	
    Loss 751.90336910937	
    Loss 752.88091854185	
    Loss 752.18575406246	
    Loss 753.26421700436	
    Loss 753.59407349373	
    Loss 752.32148122528	
    Loss 752.03306720457	
    Loss 752.35605326678	
    Loss 751.95338780195	
    Loss 752.12920265864	
    Loss 752.1931249836	
    Loss 752.32349204997	
    Loss 752.48110127973	
    Loss 752.5343365349	
    Loss 752.89633640345	
    Loss 752.95141558645	
    Loss 753.18470179791	
    Loss 753.2321241816	
    Loss 753.42368346768	
    Loss 753.57080638142	
    Loss 752.03200492879	
    Loss 752.20413831591	
    Loss 751.64133894575	
    Loss 752.0834522007	
    Loss 753.19587369208	
    Loss 754.39029345698	
    Loss 755.159756101	
    Loss 755.04647722848	
    Loss 754.4757202296	
    Loss 754.64147684502	
    Loss 753.73775429464	
    Loss 754.67549768409	
    Loss 755.1228593488	
    Loss 754.9450583725	
    Loss 754.97486473348	
    Loss 754.66631030983	
    Loss 754.4734673769	
    Loss 753.62173166275	
    Loss 753.19952191656	
    Loss 753.79694917642	
    Loss 753.30237024358	
    Loss 752.61000574423	
    Loss 752.36499475312	
    Loss 752.5489790105	
    Loss 752.52190564163	
    Loss 753.21346661371	
    Loss 753.41356424951	
    Loss 753.27711135123	
    Loss 752.79282572244	
    Loss 752.32787625174	
    Loss 752.98942557438	
    Loss 752.6901115928	
    Loss 753.03916277218	
    Loss 752.96817912022	
Epoch 6	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0341653533054	
    Loss 752.57174606881	
    Loss 752.01423279951	
    Loss 752.65137946124	
    Loss 753.24316423602	
    Loss 752.89284341951	
    Loss 752.75645002812	
    Loss 752.79285729235	
    Loss 752.42630257854	
    Loss 753.02876574416	
    Loss 753.20156659432	
    Loss 753.82450574223	
    Loss 753.16243302232	
    Loss 752.56398261843	
    Loss 752.47047175191	
    Loss 752.09173063309	
    Loss 752.38264939736	
    Loss 752.53416093772	
    Loss 753.04519130267	
    Loss 753.44067372482	
    Loss 753.133424408	
    Loss 753.20669489313	
    Loss 753.8805036971	
    Loss 754.37139329177	
    Loss 754.59881842469	
    Loss 754.42928271847	
    Loss 753.90836398399	
    Loss 753.91862350888	
    Loss 753.43679732015	
    Loss 753.35236185713	
    Loss 753.49454788741	
    Loss 754.20797808077	
    Loss 754.35461246995	
    Loss 754.35442328171	
    Loss 754.2325047751	
    Loss 754.21140875787	
    Loss 753.76104617883	
    Loss 753.16430166665	
    Loss 752.57307635663	
    Loss 753.09327487869	
    Loss 752.60409155751	
    Loss 752.46097150537	
    Loss 752.89657499913	
    Loss 752.40033353706	
    Loss 752.03341781099	
    Loss 752.3053748119	
    Loss 751.79604561732	
    Loss 751.97119456821	
    Loss 752.04942503603	
    Loss 751.94412071537	
    Loss 751.8068029701	
    Loss 751.10937135828	
    Loss 751.82222219598	
    Loss 750.34215080747	
    Loss 750.56251111724	
    Loss 751.1436735999	
    Loss 751.96557240865	
    Loss 752.34112655686	
    Loss 752.02362513052	
    Loss 752.47130319321	
    Loss 753.43601937462	
    Loss 753.48931335256	
    Loss 753.32987596429	
    Loss 753.24534150618	
    Loss 752.62278482575	
    Loss 752.88405666694	
    Loss 753.00079768896	
    Loss 753.30958851634	
    Loss 753.42681103323	
    Loss 753.54042801661	
    Loss 753.11369988514	
    Loss 752.98200055885	
    Loss 753.23806319355	
    Loss 752.53868505411	
    Loss 752.06239250646	
    Loss 751.93992601768	
    Loss 752.65440220002	
    Loss 752.56533042269	
    Loss 753.03509373287	
    Loss 753.08915076843	
    Loss 753.73878426459	
    Loss 753.38702650072	
    Loss 753.729288056	
    Loss 753.45209761801	
    Loss 753.55819216985	
    Loss 753.84157652775	
    Loss 753.29956751732	
    Loss 752.05591335264	
    Loss 752.15295419054	
    Loss 751.66973960792	
    Loss 751.90157302757	
    Loss 752.87918262836	
    Loss 752.18409867662	
    Loss 753.26263951963	
    Loss 753.59253683054	
    Loss 752.32001975561	
    Loss 752.03167271741	
    Loss 752.35467437069	
    Loss 751.95204715545	
    Loss 752.12791254109	
    Loss 752.19191376279	
    Loss 752.32232865567	
    Loss 752.47996203369	
    Loss 752.53321669209	
    Loss 752.89526282059	
    Loss 752.95034435005	
    Loss 753.18367397084	
    Loss 753.23113653656	
    Loss 753.42273125509	
    Loss 753.56988987963	
    Loss 752.0311401502	
    Loss 752.20329636112	
    Loss 751.64054697687	
    Loss 752.08264381709	
    Loss 753.19506449614	
    Loss 754.38951564551	
    Loss 755.1590306994	
    Loss 755.04578408369	
    Loss 754.47503437383	
    Loss 754.64080301874	
    Loss 753.73710778414	
    Loss 754.67487051377	
    Loss 755.12225071291	
    Loss 754.94448325362	
    Loss 754.97431109384	
    Loss 754.66576257076	
    Loss 754.47293045369	
    Loss 753.62122637596	
    Loss 753.19902946408	
    Loss 753.79645401754	
    Loss 753.30190515838	
    Loss 752.60956190207	
    Loss 752.36457677589	
    Loss 752.54857581778	
    Loss 752.52148893529	
    Loss 753.21309023521	
    Loss 753.41317976536	
    Loss 753.2767439972	
    Loss 752.79249048849	
    Loss 752.32754179588	
    Loss 752.9890808908	
    Loss 752.68980271429	
    Loss 753.03886579674	
    Loss 752.96789869962	
Epoch 7	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0320117681342	
    Loss 752.5714656404	
    Loss 752.0139414997	
    Loss 752.65108348548	
    Loss 753.24287779271	
    Loss 752.89256518479	
    Loss 752.75618955454	
    Loss 752.79260951668	
    Loss 752.42606356837	
    Loss 753.02853261922	
    Loss 753.20134536428	
    Loss 753.82428131821	
    Loss 753.16221781927	
    Loss 752.56377875066	
    Loss 752.47026869916	
    Loss 752.09154805117	
    Loss 752.38246826923	
    Loss 752.53399724939	
    Loss 753.04502672591	
    Loss 753.44051699562	
    Loss 753.13326939172	
    Loss 753.20654986256	
    Loss 753.88037213975	
    Loss 754.37126341688	
    Loss 754.59869001682	
    Loss 754.4291617998	
    Loss 753.90826053665	
    Loss 753.91852219186	
    Loss 753.43669260001	
    Loss 753.35226013941	
    Loss 753.49445116843	
    Loss 754.20787757116	
    Loss 754.35451267696	
    Loss 754.3543277785	
    Loss 754.2324239533	
    Loss 754.21133960781	
    Loss 753.76098254177	
    Loss 753.1642466521	
    Loss 752.57303036168	
    Loss 753.09323197161	
    Loss 752.60405095864	
    Loss 752.46093600304	
    Loss 752.89654784682	
    Loss 752.40031117129	
    Loss 752.03339003391	
    Loss 752.30534470443	
    Loss 751.79602561264	
    Loss 751.97117064326	
    Loss 752.04940329054	
    Loss 751.94409822885	
    Loss 751.80678167114	
    Loss 751.109346665	
    Loss 751.82219853103	
    Loss 750.34213079161	
    Loss 750.56249555652	
    Loss 751.14365714987	
    Loss 751.96555901076	
    Loss 752.34111922348	
    Loss 752.02361968261	
    Loss 752.47129862508	
    Loss 753.43601200509	
    Loss 753.48930830811	
    Loss 753.32986914081	
    Loss 753.24533555155	
    Loss 752.62278684476	
    Loss 752.88406054432	
    Loss 753.00080291313	
    Loss 753.30959434589	
    Loss 753.42681533347	
    Loss 753.54043204176	
    Loss 753.11370530679	
    Loss 752.98200766036	
    Loss 753.23806828384	
    Loss 752.5386895908	
    Loss 752.06240007156	
    Loss 751.9399348997	
    Loss 752.65440869875	
    Loss 752.56533948034	
    Loss 753.03510611803	
    Loss 753.08916617784	
    Loss 753.73879308854	
    Loss 753.38703704251	
    Loss 753.72929890558	
    Loss 753.45210714008	
    Loss 753.55820184959	
    Loss 753.84158344711	
    Loss 753.2995740786	
    Loss 752.05591823151	
    Loss 752.15296246445	
    Loss 751.6697472493	
    Loss 751.90157816428	
    Loss 752.87918749965	
    Loss 752.18410505742	
    Loss 753.26264728364	
    Loss 753.59254326649	
    Loss 752.32002769426	
    Loss 752.03168176461	
    Loss 752.35468054412	
    Loss 751.95205242087	
    Loss 752.12791800766	
    Loss 752.19192170947	
    Loss 752.32233680328	
    Loss 752.47996845956	
    Loss 752.53322121453	
    Loss 752.89526770838	
    Loss 752.95034630103	
    Loss 753.18367632013	
    Loss 753.23113917815	
    Loss 753.42273381136	
    Loss 753.56989259853	
    Loss 752.03114434938	
    Loss 752.20329968889	
    Loss 751.64055183875	
    Loss 752.08264498342	
    Loss 753.19506347072	
    Loss 754.38951497371	
    Loss 755.15903215353	
    Loss 755.04578613569	
    Loss 754.47503496739	
    Loss 754.64080270791	
    Loss 753.73710782365	
    Loss 754.67487036823	
    Loss 755.12225024833	
    Loss 754.9444837664	
    Loss 754.97431164096	
    Loss 754.66576205249	
    Loss 754.47292936327	
    Loss 753.62122625362	
    Loss 753.19902901377	
    Loss 753.79645199643	
    Loss 753.30190430874	
    Loss 752.60956148567	
    Loss 752.36457723385	
    Loss 752.5485762949	
    Loss 752.5214871179	
    Loss 753.21309054851	
    Loss 753.41317839087	
    Loss 753.27674292811	
    Loss 752.79249098727	
    Loss 752.32754132404	
    Loss 752.98907869551	
    Loss 752.68980255286	
    Loss 753.03886575166	
    Loss 752.96789914294	
Epoch 8	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0319921921468	
    Loss 752.57146559867	
    Loss 752.01393981619	
    Loss 752.65108072072	
    Loss 753.24287502177	
    Loss 752.89256240133	
    Loss 752.75618751057	
    Loss 752.79260785774	
    Loss 752.42606200619	
    Loss 753.02853092639	
    Loss 753.2013439461	
    Loss 753.82427905784	
    Loss 753.16221575281	
    Loss 752.56377706705	
    Loss 752.47026661377	
    Loss 752.09154708889	
    Loss 752.38246694874	
    Loss 752.53399683646	
    Loss 753.04502580997	
    Loss 753.44051625321	
    Loss 753.13326834563	
    Loss 753.20654922254	
    Loss 753.88037219731	
    Loss 754.37126326008	
    Loss 754.59868964577	
    Loss 754.42916169261	
    Loss 753.9082614865	
    Loss 753.91852298068	
    Loss 753.43669283288	
    Loss 753.35226033587	
    Loss 753.49445144181	
    Loss 754.20787725473	
    Loss 754.35451216111	
    Loss 754.35432733961	
    Loss 754.23242441102	
    Loss 754.21134073376	
    Loss 753.76098392432	
    Loss 753.16424848728	
    Loss 752.57303271582	
    Loss 753.09323435465	
    Loss 752.60405331607	
    Loss 752.46093859866	
    Loss 752.89655092625	
    Loss 752.40031443777	
    Loss 752.03339268958	
    Loss 752.30534701024	
    Loss 751.79602856839	
    Loss 751.97117314486	
    Loss 752.04940582008	
    Loss 751.94410055528	
    Loss 751.80678396357	
    Loss 751.10934854595	
    Loss 751.82220035266	
    Loss 750.34213277927	
    Loss 750.56249775365	
    Loss 751.14365914147	
    Loss 751.96556113162	
    Loss 752.3411217227	
    Loss 752.02362222849	
    Loss 752.47130116399	
    Loss 753.43601420898	
    Loss 753.48931061258	
    Loss 753.3298712018	
    Loss 753.24533759689	
    Loss 752.62278941191	
    Loss 752.88406317729	
    Loss 753.00080555546	
    Loss 753.30959694321	
    Loss 753.42681775203	
    Loss 753.54043435272	
    Loss 753.11370766864	
    Loss 752.98201010032	
    Loss 753.23807049254	
    Loss 752.53869169743	
    Loss 752.06240234232	
    Loss 751.93993722337	
    Loss 752.65441077871	
    Loss 752.56534171819	
    Loss 753.03510856268	
    Loss 753.08916881556	
    Loss 753.73879515846	
    Loss 753.38703918776	
    Loss 753.72930103557	
    Loss 753.45210912034	
    Loss 753.55820379901	
    Loss 753.84158512833	
    Loss 753.29957570464	
    Loss 752.05591969134	
    Loss 752.15296416189	
    Loss 751.66974886548	
    Loss 751.90157955333	
    Loss 752.8791888364	
    Loss 752.18410648315	
    Loss 753.26264878421	
    Loss 753.59254463354	
    Loss 752.32002914833	
    Loss 752.03168328371	
    Loss 752.35468180835	
    Loss 751.95205358837	
    Loss 752.12791916907	
    Loss 752.19192303801	
    Loss 752.32233812278	
    Loss 752.4799696083	
    Loss 752.53322218314	
    Loss 752.89526868002	
    Loss 752.95034701753	
    Loss 753.1836770458	
    Loss 753.23113990807	
    Loss 753.4227345093	
    Loss 753.56989329339	
    Loss 752.03114514344	
    Loss 752.20330038612	
    Loss 751.64055264119	
    Loss 752.08264547393	
    Loss 753.19506378246	
    Loss 754.38951530152	
    Loss 755.1590326423	
    Loss 755.04578666323	
    Loss 754.47503536141	
    Loss 754.64080302065	
    Loss 753.73710815372	
    Loss 754.67487067589	
    Loss 755.12225051423	
    Loss 754.94448409799	
    Loss 754.97431196001	
    Loss 754.66576227704	
    Loss 754.47292953638	
    Loss 753.62122648668	
    Loss 753.19902921435	
    Loss 753.79645206299	
    Loss 753.30190446512	
    Loss 752.60956166831	
    Loss 752.36457748062	
    Loss 752.54857653642	
    Loss 752.52148716372	
    Loss 753.21309075791	
    Loss 753.41317845954	
    Loss 753.27674301127	
    Loss 752.79249118799	
    Loss 752.32754143503	
    Loss 752.98907866351	
    Loss 752.68980268209	
    Loss 753.03886588646	
    Loss 752.96789931083	
Epoch 9	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0319912691362	
    Loss 752.57146572442	
    Loss 752.01393980628	
    Loss 752.6510806225	
    Loss 753.24287491602	
    Loss 752.89256229293	
    Loss 752.75618745735	
    Loss 752.79260783306	
    Loss 752.42606198666	
    Loss 753.02853089349	
    Loss 753.20134392572	
    Loss 753.8242789645	
    Loss 753.16221567193	
    Loss 752.56377701373	
    Loss 752.47026652749	
    Loss 752.09154708805	
    Loss 752.38246691635	
    Loss 752.53399687192	
    Loss 753.04502580343	
    Loss 753.44051625615	
    Loss 753.13326832004	
    Loss 753.20654922722	
    Loss 753.88037225559	
    Loss 754.37126330086	
    Loss 754.59868966922	
    Loss 754.42916173587	
    Loss 753.90826161191	
    Loss 753.91852309051	
    Loss 753.43669289823	
    Loss 753.35226039808	
    Loss 753.49445150616	
    Loss 754.20787726981	
    Loss 754.35451215943	
    Loss 754.35432734184	
    Loss 754.2324244817	
    Loss 754.21134085509	
    Loss 753.76098406804	
    Loss 753.16424866527	
    Loss 752.57303293531	
    Loss 753.09323457464	
    Loss 752.60405353231	
    Loss 752.46093883456	
    Loss 752.89655120023	
    Loss 752.4003147246	
    Loss 752.0333929263	
    Loss 752.30534721835	
    Loss 751.79602882799	
    Loss 751.97117336831	
    Loss 752.04940604518	
    Loss 751.94410076367	
    Loss 751.80678416944	
    Loss 751.10934871802	
    Loss 751.82220051869	
    Loss 750.34213295797	
    Loss 750.56249794662	
    Loss 751.14365931619	
    Loss 751.96556131618	
    Loss 752.34112193734	
    Loss 752.02362244658	
    Loss 752.47130138289	
    Loss 753.43601439975	
    Loss 753.48931081164	
    Loss 753.32987138015	
    Loss 753.24533777387	
    Loss 752.62278962832	
    Loss 752.8840633987	
    Loss 753.00080577593	
    Loss 753.30959715864	
    Loss 753.42681795399	
    Loss 753.54043454475	
    Loss 753.11370786507	
    Loss 752.98201030348	
    Loss 753.23807067653	
    Loss 752.53869187326	
    Loss 752.0624025299	
    Loss 751.93993741544	
    Loss 752.65441095127	
    Loss 752.56534190374	
    Loss 753.03510876404	
    Loss 753.08916903229	
    Loss 753.73879533012	
    Loss 753.38703936418	
    Loss 753.7293012109	
    Loss 753.45210928346	
    Loss 753.5582039594	
    Loss 753.84158526662	
    Loss 753.29957583923	
    Loss 752.05591981288	
    Loss 752.15296430254	
    Loss 751.66974899983	
    Loss 751.90157966984	
    Loss 752.87918894863	
    Loss 752.18410660237	
    Loss 753.26264890897	
    Loss 753.59254474774	
    Loss 752.32002926908	
    Loss 752.03168340987	
    Loss 752.35468191415	
    Loss 751.95205368638	
    Loss 752.12791926675	
    Loss 752.19192314846	
    Loss 752.32233823224	
    Loss 752.47996970336	
    Loss 752.53322226327	
    Loss 752.89526876	
    Loss 752.95034707699	
    Loss 753.18367710569	
    Loss 753.23113996821	
    Loss 753.42273456633	
    Loss 753.56989335014	
    Loss 752.0311452079	
    Loss 752.203300442	
    Loss 751.64055270528	
    Loss 752.08264551291	
    Loss 753.19506380787	
    Loss 754.38951532824	
    Loss 755.15903268219	
    Loss 755.0457867064	
    Loss 754.47503539355	
    Loss 754.64080304649	
    Loss 753.73710818098	
    Loss 754.67487070159	
    Loss 755.12225053618	
    Loss 754.94448412508	
    Loss 754.97431198572	
    Loss 754.66576229518	
    Loss 754.47292955061	
    Loss 753.62122650512	
    Loss 753.19902923032	
    Loss 753.79645206825	
    Loss 753.30190447783	
    Loss 752.60956168308	
    Loss 752.36457750066	
    Loss 752.54857655608	
    Loss 752.52148716739	
    Loss 753.21309077467	
    Loss 753.41317846508	
    Loss 753.27674301768	
    Loss 752.79249120366	
    Loss 752.32754144307	
    Loss 752.98907866014	
    Loss 752.68980269192	
    Loss 753.03886589689	
    Loss 752.96789932382	
Epoch 10	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0319911933749	
    Loss 752.57146573401	
    Loss 752.01393980506	
    Loss 752.65108061438	
    Loss 753.24287490714	
    Loss 752.89256228406	
    Loss 752.75618745294	
    Loss 752.79260783107	
    Loss 752.42606198514	
    Loss 753.02853089096	
    Loss 753.2013439238	
    Loss 753.82427895655	
    Loss 753.16221566502	
    Loss 752.56377700902	
    Loss 752.47026652025	
    Loss 752.09154708756	
    Loss 752.38246691332	
    Loss 752.5339968742	
    Loss 753.04502580244	
    Loss 753.44051625577	
    Loss 753.13326831726	
    Loss 753.20654922687	
    Loss 753.88037225954	
    Loss 754.37126330355	
    Loss 754.59868967067	
    Loss 754.42916173896	
    Loss 753.90826162156	
    Loss 753.91852309886	
    Loss 753.43669290314	
    Loss 753.35226040285	
    Loss 753.49445151092	
    Loss 754.20787727062	
    Loss 754.35451215896	
    Loss 754.3543273416	
    Loss 754.23242448681	
    Loss 754.21134086417	
    Loss 753.76098407913	
    Loss 753.16424867907	
    Loss 752.57303295253	
    Loss 753.09323459186	
    Loss 752.60405354919	
    Loss 752.46093885313	
    Loss 752.89655122191	
    Loss 752.40031474722	
    Loss 752.03339294488	
    Loss 752.30534723466	
    Loss 751.79602884847	
    Loss 751.97117338596	
    Loss 752.04940606296	
    Loss 751.94410078016	
    Loss 751.80678418581	
    Loss 751.10934873168	
    Loss 751.82220053183	
    Loss 750.34213297215	
    Loss 750.56249796179	
    Loss 751.14365932982	
    Loss 751.96556133063	
    Loss 752.34112195422	
    Loss 752.02362246377	
    Loss 752.47130140028	
    Loss 753.43601441484	
    Loss 753.48931082746	
    Loss 753.32987139425	
    Loss 753.2453377879	
    Loss 752.62278964538	
    Loss 752.8840634162	
    Loss 753.00080579324	
    Loss 753.30959717549	
    Loss 753.42681796985	
    Loss 753.54043455977	
    Loss 753.11370788048	
    Loss 752.9820103195	
    Loss 753.23807069099	
    Loss 752.5386918871	
    Loss 752.06240254462	
    Loss 751.93993743057	
    Loss 752.65441096486	
    Loss 752.56534191842	
    Loss 753.03510877996	
    Loss 753.08916904944	
    Loss 753.73879534371	
    Loss 753.38703937809	
    Loss 753.72930122474	
    Loss 753.45210929631	
    Loss 753.55820397205	
    Loss 753.84158527748	
    Loss 753.29957584986	
    Loss 752.05591982248	
    Loss 752.15296431369	
    Loss 751.66974901053	
    Loss 751.90157967912	
    Loss 752.87918895759	
    Loss 752.18410661191	
    Loss 753.26264891893	
    Loss 753.59254475688	
    Loss 752.32002927873	
    Loss 752.03168341997	
    Loss 752.35468192263	
    Loss 751.95205369426	
    Loss 752.12791927462	
    Loss 752.19192315731	
    Loss 752.32233824101	
    Loss 752.47996971095	
    Loss 752.53322226962	
    Loss 752.89526876633	
    Loss 752.95034708167	
    Loss 753.18367711041	
    Loss 753.23113997291	
    Loss 753.42273457077	
    Loss 753.56989335458	
    Loss 752.03114521293	
    Loss 752.20330044629	
    Loss 751.64055271021	
    Loss 752.08264551583	
    Loss 753.19506380975	
    Loss 754.38951533028	
    Loss 755.15903268527	
    Loss 755.04578670977	
    Loss 754.47503539602	
    Loss 754.64080304846	
    Loss 753.73710818308	
    Loss 754.67487070358	
    Loss 755.12225053787	
    Loss 754.94448412716	
    Loss 754.97431198769	
    Loss 754.66576229654	
    Loss 754.47292955168	
    Loss 753.62122650646	
    Loss 753.19902923148	
    Loss 753.79645206857	
    Loss 753.30190447875	
    Loss 752.60956168418	
    Loss 752.3645775022	
    Loss 752.54857655759	
    Loss 752.52148716759	
    Loss 753.21309077594	
    Loss 753.41317846546	
    Loss 753.27674301812	
    Loss 752.79249120481	
    Loss 752.32754144359	
    Loss 752.98907865975	
    Loss 752.68980269262	
    Loss 753.03886589765	
    Loss 752.96789932477	
Epoch 11	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	6.0319911871567	
    Loss 752.57146573467	
    Loss 752.01393980488	
    Loss 752.65108061366	
    Loss 753.24287490637	
    Loss 752.89256228329	
    Loss 752.75618745252	
    Loss 752.79260783087	
    Loss 752.42606198499	
    Loss 753.02853089075	
    Loss 753.20134392361	
    Loss 753.82427895586	
    Loss 753.16221566441	
    Loss 752.56377700859	
    Loss 752.47026651962	
    Loss 752.09154708748	
    Loss 752.38246691303	
    Loss 752.53399687432	
    Loss 753.0450258023	
    Loss 753.44051625568	
    Loss 753.13326831696	
