[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	1	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2615
 2457
 2098
 1557
 1458
  717
 4122
 2056
 3663
 1882
 4561
 2062
 3336
 2573
 4097
 1364
 3150
 7252
 1201
 2341
 1615
 2251
  903
 3904
 6682
 2520
 5630
 5103
 3323
 5747
 2483
  908
 2396
 1440
 2853
 5599
 3136
 6460
 2470
 1161
 1550
 2126
 1905
 2013
 3068
[torch.DoubleTensor of size 45]

Validation accuracy:	0.027320041272153	
Grad norm	0	
    Loss 2278896.4726198	
    Loss 2246988.5202898	
    Loss 2215539.9489692	
    Loss 2184531.3261285	
    Loss 2153964.775681	
    Loss 2123831.2829378	
    Loss 2094119.9873723	
    Loss 2064828.3987617	
    Loss 2035944.8310999	
    Loss 2007471.3993429	
    Loss 1979391.243257	
    Loss 1951711.9847631	
    Loss 1924420.0054345	
    Loss 1897511.4725098	
    Loss 1870982.8998464	
    Loss 1844820.7641782	
    Loss 1819027.3307965	
    Loss 1793595.400734	
    Loss 1768519.4591618	
    Loss 1743791.0447061	
    Loss 1719409.1824256	
    Loss 1695370.708733	
    Loss 1671667.0367011	
    Loss 1648296.6934067	
    Loss 1625254.506565	
    Loss 1602539.8761781	
    Loss 1580141.8346439	
    Loss 1558054.6046301	
    Loss 1536277.0459376	
    Loss 1514804.6143185	
    Loss 1493633.8949078	
    Loss 1472756.359185	
    Loss 1452170.8116869	
    Loss 1431875.6054167	
    Loss 1411865.4176212	
    Loss 1392133.8757721	
    Loss 1372676.2403124	
    Loss 1353494.662153	
    Loss 1334579.1810744	
    Loss 1315925.0797049	
    Loss 1297532.8754964	
    Loss 1279399.0657393	
    Loss 1261518.7530197	
    Loss 1243893.6928569	
    Loss 1226509.5484569	
    Loss 1209371.654092	
    Loss 1192472.5698506	
    Loss 1175811.1009826	
    Loss 1159384.6952875	
    Loss 1143188.1669135	
    Loss 1127216.6779258	
    Loss 1111463.4632016	
    Loss 1095936.6496258	
    Loss 1080621.205251	
    Loss 1065524.4649686	
    Loss 1050632.4361733	
    Loss 1035952.6432576	
    Loss 1021478.5895368	
    Loss 1007207.5190548	
    Loss 993134.17192888	
    Loss 979261.80642162	
    Loss 965580.83664042	
    Loss 952091.66094385	
    Loss 938790.84317011	
    Loss 925675.83962469	
    Loss 912744.16925537	
    Loss 899991.7172333	
    Loss 887419.61470192	
    Loss 875020.02360234	
    Loss 862794.22212747	
    Loss 850742.18890776	
    Loss 838861.11219064	
    Loss 827145.38433756	
    Loss 815593.44725884	
    Loss 804204.08134355	
    Loss 792972.43626171	
    Loss 781896.41945804	
    Loss 770976.14884375	
    Loss 760207.69768664	
    Loss 749588.6818838	
    Loss 739117.38387547	
    Loss 728791.32016331	
    Loss 718615.08827623	
    Loss 708577.24463923	
    Loss 698680.46732587	
    Loss 688924.10417312	
    Loss 679305.31530811	
    Loss 669816.04059697	
    Loss 660461.19929067	
    Loss 651234.06159674	
    Loss 642136.94518012	
    Loss 633169.60683945	
    Loss 624328.5464949	
    Loss 615611.91192095	
    Loss 607016.15191917	
    Loss 598539.1837084	
    Loss 590179.53994685	
    Loss 581939.81972952	
    Loss 573813.69219538	
    Loss 565799.63653023	
    Loss 557896.57734533	
    Loss 550105.64710026	
    Loss 542422.44962735	
    Loss 534848.21846463	
    Loss 527379.58432997	
    Loss 520017.12412346	
    Loss 512754.97038387	
    Loss 505595.73074017	
    Loss 498534.30232758	
    Loss 491573.82751595	
    Loss 484708.89036682	
    Loss 477940.84199797	
    Loss 471269.47516164	
    Loss 464690.37532978	
    Loss 458202.23998815	
    Loss 451804.99851861	
    Loss 445496.46067949	
    Loss 439276.60012812	
    Loss 433144.23677101	
    Loss 427096.99986317	
    Loss 421134.89615796	
    Loss 415255.55394581	
    Loss 409459.58275548	
    Loss 403743.91318913	
    Loss 398107.56237236	
    Loss 392549.3212443	
    Loss 387068.3837745	
    Loss 381663.59056666	
    Loss 376333.42065081	
    Loss 371081.01121563	
    Loss 365903.12127846	
    Loss 360795.35732892	
    Loss 355760.87976434	
    Loss 350794.692046	
    Loss 345898.43853149	
    Loss 341069.67611128	
    Loss 336309.4681306	
    Loss 331614.7025623	
    Loss 326985.65277583	
    Loss 322422.11114007	
    Loss 317923.28796507	
    Loss 313486.10316289	
    Loss 309111.21207101	
Epoch 2	
 105154
   1036
     69
     13
   5187
      0
      0
   2248
  16914
   1096
     86
      0
      0
      0
      0
      5
[torch.DoubleTensor of size 16]

Validation accuracy:	0.10121540422433	
Grad norm	98.629758720504	
    Loss 306516.60342584	
    Loss 302238.92842442	
    Loss 298021.7406342	
    Loss 293861.43185842	
    Loss 289761.09090709	
    Loss 285717.98629135	
    Loss 281731.94747128	
    Loss 277800.37269569	
    Loss 273923.91851229	
    Loss 270102.68536349	
    Loss 266334.26512771	
    Loss 262618.85634131	
    Loss 258955.6642796	
    Loss 255343.65458952	
    Loss 251781.90588834	
    Loss 248268.81329959	
    Loss 244805.44699978	
    Loss 241390.55402222	
    Loss 238023.82716232	
    Loss 234702.60970373	
    Loss 231428.13140065	
    Loss 228199.74686689	
    Loss 225015.91655249	
    Loss 221877.11135914	
    Loss 218781.81852849	
    Loss 215731.39909362	
    Loss 212722.76255994	
    Loss 209756.30295768	
    Loss 206831.07470543	
    Loss 203946.7709501	
    Loss 201102.9748727	
    Loss 198297.67426958	
    Loss 195531.64110667	
    Loss 192805.12728939	
    Loss 190116.747323	
    Loss 187465.76769786	
    Loss 184851.30314024	
    Loss 182273.76677803	
    Loss 179732.22485281	
    Loss 177225.14861553	
    Loss 174753.60267124	
    Loss 172316.5644309	
    Loss 169913.19757647	
    Loss 167544.79833233	
    Loss 165209.10354699	
    Loss 162906.13924971	
    Loss 160634.98987304	
    Loss 158396.01432075	
    Loss 156188.41719057	
    Loss 154011.66294828	
    Loss 151865.26080733	
    Loss 149747.52061845	
    Loss 147660.46593942	
    Loss 145601.84891885	
    Loss 143573.13083452	
    Loss 141571.28158866	
    Loss 139597.94290735	
    Loss 137652.47689659	
    Loss 135734.27035033	
    Loss 133842.74957449	
    Loss 131977.90644861	
    Loss 130138.92756798	
    Loss 128325.68204796	
    Loss 126537.25605847	
    Loss 124774.3377702	
    Loss 123035.70890425	
    Loss 121321.15564254	
    Loss 119631.30794306	
    Loss 117964.02892927	
    Loss 116320.3283152	
    Loss 114699.85672646	
    Loss 113102.63328599	
    Loss 111527.58042601	
    Loss 109974.58928814	
    Loss 108443.72245054	
    Loss 106933.8473342	
    Loss 105444.55291392	
    Loss 103976.30031294	
    Loss 102528.51970596	
    Loss 101101.00500698	
    Loss 99692.486368625	
    Loss 98303.786341916	
    Loss 96935.351454335	
    Loss 95585.320515841	
    Loss 94254.723226011	
    Loss 92942.478569268	
    Loss 91648.916118512	
    Loss 90372.59895332	
    Loss 89114.476553379	
    Loss 87873.328115982	
    Loss 86650.050634932	
    Loss 85443.804171216	
    Loss 84255.205512483	
    Loss 83083.104026882	
    Loss 81926.894394202	
    Loss 80786.798784025	
    Loss 79662.438521963	
    Loss 78554.056807344	
    Loss 77461.29538436	
    Loss 76383.2897308	
    Loss 75320.298963644	
    Loss 74272.44962477	
    Loss 73238.843820875	
    Loss 72220.220099211	
    Loss 71216.003178068	
    Loss 70225.91327291	
    Loss 69248.831773169	
    Loss 68286.032459488	
    Loss 67335.971977518	
    Loss 66398.984532345	
    Loss 65475.652069956	
    Loss 64565.149150652	
    Loss 63667.983145484	
    Loss 62783.311600201	
    Loss 61910.713701364	
    Loss 61050.698961075	
    Loss 60201.740348531	
    Loss 59365.075494159	
    Loss 58540.068972176	
    Loss 57726.429789072	
    Loss 56924.554539083	
    Loss 56133.769202753	
    Loss 55354.057069903	
    Loss 54585.173222092	
    Loss 53826.915400223	
    Loss 53078.886333025	
    Loss 52341.203321227	
    Loss 51613.836039726	
    Loss 50896.799444591	
    Loss 50190.228911175	
    Loss 49493.534692983	
    Loss 48806.284271996	
    Loss 48128.883851907	
    Loss 47460.508319269	
    Loss 46801.925476048	
    Loss 46152.10924619	
    Loss 45511.789608969	
    Loss 44879.889224172	
    Loss 44257.233880785	
    Loss 43643.358157087	
    Loss 43038.098350942	
    Loss 42441.354944779	
    Loss 41852.8355583	
Epoch 3	
 126057
      3
      0
      0
    194
      0
      0
     26
   5526
      2
[torch.DoubleTensor of size 10]

Validation accuracy:	0.097816521000243	
Grad norm	39.516169573142	
    Loss 41503.577368046	
    Loss 40927.989743213	
    Loss 40360.920615867	
    Loss 39801.080481643	
    Loss 39249.614723106	
    Loss 38705.466991255	
    Loss 38169.342648151	
    Loss 37640.305285703	
    Loss 37118.872231537	
    Loss 36604.866544698	
    Loss 36098.043608783	
    Loss 35598.065877293	
    Loss 35105.121310943	
    Loss 34618.954007911	
    Loss 34139.658957092	
    Loss 33666.810209467	
    Loss 33201.021749332	
    Loss 32741.657177899	
    Loss 32288.852661699	
    Loss 31841.639626115	
    Loss 31401.184291527	
    Loss 30967.051955713	
    Loss 30538.49785373	
    Loss 30116.174640512	
    Loss 29699.223387153	
    Loss 29288.867765031	
    Loss 28883.762313108	
    Loss 28484.70583856	
    Loss 28091.133629717	
    Loss 27703.033872354	
    Loss 27320.674819061	
    Loss 26942.878606506	
    Loss 26570.326913104	
    Loss 26203.451365112	
    Loss 25841.765940788	
    Loss 25485.103876474	
    Loss 25133.160848031	
    Loss 24786.148757269	
    Loss 24444.2207093	
    Loss 24106.538943473	
    Loss 23773.934704418	
    Loss 23445.818039528	
    Loss 23121.787484948	
    Loss 22803.102588357	
    Loss 22489.040342423	
    Loss 22179.176827321	
    Loss 21873.463963613	
    Loss 21572.153286439	
    Loss 21275.080475355	
    Loss 20981.81497122	
    Loss 20692.809561625	
    Loss 20407.666453811	
    Loss 20126.290832737	
    Loss 19849.381669542	
    Loss 19576.487765912	
    Loss 19307.26012549	
    Loss 19041.572704504	
    Loss 18779.73293399	
    Loss 18521.73726791	
    Loss 18267.470947737	
    Loss 18016.408027521	
    Loss 17769.029498116	
    Loss 17525.165649488	
    Loss 17284.15470519	
    Loss 17047.009949049	
    Loss 16812.963207413	
    Loss 16582.147233658	
    Loss 16354.939791855	
    Loss 16130.365773075	
    Loss 15909.100439546	
    Loss 15690.900437763	
    Loss 15475.831409002	
    Loss 15263.757257701	
    Loss 15054.740684965	
    Loss 14848.877379182	
    Loss 14645.774137546	
    Loss 14445.21269341	
    Loss 14247.480507319	
    Loss 14052.586622251	
    Loss 13860.797546005	
    Loss 13670.883767156	
    Loss 13483.970798211	
    Loss 13299.633165325	
    Loss 13117.789249205	
    Loss 12939.021684388	
    Loss 12761.929882554	
    Loss 12587.60125606	
    Loss 12415.746138965	
    Loss 12246.149358066	
    Loss 12078.972443467	
    Loss 11914.547032993	
    Loss 11751.934310682	
    Loss 11592.22344871	
    Loss 11434.596895315	
    Loss 11278.675499238	
    Loss 11125.104451821	
    Loss 10973.702029002	
    Loss 10824.284527145	
    Loss 10677.22988289	
    Loss 10532.012033608	
    Loss 10388.887363063	
    Loss 10247.859869313	
    Loss 10108.594032416	
    Loss 9971.5378940661	
    Loss 9836.6610099948	
    Loss 9703.5080727742	
    Loss 9571.8263166039	
    Loss 9442.410190386	
    Loss 9314.4290796827	
    Loss 9187.5300353104	
    Loss 9063.3110158353	
    Loss 8940.5642267768	
    Loss 8819.912439817	
    Loss 8700.9760352476	
    Loss 8583.5587942581	
    Loss 8468.2703112193	
    Loss 8353.7523682312	
    Loss 8241.1767925542	
    Loss 8130.0275586994	
    Loss 8020.3368735292	
    Loss 7912.4576989516	
    Loss 7806.1261001489	
    Loss 7701.1362773841	
    Loss 7597.6249615961	
    Loss 7495.5119361094	
    Loss 7394.642218221	
    Loss 7295.0564822869	
    Loss 7196.9091457302	
    Loss 7100.6614724285	
    Loss 7005.4735402887	
    Loss 6911.5178811107	
    Loss 6818.9621986802	
    Loss 6727.5859686241	
    Loss 6637.4400369991	
    Loss 6548.9281692161	
    Loss 6461.3110174669	
    Loss 6375.1594692924	
    Loss 6289.8536813543	
    Loss 6206.2073495322	
    Loss 6123.6662626345	
    Loss 6042.173412359	
    Loss 5962.0634447357	
    Loss 5882.9483222272	
Epoch 4	
 128634
      0
      0
      0
     11
      0
      0
      0
   3163
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096837824714737	
Grad norm	17.909619975327	
    Loss 5835.7342395189	
    Loss 5758.2321212571	
    Loss 5682.1942614107	
    Loss 5606.8142085681	
    Loss 5532.6928767106	
    Loss 5459.2572912598	
    Loss 5387.206250807	
    Loss 5315.9467247739	
    Loss 5245.8971062442	
    Loss 5176.7585724684	
    Loss 5108.7027616448	
    Loss 5041.3360394016	
    Loss 4974.8051155703	
    Loss 4909.1765113149	
    Loss 4844.5812388519	
    Loss 4780.8330268271	
    Loss 4718.3918995182	
    Loss 4656.6705185266	
    Loss 4595.8536520802	
    Loss 4535.4256786423	
    Loss 4476.2789182148	
    Loss 4418.1107944749	
    Loss 4360.3134749034	
    Loss 4303.4938312354	
    Loss 4246.9825052917	
    Loss 4191.7642722136	
    Loss 4137.0117085523	
    Loss 4083.3746218012	
    Loss 4030.4620553178	
    Loss 3978.2300814877	
    Loss 3927.0430087353	
    Loss 3875.9492105415	
    Loss 3825.5310514606	
    Loss 3776.114870482	
    Loss 3727.5034207546	
    Loss 3679.5274216957	
    Loss 3632.0679749208	
    Loss 3585.2345667269	
    Loss 3539.3201548935	
    Loss 3493.6649674305	
    Loss 3448.9386928362	
    Loss 3404.6423953869	
    Loss 3360.4830049263	
    Loss 3317.6589295165	
    Loss 3275.5824426124	
    Loss 3233.8972229402	
    Loss 3192.6925494812	
    Loss 3152.1180779151	
    Loss 3112.1474413031	
    Loss 3072.3333176155	
    Loss 3033.2702360707	
    Loss 2994.8657482569	
    Loss 2956.4852989588	
    Loss 2919.4711053307	
    Loss 2882.8090172959	
    Loss 2846.8455644532	
    Loss 2810.9937136852	
    Loss 2775.7536663187	
    Loss 2741.2450520292	
    Loss 2707.3488597629	
    Loss 2673.4888139391	
    Loss 2640.3656909372	
    Loss 2607.7390257346	
    Loss 2575.0325083707	
    Loss 2543.2137179915	
    Loss 2511.6940778489	
    Loss 2480.6154681859	
    Loss 2450.2531091797	
    Loss 2419.8957592669	
    Loss 2390.0717915723	
    Loss 2360.6798140159	
    Loss 2331.6013383817	
    Loss 2302.9673294546	
    Loss 2274.8185401523	
    Loss 2247.2334021325	
    Loss 2220.0001973479	
    Loss 2192.9149296632	
    Loss 2166.1739181522	
    Loss 2139.8787044301	
    Loss 2114.4237090043	
    Loss 2088.6109108732	
    Loss 2063.5395391098	
    Loss 2038.5866477219	
    Loss 2014.0469860892	
    Loss 1990.3732000011	
    Loss 1966.0601041623	
    Loss 1942.3872325881	
    Loss 1919.276024956	
    Loss 1896.1855970702	
    Loss 1873.6488291757	
    Loss 1851.7988096937	
    Loss 1829.7114515452	
    Loss 1808.5107432007	
    Loss 1787.46260296	
    Loss 1766.1818274341	
    Loss 1745.4252225901	
    Loss 1725.0148921242	
    Loss 1704.7118581715	
    Loss 1684.9421273516	
    Loss 1665.3258096466	
    Loss 1646.0578079121	
    Loss 1627.1110223136	
    Loss 1608.2754975536	
    Loss 1589.8972319072	
    Loss 1572.0513060101	
    Loss 1554.2565627243	
    Loss 1536.4439489328	
    Loss 1519.2385837413	
    Loss 1501.9852793484	
    Loss 1484.1654067566	
    Loss 1467.5402455366	
    Loss 1450.8508190955	
    Loss 1434.7279169694	
    Loss 1418.8420287299	
    Loss 1403.0821539532	
    Loss 1388.0428381571	
    Loss 1372.4585450404	
    Loss 1357.3847510814	
    Loss 1342.354277215	
    Loss 1327.4438624492	
    Loss 1312.9821814429	
    Loss 1298.8281392584	
    Loss 1284.6823018226	
    Loss 1270.7545406472	
    Loss 1256.9773053561	
    Loss 1243.2803668168	
    Loss 1229.6376939456	
    Loss 1216.2307886369	
    Loss 1203.6522298037	
    Loss 1190.7480954293	
    Loss 1177.9302095826	
    Loss 1165.462792087	
    Loss 1152.9657433687	
    Loss 1140.704607654	
    Loss 1128.9515985397	
    Loss 1117.0388901926	
    Loss 1105.4858772256	
    Loss 1093.7829606558	
    Loss 1082.7254709528	
    Loss 1071.7287629	
    Loss 1060.7590754787	
    Loss 1050.208192263	
    Loss 1039.6827857931	
Epoch 5	
 128816
      0
      0
      0
      9
      0
      0
      0
   2983
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096989560572955	
Grad norm	10.083486731864	
    Loss 1033.1287388673	
    Loss 1022.6996920768	
    Loss 1012.7630170714	
    Loss 1002.6352722022	
    Loss 992.74497612707	
    Loss 982.67724575512	
    Loss 973.09104665872	
    Loss 963.4596010741	
    Loss 954.18580272558	
    Loss 944.91442021409	
    Loss 935.92145114838	
    Loss 926.80193322968	
    Loss 917.64978053328	
    Loss 908.63645414581	
    Loss 899.86353448256	
    Loss 891.21210842305	
    Loss 883.0853777297	
    Loss 874.89819276919	
    Loss 866.8328965899	
    Loss 858.51611188132	
    Loss 850.70210854791	
    Loss 843.1512591731	
    Loss 835.27915090961	
    Loss 827.66873356056	
    Loss 819.70871483543	
    Loss 812.27639319549	
    Loss 804.70784868619	
    Loss 797.56657364886	
    Loss 790.52409410897	
    Loss 783.50357349949	
    Loss 776.89205727555	
    Loss 769.80544476567	
    Loss 762.78290847125	
    Loss 756.08839075693	
    Loss 749.62657822758	
    Loss 743.19486578263	
    Loss 736.74257512683	
    Loss 730.33368196752	
    Loss 724.28055394498	
    Loss 717.97599264162	
    Loss 712.02570720104	
    Loss 705.95325676564	
    Loss 699.49777599431	
    Loss 693.82685500545	
    Loss 688.3478836542	
    Loss 682.75756170929	
    Loss 677.17188693546	
    Loss 671.69621360542	
    Loss 666.33818236432	
    Loss 660.64429337147	
    Loss 655.22999780952	
    Loss 650.08632731096	
    Loss 644.42457561835	
    Loss 639.74957197332	
    Loss 634.87183780205	
    Loss 630.3419858471	
    Loss 625.4394550444	
    Loss 620.71603065302	
    Loss 616.31621054494	
    Loss 612.09701894101	
    Loss 607.48490428714	
    Loss 603.22631970124	
    Loss 599.0454795461	
    Loss 594.39967077915	
    Loss 590.21773998796	
    Loss 585.9745061682	
    Loss 581.80322426466	
    Loss 577.93943071306	
    Loss 573.74670736086	
    Loss 569.69695517067	
    Loss 565.75151071583	
    Loss 561.70403446771	
    Loss 557.76831690066	
    Loss 553.96462137526	
    Loss 550.36231909892	
    Loss 546.80479607381	
    Loss 543.08416686166	
    Loss 539.36090018669	
    Loss 535.75459453329	
    Loss 532.69519147435	
    Loss 529.00214767838	
    Loss 525.75061259878	
    Loss 522.25934316759	
    Loss 518.92062287755	
    Loss 516.14784334669	
    Loss 512.40037400254	
    Loss 509.01398143788	
    Loss 505.95689528811	
    Loss 502.58821657194	
    Loss 499.55236922614	
    Loss 496.91533481751	
    Loss 493.76544043984	
    Loss 491.22117434322	
    Loss 488.5684175456	
    Loss 485.41215838956	
    Loss 482.5469154021	
    Loss 479.78105862585	
    Loss 476.87095447855	
    Loss 474.23284278283	
    Loss 471.54021302357	
    Loss 468.96102606041	
    Loss 466.45433123643	
    Loss 463.84724017273	
    Loss 461.45030691283	
    Loss 459.36306323269	
    Loss 457.10027188959	
    Loss 454.6405596418	
    Loss 452.55144496036	
    Loss 450.22707702998	
    Loss 447.10266684458	
    Loss 444.96875994262	
    Loss 442.56347075316	
    Loss 440.51430772394	
    Loss 438.49294908468	
    Loss 436.41843506163	
    Loss 434.873997685	
    Loss 432.63192535371	
    Loss 430.69371780845	
    Loss 428.61012534842	
    Loss 426.46435814773	
    Loss 424.57703486279	
    Loss 422.84560469765	
    Loss 420.93432275794	
    Loss 419.07007848333	
    Loss 417.18593919759	
    Loss 415.24124358681	
    Loss 413.18314918339	
    Loss 411.19641513185	
    Loss 409.9151903756	
    Loss 408.0811386302	
    Loss 406.18470067623	
    Loss 404.50845165511	
    Loss 402.62504896065	
    Loss 400.86358557532	
    Loss 399.44779964454	
    Loss 397.73175294142	
    Loss 396.21938345116	
    Loss 394.43039225902	
    Loss 393.15348510603	
    Loss 391.79261971175	
    Loss 390.32144504946	
    Loss 389.13651800539	
    Loss 387.84681187555	
Epoch 6	
 128837
      0
      0
      0
      8
      0
      0
      0
   2963
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097095775673707	
Grad norm	7.3110240669185	
    Loss 386.76501134385	
    Loss 385.37072368013	
    Loss 384.33045339436	
    Loss 382.99750677479	
    Loss 381.74179623558	
    Loss 380.20286505199	
    Loss 379.02103276221	
    Loss 377.68417500463	
    Loss 376.59392045681	
    Loss 375.37216569492	
    Loss 374.32645076732	
    Loss 373.04953519795	
    Loss 371.61130478703	
    Loss 370.21799585061	
    Loss 368.95497153414	
    Loss 367.72596337108	
    Loss 366.91387538579	
    Loss 365.93239410529	
    Loss 364.95975795976	
    Loss 363.6703487314	
    Loss 362.76408015972	
    Loss 362.02652565208	
    Loss 360.87685875642	
    Loss 359.89003126512	
    Loss 358.47324470047	
    Loss 357.46354988763	
    Loss 356.251212505	
    Loss 355.36499794014	
    Loss 354.49826859832	
    Loss 353.55937339099	
    Loss 352.94386629214	
    Loss 351.78817267696	
    Loss 350.61514984084	
    Loss 349.66376019091	
    Loss 348.87510831417	
    Loss 348.02905868438	
    Loss 347.1001879714	
    Loss 346.13519054466	
    Loss 345.44920069824	
    Loss 344.45195898842	
    Loss 343.7263932348	
    Loss 342.80267331705	
    Loss 341.4301399101	
    Loss 340.76385014082	
    Loss 340.20271546739	
    Loss 339.46594210666	
    Loss 338.67635639509	
    Loss 337.9221639394	
    Loss 337.2208319641	
    Loss 336.11758707301	
    Loss 335.23078416324	
    Loss 334.57769839244	
    Loss 333.32046533649	
    Loss 333.01140496271	
    Loss 332.40340961809	
    Loss 332.11244491753	
    Loss 331.3760455234	
    Loss 330.7623570113	
    Loss 330.42086060593	
    Loss 330.19722783294	
    Loss 329.52215505136	
    Loss 329.15353014534	
    Loss 328.80205409451	
    Loss 327.93712205364	
    Loss 327.47144296369	
    Loss 326.90126073998	
    Loss 326.35587266673	
    Loss 326.05591031195	
    Loss 325.38937950527	
    Loss 324.80764092187	
    Loss 324.29473988527	
    Loss 323.61205613589	
    Loss 322.99993397753	
    Loss 322.4697830292	
    Loss 322.08733978837	
    Loss 321.71465555123	
    Loss 321.14079527316	
    Loss 320.51323387811	
    Loss 319.95625330275	
    Loss 319.909680353	
    Loss 319.20112403464	
    Loss 318.89609536392	
    Loss 318.29321350548	
    Loss 317.81415735432	
    Loss 317.86008322994	
    Loss 316.87799169531	
    Loss 316.22185744751	
    Loss 315.87225590361	
    Loss 315.15568594685	
    Loss 314.75255164433	
    Loss 314.70636237837	
    Loss 314.1105798691	
    Loss 314.07930460941	
    Loss 313.90320754622	
    Loss 313.18361277011	
    Loss 312.72894127698	
    Loss 312.33941269329	
    Loss 311.77191461592	
    Loss 311.43643918362	
    Loss 311.02480872646	
    Loss 310.69569065016	
    Loss 310.40173731261	
    Loss 309.98262338343	
    Loss 309.73648548847	
    Loss 309.76972273203	
    Loss 309.59628774062	
    Loss 309.20903255394	
    Loss 309.15631340051	
    Loss 308.84712077356	
    Loss 307.70246781419	
    Loss 307.51896149051	
    Loss 307.03679687796	
    Loss 306.88117330606	
    Loss 306.72155009122	
    Loss 306.48709676101	
    Loss 306.75696422515	
    Loss 306.31727342165	
    Loss 306.14871515021	
    Loss 305.80834146137	
    Loss 305.38051367322	
    Loss 305.18347299415	
    Loss 305.12692307391	
    Loss 304.86252045004	
    Loss 304.6213985252	
    Loss 304.33654537339	
    Loss 303.97761535999	
    Loss 303.48224623053	
    Loss 303.03529021341	
    Loss 303.28469066353	
    Loss 302.93672102084	
    Loss 302.50820438946	
    Loss 302.28610751232	
    Loss 301.82772754485	
    Loss 301.48324406595	
    Loss 301.45837319614	
    Loss 301.11471364307	
    Loss 300.95171245778	
    Loss 300.49740718995	
    Loss 300.53851207826	
    Loss 300.47470531881	
    Loss 300.28239121891	
    Loss 300.3571834747	
    Loss 300.30959980945	
Epoch 7	
 128842
      0
      0
      0
      8
      0
      0
      0
   2958
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097103362466618	
Grad norm	6.3536620071267	
    Loss 299.96297791571	
    Loss 299.78514931741	
    Loss 299.94018171764	
    Loss 299.79338081979	
    Loss 299.69413490306	
    Loss 299.30111999679	
    Loss 299.24750082805	
    Loss 299.02506664554	
    Loss 299.03569290497	
    Loss 298.89257420804	
    Loss 298.91374605561	
    Loss 298.69175811987	
    Loss 298.28692598809	
    Loss 297.91742299837	
    Loss 297.66194852943	
    Loss 297.43239256915	
    Loss 297.60457287202	
    Loss 297.59137422855	
    Loss 297.56906956035	
    Loss 297.22850265483	
    Loss 297.24969612637	
    Loss 297.42771378155	
    Loss 297.18203944581	
    Loss 297.08519121895	
    Loss 296.55043372957	
    Loss 296.4004855019	
    Loss 296.04410766205	
    Loss 295.99723303566	
    Loss 295.96080267664	
    Loss 295.83747828659	
    Loss 296.02641890654	
    Loss 295.67033615756	
    Loss 295.28619865308	
    Loss 295.10392725137	
    Loss 295.07761810212	
    Loss 294.97979608276	
    Loss 294.79425189245	
    Loss 294.56161330148	
    Loss 294.59730823172	
    Loss 294.316787742	
    Loss 294.29507066968	
    Loss 294.06452665018	
    Loss 293.37786214162	
    Loss 293.38534696208	
    Loss 293.48203731296	
    Loss 293.39559903988	
    Loss 293.25103697201	
    Loss 293.13015134139	
    Loss 293.05355655964	
    Loss 292.56637282484	
    Loss 292.28722538509	
    Loss 292.24233446531	
    Loss 291.57715349721	
    Loss 291.85918817884	
    Loss 291.82177333464	
    Loss 292.10281915874	
    Loss 291.92612100214	
    Loss 291.86540295829	
    Loss 292.07107816769	
    Loss 292.38445998506	
    Loss 292.23833415297	
    Loss 292.39383182439	
    Loss 292.55691939534	
    Loss 292.20115773403	
    Loss 292.23343980253	
    Loss 292.15720759893	
    Loss 292.10034902406	
    Loss 292.27805278764	
    Loss 292.08676447321	
    Loss 291.97056646051	
    Loss 291.92124235141	
    Loss 291.68911375638	
    Loss 291.52320512198	
    Loss 291.43166963565	
    Loss 291.47885022826	
    Loss 291.53339121431	
    Loss 291.38293772624	
    Loss 291.17038242285	
    Loss 291.02151182718	
    Loss 291.37888109214	
    Loss 291.07358933373	
    Loss 291.16782077688	
    Loss 290.95281934649	
    Loss 290.85988334924	
    Loss 291.28628979325	
    Loss 290.6746442356	
    Loss 290.38516990415	
    Loss 290.40206594676	
    Loss 290.04088487207	
    Loss 289.99406346721	
    Loss 290.29758883346	
    Loss 290.04675276554	
    Loss 290.35376361775	
    Loss 290.51041145702	
    Loss 290.11691699235	
    Loss 289.98692196104	
    Loss 289.91701013392	
    Loss 289.66454448654	
    Loss 289.63719498154	
    Loss 289.5329417395	
    Loss 289.50749229369	
    Loss 289.51071232385	
    Loss 289.38675271529	
    Loss 289.42938806652	
    Loss 289.74719234383	
    Loss 289.85390921152	
    Loss 289.74696458208	
    Loss 289.96838898642	
    Loss 289.93178640873	
    Loss 289.05356552798	
    Loss 289.13176296943	
    Loss 288.90804783999	
    Loss 289.00656011696	
    Loss 289.09549013896	
    Loss 289.10737290654	
    Loss 289.62015631325	
    Loss 289.42482968354	
    Loss 289.49448193115	
    Loss 289.38836522662	
    Loss 289.19111546447	
    Loss 289.22031951811	
    Loss 289.3895974974	
    Loss 289.34643015357	
    Loss 289.32300544411	
    Loss 289.25230883804	
    Loss 289.10762306962	
    Loss 288.82329736241	
    Loss 288.58402871676	
    Loss 289.04245247846	
    Loss 288.89270974261	
    Loss 288.66058728415	
    Loss 288.63433358005	
    Loss 288.36601079657	
    Loss 288.21317645327	
    Loss 288.37484241527	
    Loss 288.21533508906	
    Loss 288.23282793469	
    Loss 287.95781958599	
    Loss 288.17646536186	
    Loss 288.28687147639	
    Loss 288.26647840027	
    Loss 288.51006954717	
    Loss 288.62886551101	
Epoch 8	
 128839
      0
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	6.0302813201182	
    Loss 288.3804820211	
    Loss 288.36613210962	
    Loss 288.68093264792	
    Loss 288.69427887079	
    Loss 288.74828990942	
    Loss 288.50841768886	
    Loss 288.60522808777	
    Loss 288.53173177725	
    Loss 288.68999186767	
    Loss 288.68998178142	
    Loss 288.85337777279	
    Loss 288.77286734836	
    Loss 288.50509325919	
    Loss 288.27242148321	
    Loss 288.1510123722	
    Loss 288.05575059197	
    Loss 288.35999852408	
    Loss 288.47625597796	
    Loss 288.58002354017	
    Loss 288.36788109755	
    Loss 288.51274348994	
    Loss 288.81316949073	
    Loss 288.68853422027	
    Loss 288.71063301554	
    Loss 288.29475516799	
    Loss 288.25859008438	
    Loss 288.01731196294	
    Loss 288.08223934156	
    Loss 288.15693540043	
    Loss 288.14199289097	
    Loss 288.43807978075	
    Loss 288.18984755196	
    Loss 287.91216832482	
    Loss 287.83179758955	
    Loss 287.90745085053	
    Loss 287.90880572877	
    Loss 287.82297186576	
    Loss 287.68852568901	
    Loss 287.82084624272	
    Loss 287.63735731345	
    Loss 287.71033803327	
    Loss 287.57294360402	
    Loss 286.9790165094	
    Loss 287.07703695427	
    Loss 287.26059161665	
    Loss 287.26041298923	
    Loss 287.20228179152	
    Loss 287.16563926318	
    Loss 287.17216165874	
    Loss 286.76703846371	
    Loss 286.56884787924	
    Loss 286.60697418323	
    Loss 286.02103826023	
    Loss 286.38363705565	
    Loss 286.42134940502	
    Loss 286.77963259611	
    Loss 286.67767705754	
    Loss 286.69115075563	
    Loss 286.97060530835	
    Loss 287.35577540725	
    Loss 287.28030975537	
    Loss 287.50632164093	
    Loss 287.7381700479	
    Loss 287.45082643874	
    Loss 287.5491763452	
    Loss 287.53909824499	
    Loss 287.54798974375	
    Loss 287.78909461392	
    Loss 287.6618247643	
    Loss 287.60771393741	
    Loss 287.62113766087	
    Loss 287.44869792148	
    Loss 287.34229455483	
    Loss 287.30895191461	
    Loss 287.41247965644	
    Loss 287.52388344941	
    Loss 287.43023033201	
    Loss 287.27281095648	
    Loss 287.17794641767	
    Loss 287.58892935764	
    Loss 287.33828314413	
    Loss 287.48709041562	
    Loss 287.32385480129	
    Loss 287.28314630028	
    Loss 287.76100304083	
    Loss 287.19844933608	
    Loss 286.95792143662	
    Loss 287.02473515836	
    Loss 286.71071245236	
    Loss 286.71238579735	
    Loss 287.0632185406	
    Loss 286.85912271045	
    Loss 287.21159083796	
    Loss 287.41264964656	
    Loss 287.06221811495	
    Loss 286.97589539216	
    Loss 286.94879466412	
    Loss 286.73847456982	
    Loss 286.75185393833	
    Loss 286.68896630167	
    Loss 286.70461063159	
    Loss 286.74749629028	
    Loss 286.66339243123	
    Loss 286.74454247993	
    Loss 287.100245776	
    Loss 287.24418362113	
    Loss 287.17530760862	
    Loss 287.43355878783	
    Loss 287.43400163028	
    Loss 286.59150925949	
    Loss 286.70448656705	
    Loss 286.51532071048	
    Loss 286.64771763694	
    Loss 286.7693141709	
    Loss 286.81376097179	
    Loss 287.35869457281	
    Loss 287.1967943596	
    Loss 287.29842170839	
    Loss 287.22359958832	
    Loss 287.05704608111	
    Loss 287.11618067221	
    Loss 287.31587887666	
    Loss 287.3022636219	
    Loss 287.30776859755	
    Loss 287.26539967907	
    Loss 287.14972717494	
    Loss 286.8939663789	
    Loss 286.68272289004	
    Loss 287.17024805676	
    Loss 287.04649239282	
    Loss 286.84031457155	
    Loss 286.84038388404	
    Loss 286.59693779389	
    Loss 286.47017231128	
    Loss 286.65661132917	
    Loss 286.52158441324	
    Loss 286.56291398135	
    Loss 286.31183743536	
    Loss 286.55436002614	
    Loss 286.68802394713	
    Loss 286.69064948587	
    Loss 286.95661988731	
    Loss 287.09744520653	
Epoch 9	
 128838
      0
      0
      0
      8
      0
      0
      0
   2962
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097156470016994	
Grad norm	5.9243437383384	
    Loss 286.86199632994	
    Loss 286.86949713842	
    Loss 287.20535103998	
    Loss 287.24037683449	
    Loss 287.31410680539	
    Loss 287.0943984991	
    Loss 287.21089007612	
    Loss 287.15701156175	
    Loss 287.33490299007	
    Loss 287.35335963884	
    Loss 287.53534239143	
    Loss 287.47366138259	
    Loss 287.22356902193	
    Loss 287.00892233305	
    Loss 286.9049398998	
    Loss 286.8276064595	
    Loss 287.14943374126	
    Loss 287.28275658247	
    Loss 287.40279249625	
    Loss 287.20813055117	
    Loss 287.36916307321	
    Loss 287.685725005	
    Loss 287.57709676581	
    Loss 287.61486079296	
    Loss 287.21499013113	
    Loss 287.19341539322	
    Loss 286.96755326869	
    Loss 287.04707766145	
    Loss 287.13645875554	
    Loss 287.13555184018	
    Loss 287.44563647794	
    Loss 287.2119674659	
    Loss 286.94867175428	
    Loss 286.88140895273	
    Loss 286.9705329087	
    Loss 286.98465343389	
    Loss 286.91206514043	
    Loss 286.79066397349	
    Loss 286.93576758158	
    Loss 286.76549136164	
    Loss 286.85116098479	
    Loss 286.72622245449	
    Loss 286.14490621566	
    Loss 286.2550292437	
    Loss 286.44964639014	
    Loss 286.46058944299	
    Loss 286.41392566995	
    Loss 286.38823331845	
    Loss 286.40555581558	
    Loss 286.01113919256	
    Loss 285.8235144274	
    Loss 285.87320266503	
    Loss 285.29775640333	
    Loss 285.67152103528	
    Loss 285.71871696799	
    Loss 286.08742957227	
    Loss 285.99528839387	
    Loss 286.01864050384	
    Loss 286.30804074242	
    Loss 286.70266138185	
    Loss 286.63648764513	
    Loss 286.87193923298	
    Loss 287.11283476971	
    Loss 286.8346225148	
    Loss 286.94150266527	
    Loss 286.94018723127	
    Loss 286.95789791309	
    Loss 287.20719418123	
    Loss 287.08853331341	
    Loss 287.04256115063	
    Loss 287.06451995576	
    Loss 286.89976160202	
    Loss 286.80114762112	
    Loss 286.77532162591	
    Loss 286.88588794556	
    Loss 287.00470178104	
    Loss 286.91860693399	
    Loss 286.7683349915	
    Loss 286.68040387607	
    Loss 287.09829758244	
    Loss 286.85510386213	
    Loss 287.01154374639	
    Loss 286.85510271197	
    Loss 286.82148328841	
    Loss 287.30632708494	
    Loss 286.75008461041	
    Loss 286.51598956212	
    Loss 286.58970764579	
    Loss 286.28177822515	
    Loss 286.29014104568	
    Loss 286.64741023551	
    Loss 286.44970375941	
    Loss 286.8082554404	
    Loss 287.01512850802	
    Loss 286.67016582651	
    Loss 286.58969610648	
    Loss 286.56826427646	
    Loss 286.36349502732	
    Loss 286.3820789206	
    Loss 286.32473313196	
    Loss 286.34598452864	
    Loss 286.39407826371	
    Loss 286.31537008598	
    Loss 286.40156693685	
    Loss 286.76221380644	
    Loss 286.91097294164	
    Loss 286.84731449238	
    Loss 287.110486065	
    Loss 287.11602029344	
    Loss 286.27826974487	
    Loss 286.39573868126	
    Loss 286.21112484048	
    Loss 286.34796424167	
    Loss 286.47367166188	
    Loss 286.52226819082	
    Loss 287.07132264817	
    Loss 286.91409789579	
    Loss 287.01998307658	
    Loss 286.9492723445	
    Loss 286.78671118228	
    Loss 286.84968067242	
    Loss 287.05345971204	
    Loss 287.04373331018	
    Loss 287.05298337874	
    Loss 287.01423095193	
    Loss 286.90250631061	
    Loss 286.65063184213	
    Loss 286.44317302948	
    Loss 286.93494144766	
    Loss 286.8144321957	
    Loss 286.61156379501	
    Loss 286.61515140619	
    Loss 286.37478936338	
    Loss 286.25162538282	
    Loss 286.44126294211	
    Loss 286.3094015275	
    Loss 286.35377092007	
    Loss 286.10583134162	
    Loss 286.35154962703	
    Loss 286.4882695664	
    Loss 286.49394680031	
    Loss 286.76279704723	
    Loss 286.90644511077	
Epoch 10	
 128839
      0
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	5.8901481046965	
    Loss 286.67263078763	
    Loss 286.68300948917	
    Loss 287.02152988754	
    Loss 287.05951226172	
    Loss 287.13556589839	
    Loss 286.91840323484	
    Loss 287.03733286325	
    Loss 286.98593618388	
    Loss 287.16637966963	
    Loss 287.18703010912	
    Loss 287.37130479093	
    Loss 287.31208278566	
    Loss 287.06409369855	
    Loss 286.8517282904	
    Loss 286.7498599226	
    Loss 286.67487573028	
    Loss 286.9989920122	
    Loss 287.13447678249	
    Loss 287.25644696268	
    Loss 287.06419758189	
    Loss 287.22722648931	
    Loss 287.54583306877	
    Loss 287.43924808623	
    Loss 287.4789918856	
    Loss 287.08127005421	
    Loss 287.06139784934	
    Loss 286.83758004804	
    Loss 286.91890542935	
    Loss 287.01015860771	
    Loss 287.01093392325	
    Loss 287.32275233221	
    Loss 287.0910575443	
    Loss 286.82971101737	
    Loss 286.76399404666	
    Loss 286.8548407631	
    Loss 286.87046744205	
    Loss 286.79958927171	
    Loss 286.67987908816	
    Loss 286.82661807511	
    Loss 286.65816650441	
    Loss 286.74551621512	
    Loss 286.62222272913	
    Loss 286.04264964371	
    Loss 286.15436918129	
    Loss 286.35024715587	
    Loss 286.36250829383	
    Loss 286.31732622452	
    Loss 286.29296470499	
    Loss 286.31159567897	
    Loss 285.91849480684	
    Loss 285.73216890518	
    Loss 285.78355037526	
    Loss 285.20945279972	
    Loss 285.58483001221	
    Loss 285.63307337516	
    Loss 286.00318996628	
    Loss 285.91227692383	
    Loss 285.93691786157	
    Loss 286.2276574717	
    Loss 286.62346827263	
    Loss 286.55846359181	
    Loss 286.79515995246	
    Loss 287.03719452539	
    Loss 286.7601773474	
    Loss 286.86807454557	
    Loss 286.8678861414	
    Loss 286.88676840933	
    Loss 287.13704266745	
    Loss 287.0195340522	
    Loss 286.97457967776	
    Loss 286.99771278186	
    Loss 286.83386302809	
    Loss 286.73621593794	
    Loss 286.71128867441	
    Loss 286.82260612462	
    Loss 286.94232964707	
    Loss 286.85721889122	
    Loss 286.70781021837	
    Loss 286.62069477595	
    Loss 287.03940246291	
    Loss 286.79724037885	
    Loss 286.95481054116	
    Loss 286.7992214773	
    Loss 286.7665711495	
    Loss 287.25237428086	
    Loss 286.69687199618	
    Loss 286.46358816309	
    Loss 286.53829832037	
    Loss 286.23109810603	
    Loss 286.24041366397	
    Loss 286.59857252803	
    Loss 286.40176035625	
    Loss 286.76111924757	
    Loss 286.96871070256	
    Loss 286.62436051291	
    Loss 286.54466770418	
    Loss 286.52396308992	
    Loss 286.31989117151	
    Loss 286.33907605382	
    Loss 286.2824625831	
    Loss 286.30449814811	
    Loss 286.35324563686	
    Loss 286.27527324265	
    Loss 286.36210029381	
    Loss 286.72335497626	
    Loss 286.87269401743	
    Loss 286.80976573812	
    Loss 287.07358612277	
    Loss 287.07983943362	
    Loss 286.24269979734	
    Loss 286.36069871485	
    Loss 286.17666089909	
    Loss 286.31405733094	
    Loss 286.44021691793	
    Loss 286.48928398627	
    Loss 287.0388182591	
    Loss 286.88228343274	
    Loss 286.98872304213	
    Loss 286.91852690803	
    Loss 286.75645003082	
    Loss 286.81986648476	
    Loss 287.02418506898	
    Loss 287.01495008956	
    Loss 287.02464833752	
    Loss 286.9863087861	
    Loss 286.87512674534	
    Loss 286.62378768367	
    Loss 286.41684107684	
    Loss 286.90929343116	
    Loss 286.78913221138	
    Loss 286.58664361957	
    Loss 286.59069451414	
    Loss 286.35065117195	
    Loss 286.22800572319	
    Loss 286.41802214747	
    Loss 286.28653608385	
    Loss 286.33125468917	
    Loss 286.08370519032	
    Loss 286.32984528947	
    Loss 286.46695008243	
    Loss 286.47302230056	
    Loss 286.7422113394	
    Loss 286.88618667831	
Epoch 11	
 128839
      0
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	5.8789474475164	
    Loss 286.65255431387	
    Loss 286.6632969367	
    Loss 287.00211610129	
    Loss 287.04051010605	
    Loss 287.11675829699	
    Loss 286.8998771147	
    Loss 287.0190580949	
    Loss 286.96793867634	
    Loss 287.14869351215	
    Loss 287.16953401318	
    Loss 287.3540400592	
    Loss 287.29512398295	
    Loss 287.04731963388	
    Loss 286.83520949319	
    Loss 286.73354116643	
    Loss 286.65884827322	
    Loss 286.9832441999	
    Loss 287.11897077112	
    Loss 287.24110972681	
    Loss 287.04920471657	
    Loss 287.21243603705	
    Loss 287.53127187831	
    Loss 287.42492040087	
    Loss 287.46488419675	
    Loss 287.06744830583	
    Loss 287.04771233907	
    Loss 286.82415899822	
    Loss 286.90566867036	
    Loss 286.99713539789	
    Loss 286.99806133878	
    Loss 287.31006094546	
    Loss 287.07863737158	
    Loss 286.81755717797	
    Loss 286.7519710039	
    Loss 286.8430184481	
    Loss 286.85877217304	
    Loss 286.78809604053	
    Loss 286.66859041266	
    Loss 286.81551834244	
    Loss 286.64732741849	
    Loss 286.73489192848	
    Loss 286.61180895661	
    Loss 286.0324878612	
    Loss 286.14441077222	
    Loss 286.34037677414	
    Loss 286.35275104886	
    Loss 286.30774649007	
    Loss 286.28351256108	
    Loss 286.3022664936	
    Loss 285.90929736083	
    Loss 285.7231013979	
    Loss 285.77476025976	
    Loss 285.20082241966	
    Loss 285.57645520036	
    Loss 285.62475703907	
    Loss 285.99505975931	
    Loss 285.90427818213	
    Loss 285.92907799927	
    Loss 286.21999746247	
    Loss 286.61593805321	
    Loss 286.55106127906	
    Loss 286.78791442307	
    Loss 287.03007357253	
    Loss 286.75320396215	
    Loss 286.86119143749	
    Loss 286.86113636394	
    Loss 286.88017005174	
    Loss 287.13053157847	
    Loss 287.01317524048	
    Loss 286.96833085278	
    Loss 286.99162989474	
    Loss 286.82785852071	
    Loss 286.73031202842	
    Loss 286.70546603518	
    Loss 286.81681463517	
    Loss 286.93662963298	
    Loss 286.85163932974	
    Loss 286.70231181513	
    Loss 286.61526553092	
    Loss 287.03403964929	
    Loss 286.79202537251	
    Loss 286.949785308	
    Loss 286.7942889458	
    Loss 286.76177299104	
    Loss 287.24771162147	
    Loss 286.6922693035	
    Loss 286.45907608445	
    Loss 286.53394180719	
    Loss 286.22680780888	
    Loss 286.23626908909	
    Loss 286.59455619489	
    Loss 286.39787689747	
    Loss 286.75734106668	
    Loss 286.96500492019	
    Loss 286.6206913254	
    Loss 286.54109896994	
    Loss 286.52047923942	
    Loss 286.31648187347	
    Loss 286.33571247781	
    Loss 286.27919176958	
    Loss 286.30134469146	
    Loss 286.35016365321	
    Loss 286.27229366135	
    Loss 286.35918854298	
    Loss 286.72050450183	
    Loss 286.86989685637	
    Loss 286.80707559199	
    Loss 287.07097878555	
    Loss 287.07734027447	
    Loss 286.24027249743	
    Loss 286.35831437608	
    Loss 286.17434089101	
    Loss 286.31179845511	
    Loss 286.43798342218	
    Loss 286.48708116715	
    Loss 287.03665333832	
    Loss 286.88023259539	
    Loss 286.98673979035	
    Loss 286.91659852949	
    Loss 286.75456718586	
    Loss 286.81801940297	
    Loss 287.02240589977	
    Loss 287.0132258724	
    Loss 287.02296392772	
    Loss 286.98465266285	
    Loss 286.87354674717	
    Loss 286.6222837058	
    Loss 286.4154068904	
    Loss 286.90799078967	
    Loss 286.78784519865	
    Loss 286.58538446407	
    Loss 286.58949393265	
    Loss 286.34945870268	
    Loss 286.22689569866	
    Loss 286.4169439929	
    Loss 286.28548918321	
    Loss 286.33023374228	
    Loss 286.08272486026	
    Loss 286.32891836866	
    Loss 286.46606588161	
    Loss 286.47218619119	
    Loss 286.7414031536	
    Loss 286.88540346379	
Epoch 12	
 128839
      0
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	5.8752162209309	
    Loss 286.65178214895	
    Loss 286.66256537937	
    Loss 287.00140334333	
    Loss 287.03985787741	
    Loss 287.1160902103	
    Loss 286.8992249608	
    Loss 287.01841201881	
    Loss 286.96730983779	
    Loss 287.14809534802	
    Loss 287.16892372005	
    Loss 287.35343280926	
    Loss 287.29454982252	
    Loss 287.04673554659	
    Loss 286.8346414602	
    Loss 286.73296953807	
    Loss 286.65830650322	
    Loss 286.98272979018	
    Loss 287.11847117301	
    Loss 287.24060019865	
    Loss 287.04874824082	
    Loss 287.21198266753	
    Loss 287.53083299256	
    Loss 287.42449771632	
    Loss 287.46447454258	
    Loss 287.06707574388	
    Loss 287.0473252144	
    Loss 286.82380393945	
    Loss 286.90531792529	
    Loss 286.99679947886	
    Loss 286.99771816791	
    Loss 287.30972418889	
    Loss 287.07833945034	
    Loss 286.81729649204	
    Loss 286.75170074792	
    Loss 286.8427646049	
    Loss 286.85850862301	
    Loss 286.78784907452	
    Loss 286.66836299613	
    Loss 286.81530513742	
    Loss 286.64715438578	
    Loss 286.73474343183	
    Loss 286.61168517641	
    Loss 286.03240469686	
    Loss 286.14435101917	
    Loss 286.34029974538	
    Loss 286.35266631258	
    Loss 286.30767799061	
    Loss 286.28344279764	
    Loss 286.3021939922	
    Loss 285.90922629839	
    Loss 285.72303160286	
    Loss 285.77474582785	
    Loss 285.20082205962	
    Loss 285.57650297053	
    Loss 285.62478307821	
    Loss 285.99510900399	
    Loss 285.9043329625	
    Loss 285.92914903415	
    Loss 286.22009245643	
    Loss 286.61603938953	
    Loss 286.55116933239	
    Loss 286.78803936356	
    Loss 287.03020492853	
    Loss 286.75335020344	
    Loss 286.86133335705	
    Loss 286.86128994582	
    Loss 286.88034162393	
    Loss 287.13069923078	
    Loss 287.01336232235	
    Loss 286.96852340908	
    Loss 286.99184734034	
    Loss 286.82807118082	
    Loss 286.73052759689	
    Loss 286.70567868078	
    Loss 286.81700659951	
    Loss 286.93682303417	
    Loss 286.85184468122	
    Loss 286.70251571626	
    Loss 286.61546474698	
    Loss 287.03423196445	
    Loss 286.79224050116	
    Loss 286.95003982857	
    Loss 286.7945483099	
    Loss 286.76205148948	
    Loss 287.24801061873	
    Loss 286.69256204899	
    Loss 286.45937480628	
    Loss 286.53426935286	
    Loss 286.22713312677	
    Loss 286.2366199618	
    Loss 286.59492749984	
    Loss 286.39827070041	
    Loss 286.75774815989	
    Loss 286.96541270944	
    Loss 286.62108716756	
    Loss 286.54150682281	
    Loss 286.52089390142	
    Loss 286.3168992515	
    Loss 286.33612364683	
    Loss 286.27961310039	
    Loss 286.30178654032	
    Loss 286.35060941776	
    Loss 286.27275446907	
    Loss 286.35965268389	
    Loss 286.72096962782	
    Loss 286.87036035622	
    Loss 286.80755631439	
    Loss 287.07146919238	
    Loss 287.07784918197	
    Loss 286.24078724759	
    Loss 286.3588242813	
    Loss 286.1748547601	
    Loss 286.31231589451	
    Loss 286.43849185086	
    Loss 286.48758167989	
    Loss 287.03714949576	
    Loss 286.88075158397	
    Loss 286.98726528614	
    Loss 286.91712614012	
    Loss 286.75509373465	
    Loss 286.81854219333	
    Loss 287.02293578853	
    Loss 287.01375927957	
    Loss 287.02349522926	
    Loss 286.98517777678	
    Loss 286.87408297246	
    Loss 286.62283149197	
    Loss 286.4159643931	
    Loss 286.9085799108	
    Loss 286.78842547841	
    Loss 286.58596027812	
    Loss 286.59007629728	
    Loss 286.35002965901	
    Loss 286.2274825255	
    Loss 286.41752810976	
    Loss 286.28607028521	
    Loss 286.33081097103	
    Loss 286.0833032105	
    Loss 286.3295025932	
    Loss 286.46665279221	
    Loss 286.47277798401	
    Loss 286.74199234601	
    Loss 286.88598916072	
Epoch 13	
 128839
      0
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	5.8740641801304	
    Loss 286.65236460199	
    Loss 286.66315034475	
    Loss 287.00198322796	
    Loss 287.04044785568	
    Loss 287.11666311968	
    Loss 286.89979216273	
    Loss 287.01897002217	
    Loss 286.96786308542	
    Loss 287.14864881992	
