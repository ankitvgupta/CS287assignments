[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.1	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 6293
 5899
 1846
 3951
 3317
 1147
 1859
 1460
  986
 2513
 4835
 2880
 7964
 1477
 4112
 1409
 1776
 3189
 1269
 2417
 4489
 5843
 1153
 4688
 2521
 2136
 2583
 2240
 2080
  978
 1818
 2160
 3093
 5399
 2343
 1568
 1811
 1225
 3089
 1242
 3796
 3530
 2403
 7415
 1606
[torch.DoubleTensor of size 45]

Validation accuracy:	0.017980699198835	
Grad norm	0	
    Loss 11389948.432705	
    Loss 11310235.266924	
    Loss 11231103.37521	
    Loss 11152530.813891	
    Loss 11074510.980512	
    Loss 10997035.358356	
    Loss 10920102.700493	
    Loss 10843707.756607	
    Loss 10767849.112074	
    Loss 10692521.135212	
    Loss 10617725.681214	
    Loss 10543448.710091	
    Loss 10469692.454613	
    Loss 10396451.913994	
    Loss 10323726.831943	
    Loss 10251509.109674	
    Loss 10179797.603716	
    Loss 10108588.447077	
    Loss 10037879.445461	
    Loss 9967664.1537767	
    Loss 9897939.298309	
    Loss 9828702.5315555	
    Loss 9759952.154245	
    Loss 9691680.4056252	
    Loss 9623888.6151662	
    Loss 9556571.3864577	
    Loss 9489723.6722541	
    Loss 9423343.2765548	
    Loss 9357429.5538357	
    Loss 9291976.8444199	
    Loss 9226981.8786686	
    Loss 9162440.66071	
    Loss 9098349.3680672	
    Loss 9034709.8960445	
    Loss 8971516.7355215	
    Loss 8908764.5994286	
    Loss 8846451.0138073	
    Loss 8784574.4457683	
    Loss 8723129.19665	
    Loss 8662114.5551381	
    Loss 8601526.3423402	
    Loss 8541362.4488938	
    Loss 8481618.4989347	
    Loss 8422292.5420058	
    Loss 8363383.3742514	
    Loss 8304884.8257324	
    Loss 8246796.8033001	
    Loss 8189114.6652478	
    Loss 8131835.5660338	
    Loss 8074956.7044308	
    Loss 8018475.8972668	
    Loss 7962391.1280342	
    Loss 7906699.7976409	
    Loss 7851397.7709757	
    Loss 7796481.3082505	
    Loss 7741948.7888681	
    Loss 7687798.7539585	
    Loss 7634028.0257495	
    Loss 7580632.9861405	
    Loss 7527611.3157676	
    Loss 7474962.0857409	
    Loss 7422681.4490159	
    Loss 7370765.6575064	
    Loss 7319211.2117321	
    Loss 7268018.2195483	
    Loss 7217184.2473253	
    Loss 7166705.4925975	
    Loss 7116578.9722001	
    Loss 7066803.8162531	
    Loss 7017377.8179245	
    Loss 6968298.0366726	
    Loss 6919562.0400677	
    Loss 6871165.9744894	
    Loss 6823109.4345016	
    Loss 6775388.1611188	
    Loss 6727998.8299261	
    Loss 6680942.7874688	
    Loss 6634215.4020169	
    Loss 6587814.983951	
    Loss 6541737.9003659	
    Loss 6495984.5706245	
    Loss 6450551.1592739	
    Loss 6405436.7849416	
    Loss 6360636.9107602	
    Loss 6316151.0683534	
    Loss 6271976.1202416	
    Loss 6228109.5111427	
    Loss 6184550.5348011	
    Loss 6141294.699577	
    Loss 6098342.4980646	
    Loss 6055689.5873738	
    Loss 6013337.351498	
    Loss 5971279.5315469	
    Loss 5929516.6759069	
    Loss 5888047.7090821	
    Loss 5846867.6617679	
    Loss 5805975.5873497	
    Loss 5765372.2104749	
    Loss 5725050.4291823	
    Loss 5685009.1485291	
    Loss 5645248.0682585	
    Loss 5605766.6251729	
    Loss 5566559.858009	
    Loss 5527628.330807	
    Loss 5488968.0144742	
    Loss 5450578.7053365	
    Loss 5412457.8932776	
    Loss 5374604.8889795	
    Loss 5337016.7822044	
    Loss 5299691.5104291	
    Loss 5262625.9625154	
    Loss 5225821.192165	
    Loss 5189274.7955586	
    Loss 5152981.3608434	
    Loss 5116942.1274709	
    Loss 5081154.0518041	
    Loss 5045617.1555727	
    Loss 5010329.8034992	
    Loss 4975288.9230611	
    Loss 4940494.1247036	
    Loss 4905942.0955115	
    Loss 4871631.2023144	
    Loss 4837559.7458688	
    Loss 4803727.194047	
    Loss 4770132.0260756	
    Loss 4736771.8442572	
    Loss 4703644.3715892	
    Loss 4670749.2305493	
    Loss 4638082.9101478	
    Loss 4605646.1318156	
    Loss 4573437.1878422	
    Loss 4541452.6370002	
    Loss 4509691.5806908	
    Loss 4478152.9823947	
    Loss 4446834.482489	
    Loss 4415734.5192312	
    Loss 4384853.2329391	
    Loss 4354187.2089201	
    Loss 4323736.1104953	
    Loss 4293497.8602606	
    Loss 4263472.2379117	
    Loss 4233654.8108744	
    Loss 4204047.0106225	
Epoch 2	
 68675
  6009
  1404
   499
 13785
    79
    59
  9152
 23226
  6198
  1546
    79
    77
    66
    72
   700
    95
    87
[torch.DoubleTensor of size 18]

Validation accuracy:	0.096314336003884	
Grad norm	777.92145171652	
    Loss 4186382.5478871	
    Loss 4157104.5275593	
    Loss 4128031.7974568	
    Loss 4099161.7359687	
    Loss 4070494.4187343	
    Loss 4042027.9763202	
    Loss 4013759.9794019	
    Loss 3985690.089501	
    Loss 3957815.5499907	
    Loss 3930136.862491	
    Loss 3902653.2815756	
    Loss 3875360.4057222	
    Loss 3848258.3440689	
    Loss 3821346.180961	
    Loss 3794623.2253047	
    Loss 3768085.8293121	
    Loss 3741734.1904587	
    Loss 3715566.7269795	
    Loss 3689583.4194452	
    Loss 3663780.9941356	
    Loss 3638158.3512479	
    Loss 3612715.323061	
    Loss 3587450.6197673	
    Loss 3562361.6423613	
    Loss 3537449.8437229	
    Loss 3512712.5249806	
    Loss 3488147.5360982	
    Loss 3463753.8997267	
    Loss 3439531.4096538	
    Loss 3415478.2988281	
    Loss 3391593.2570609	
    Loss 3367874.6750518	
    Loss 3344321.1286059	
    Loss 3320933.456094	
    Loss 3297710.1212023	
    Loss 3274648.6373532	
    Loss 3251748.3914679	
    Loss 3229008.9418051	
    Loss 3206427.3028723	
    Loss 3184004.5553317	
    Loss 3161738.5018688	
    Loss 3139628.4259293	
    Loss 3117672.3314223	
    Loss 3095870.384209	
    Loss 3074221.0590018	
    Loss 3052722.0876534	
    Loss 3031374.6182847	
    Loss 3010176.0032324	
    Loss 2989125.5032885	
    Loss 2968222.1145056	
    Loss 2947464.9688098	
    Loss 2926853.214485	
    Loss 2906386.4087239	
    Loss 2886062.0864018	
    Loss 2865879.8041367	
    Loss 2845838.422027	
    Loss 2825937.7927053	
    Loss 2806176.5208837	
    Loss 2786553.1515914	
    Loss 2767067.1158152	
    Loss 2747717.8544617	
    Loss 2728504.504848	
    Loss 2709424.6959547	
    Loss 2690477.231909	
    Loss 2671663.1213106	
    Loss 2652980.7029344	
    Loss 2634428.7159957	
    Loss 2616006.2138193	
    Loss 2597712.7460649	
    Loss 2579547.5778549	
    Loss 2561510.2545318	
    Loss 2543599.270021	
    Loss 2525812.8405115	
    Loss 2508151.3425127	
    Loss 2490612.8344872	
    Loss 2473196.5560129	
    Loss 2455902.3879643	
    Loss 2438729.3347664	
    Loss 2421676.1507363	
    Loss 2404741.7245923	
    Loss 2387926.2334766	
    Loss 2371228.5273239	
    Loss 2354648.1173231	
    Loss 2338183.0594339	
    Loss 2321833.6258692	
    Loss 2305598.2834294	
    Loss 2289476.2343088	
    Loss 2273467.0283544	
    Loss 2257569.2047649	
    Loss 2241783.4647909	
    Loss 2226107.3838453	
    Loss 2210541.9714746	
    Loss 2195084.7894167	
    Loss 2179735.9376507	
    Loss 2164495.1342232	
    Loss 2149360.4422119	
    Loss 2134331.6101649	
    Loss 2119409.1891598	
    Loss 2104589.6837491	
    Loss 2089873.4296107	
    Loss 2075260.2000083	
    Loss 2060749.6630313	
    Loss 2046339.8696096	
    Loss 2032031.5119463	
    Loss 2017822.7211519	
    Loss 2003713.3850228	
    Loss 1989702.9124337	
    Loss 1975791.0613533	
    Loss 1961976.3561964	
    Loss 1948258.3170771	
    Loss 1934635.6166015	
    Loss 1921108.7235188	
    Loss 1907676.9094785	
    Loss 1894337.8222845	
    Loss 1881092.3539138	
    Loss 1867939.0131463	
    Loss 1854878.0024595	
    Loss 1841908.7492316	
    Loss 1829030.103583	
    Loss 1816241.928715	
    Loss 1803543.0877854	
    Loss 1790932.797729	
    Loss 1778410.2615863	
    Loss 1765975.7564544	
    Loss 1753628.5024364	
    Loss 1741367.4867091	
    Loss 1729191.9145123	
    Loss 1717101.9809617	
    Loss 1705095.8901377	
    Loss 1693174.1506272	
    Loss 1681336.2696953	
    Loss 1669580.6737461	
    Loss 1657907.3468885	
    Loss 1646315.5781166	
    Loss 1634804.9292999	
    Loss 1623374.5746045	
    Loss 1612024.6616389	
    Loss 1600753.6966281	
    Loss 1589561.7943601	
    Loss 1578448.0981342	
    Loss 1567412.5985544	
    Loss 1556453.4973319	
    Loss 1545571.4111762	
Epoch 3	
 96131
  1924
   155
    15
  7944
     1
     0
  4061
 19535
  1817
   188
     0
     1
     1
     0
    35
[torch.DoubleTensor of size 16]

Validation accuracy:	0.10122299101724	
Grad norm	473.59189308785	
    Loss 1539078.9969361	
    Loss 1528317.9817256	
    Loss 1517632.6322743	
    Loss 1507021.7680009	
    Loss 1496485.3673019	
    Loss 1486022.8435663	
    Loss 1475633.3057997	
    Loss 1465316.3704254	
    Loss 1455071.2183466	
    Loss 1444898.290828	
    Loss 1434797.5028021	
    Loss 1424766.1304323	
    Loss 1414804.7183837	
    Loss 1404913.2889763	
    Loss 1395091.6234501	
    Loss 1385337.9560049	
    Loss 1375652.4992744	
    Loss 1366034.8815124	
    Loss 1356485.1002984	
    Loss 1347001.5340221	
    Loss 1337584.0780205	
    Loss 1328232.8528152	
    Loss 1318947.0742446	
    Loss 1309725.778854	
    Loss 1300569.6483333	
    Loss 1291477.7043038	
    Loss 1282448.9494891	
    Loss 1273483.105072	
    Loss 1264580.3125989	
    Loss 1255739.655963	
    Loss 1246960.9301472	
    Loss 1238243.2083238	
    Loss 1229585.9946259	
    Loss 1220989.8297958	
    Loss 1212454.2484991	
    Loss 1203978.0012097	
    Loss 1195561.0355543	
    Loss 1187203.0922886	
    Loss 1178903.1326833	
    Loss 1170661.7796355	
    Loss 1162477.9723577	
    Loss 1154351.6666233	
    Loss 1146281.6485329	
    Loss 1138268.4176811	
    Loss 1130311.4389483	
    Loss 1122409.3341006	
    Loss 1114563.2778578	
    Loss 1106771.7219494	
    Loss 1099034.4704619	
    Loss 1091351.2948379	
    Loss 1083721.8250274	
    Loss 1076146.0295562	
    Loss 1068623.4208029	
    Loss 1061153.1488622	
    Loss 1053735.2245515	
    Loss 1046369.0514734	
    Loss 1039054.6180281	
    Loss 1031791.445256	
    Loss 1024578.8657753	
    Loss 1017416.8501317	
    Loss 1010304.9430987	
    Loss 1003243.219301	
    Loss 996230.30130493	
    Loss 989265.78331784	
    Loss 982350.74297209	
    Loss 975483.89391041	
    Loss 968665.06342612	
    Loss 961893.7418841	
    Loss 955169.92176442	
    Loss 948493.23019829	
    Loss 941863.66174672	
    Loss 935280.59411907	
    Loss 928743.11419063	
    Loss 922251.63199165	
    Loss 915805.21823249	
    Loss 909403.93082424	
    Loss 903047.28186904	
    Loss 896735.34448837	
    Loss 890467.44950026	
    Loss 884243.15512988	
    Loss 878062.44137026	
    Loss 871925.25764624	
    Loss 865831.0220158	
    Loss 859779.13490231	
    Loss 853769.89032882	
    Loss 847802.38463152	
    Loss 841876.46302464	
    Loss 835992.04229	
    Loss 830148.57399169	
    Loss 824346.63352851	
    Loss 818584.85960291	
    Loss 812863.68839947	
    Loss 807182.36890286	
    Loss 801540.76764866	
    Loss 795938.85483459	
    Loss 790375.95496474	
    Loss 784852.07922974	
    Loss 779367.35678288	
    Loss 773920.31338424	
    Loss 768511.27791835	
    Loss 763140.15353313	
    Loss 757806.72283347	
    Loss 752510.20742764	
    Loss 747251.13236216	
    Loss 742028.55596688	
    Loss 736842.43368101	
    Loss 731692.80673479	
    Loss 726579.52766106	
    Loss 721501.86366944	
    Loss 716459.6002498	
    Loss 711452.46500566	
    Loss 706480.49025165	
    Loss 701543.5541404	
    Loss 696640.65073483	
    Loss 691772.27183567	
    Loss 686937.65252169	
    Loss 682136.89032535	
    Loss 677369.8971415	
    Loss 672636.18134063	
    Loss 667935.76618017	
    Loss 663268.26782462	
    Loss 658633.26949791	
    Loss 654030.33727232	
    Loss 649459.90827727	
    Loss 644921.56172182	
    Loss 640414.84319066	
    Loss 635939.47282082	
    Loss 631495.81614458	
    Loss 627082.83675618	
    Loss 622700.77494947	
    Loss 618349.55455747	
    Loss 614028.53330072	
    Loss 609737.78290355	
    Loss 605476.96961254	
    Loss 601246.13710797	
    Loss 597044.79272355	
    Loss 592873.05829932	
    Loss 588730.21440345	
    Loss 584616.47660688	
    Loss 580531.49328869	
    Loss 576475.23140588	
    Loss 572447.03020149	
    Loss 568447.09254325	
Epoch 4	
 118459
    176
      1
      0
   2084
      0
      0
    670
  10253
    163
      2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.10290725904346	
Grad norm	289.14057207009	
    Loss 566060.64991556	
    Loss 562105.1391989	
    Loss 558177.58874875	
    Loss 554277.49320293	
    Loss 550404.67718394	
    Loss 546559.00853063	
    Loss 542740.19144802	
    Loss 538947.98163915	
    Loss 535182.16464052	
    Loss 531443.00122111	
    Loss 527730.64071107	
    Loss 524043.3210176	
    Loss 520381.61917316	
    Loss 516745.71601727	
    Loss 513135.64067317	
    Loss 509550.47029432	
    Loss 505990.33614165	
    Loss 502455.2300269	
    Loss 498945.14278896	
    Loss 495459.21656932	
    Loss 491997.65869143	
    Loss 488560.58191453	
    Loss 485147.51362286	
    Loss 481758.0823468	
    Loss 478392.59480558	
    Loss 475050.71167535	
    Loss 471731.95976049	
    Loss 468436.32057797	
    Loss 465163.96038265	
    Loss 461914.35335304	
    Loss 458687.64659887	
    Loss 455483.23821728	
    Loss 452300.98547679	
    Loss 449141.24331906	
    Loss 446003.86436545	
    Loss 442888.17749922	
    Loss 439794.30200498	
    Loss 436722.03549273	
    Loss 433671.15134086	
    Loss 430641.88505165	
    Loss 427633.73500115	
    Loss 424646.85623822	
    Loss 421680.43797068	
    Loss 418734.93946814	
    Loss 415810.30735304	
    Loss 412905.58572366	
    Loss 410021.69212013	
    Loss 407157.69115219	
    Loss 404313.56092235	
    Loss 401489.29624458	
    Loss 398684.74238135	
    Loss 395900.12309229	
    Loss 393134.91675259	
    Loss 390388.98730337	
    Loss 387662.40323249	
    Loss 384954.85055907	
    Loss 382266.27951313	
    Loss 379596.56529546	
    Loss 376945.40036438	
    Loss 374312.91997895	
    Loss 371698.71594446	
    Loss 369103.0863651	
    Loss 366525.23607868	
    Loss 363965.00472172	
    Loss 361423.29858671	
    Loss 358899.16165172	
    Loss 356392.76265984	
    Loss 353903.73966095	
    Loss 351432.25841371	
    Loss 348978.05355412	
    Loss 346541.21987204	
    Loss 344121.54264824	
    Loss 341718.4721214	
    Loss 339332.36549197	
    Loss 336962.74810819	
    Loss 334609.87827348	
    Loss 332273.24345126	
    Loss 329953.17604788	
    Loss 327649.31156587	
    Loss 325361.4628275	
    Loss 323089.50277427	
    Loss 320833.72515439	
    Loss 318593.58274769	
    Loss 316369.0112886	
    Loss 314160.20084744	
    Loss 311966.57024723	
    Loss 309788.15846214	
    Loss 307625.0839293	
    Loss 305477.05580467	
    Loss 303344.53141207	
    Loss 301226.72534068	
    Loss 299123.70981492	
    Loss 297035.44875725	
    Loss 294961.69097725	
    Loss 292902.45512908	
    Loss 290857.59974897	
    Loss 288827.19870846	
    Loss 286811.16509259	
    Loss 284808.95104265	
    Loss 282820.72470413	
    Loss 280846.46152524	
    Loss 278886.03571539	
    Loss 276939.11046586	
    Loss 275006.04079257	
    Loss 273086.3231025	
    Loss 271179.93587352	
    Loss 269287.0675714	
    Loss 267407.6267522	
    Loss 265541.22192179	
    Loss 263687.66086215	
    Loss 261847.14308348	
    Loss 260019.4943836	
    Loss 258204.80802878	
    Loss 256402.64990169	
    Loss 254613.26343511	
    Loss 252836.21138184	
    Loss 251071.50339674	
    Loss 249319.2189882	
    Loss 247579.15804599	
    Loss 245851.35824164	
    Loss 244135.74310948	
    Loss 242432.03933287	
    Loss 240739.98832704	
    Loss 239059.97708224	
    Loss 237391.7578076	
    Loss 235735.146622	
    Loss 234089.98701128	
    Loss 232456.66271325	
    Loss 230834.55374316	
    Loss 229223.71615222	
    Loss 227624.20961207	
    Loss 226035.80659724	
    Loss 224458.54424924	
    Loss 222892.27147151	
    Loss 221337.15559368	
    Loss 219792.84869916	
    Loss 218259.45117589	
    Loss 216736.56886562	
    Loss 215224.40859033	
    Loss 213722.84954342	
    Loss 212231.81985449	
    Loss 210751.10722282	
    Loss 209280.74464654	
Epoch 5	
 129548
      4
      0
      0
    103
      0
      0
     24
   2127
      2
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10065398154892	
Grad norm	177.33958772335	
    Loss 208403.46940751	
    Loss 206949.39234136	
    Loss 205505.70480366	
    Loss 204072.18665839	
    Loss 202648.59675984	
    Loss 201234.97188165	
    Loss 199831.22900822	
    Loss 198437.24098036	
    Loss 197052.97930581	
    Loss 195678.56523006	
    Loss 194314.18267595	
    Loss 192958.6774499	
    Loss 191612.5185989	
    Loss 190275.89168858	
    Loss 188948.88509569	
    Loss 187631.00256291	
    Loss 186322.30231167	
    Loss 185022.86224045	
    Loss 183732.66971098	
    Loss 182451.23220111	
    Loss 181178.80924833	
    Loss 179915.4847357	
    Loss 178660.95542786	
    Loss 177415.07569553	
    Loss 176177.98065117	
    Loss 174949.55145408	
    Loss 173729.56507748	
    Loss 172518.08393006	
    Loss 171315.22342717	
    Loss 170120.67245675	
    Loss 168934.65298571	
    Loss 167756.73585972	
    Loss 166586.9138224	
    Loss 165425.41043985	
    Loss 164272.1773634	
    Loss 163126.81969086	
    Loss 161989.50676623	
    Loss 160860.05853944	
    Loss 159738.57505229	
    Loss 158625.04451148	
    Loss 157519.25432387	
    Loss 156421.39286223	
    Loss 155330.87222334	
    Loss 154248.06279056	
    Loss 153173.08022201	
    Loss 152105.23769584	
    Loss 151045.20257897	
    Loss 149992.39018995	
    Loss 148946.82002539	
    Loss 147908.53990591	
    Loss 146877.47918702	
    Loss 145853.91625131	
    Loss 144837.33988354	
    Loss 143827.91247811	
    Loss 142825.68122225	
    Loss 141830.4763532	
    Loss 140842.19979382	
    Loss 139860.84478403	
    Loss 138886.28948544	
    Loss 137918.70288272	
    Loss 136957.71268057	
