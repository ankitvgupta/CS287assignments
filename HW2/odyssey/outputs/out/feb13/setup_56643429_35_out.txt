[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	50	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2088
  636
 3914
 3617
 2612
 1624
 1582
 2107
 2596
 3358
 4058
 1339
  917
 2475
 1155
 1743
 1054
 2197
 2056
 2213
  663
 6461
 1909
 5554
 1882
 8012
 4465
 5095
 2421
 3052
 2601
 1403
 2349
 5512
 3582
 7823
 5011
 3254
 2790
 3016
 1476
 1991
 2510
 2044
 3591
[torch.DoubleTensor of size 45]

Validation accuracy:	0.022585882495751	
Grad norm	0	
    Loss 11392193.031159	
    Loss 341702.8672176	
    Loss 18723.406702593	
    Loss 8961.5767551094	
    Loss 11330.834619447	
    Loss 9418.551864132	
    Loss 9852.3783722746	
    Loss 12099.468242892	
    Loss 7535.1449053526	
    Loss 9894.8551176099	
    Loss 7935.6151773959	
    Loss 9895.5879011432	
    Loss 8838.0949600612	
    Loss 13080.995266992	
    Loss 8236.0308175629	
    Loss 8188.1561940145	
    Loss 9724.087237919	
    Loss 11078.70593934	
    Loss 11037.819172172	
    Loss 8880.8740690077	
    Loss 8218.1439805426	
    Loss 8791.2095545998	
    Loss 12461.720873081	
    Loss 11062.131632651	
    Loss 10422.450557706	
    Loss 9586.9741234949	
    Loss 9512.697126122	
    Loss 8609.3553908696	
    Loss 8704.6768692906	
    Loss 11559.223779838	
    Loss 9262.4381975817	
    Loss 10773.054100817	
    Loss 8580.282411775	
    Loss 8758.1540789626	
    Loss 9894.0409255989	
    Loss 11262.789660495	
    Loss 10031.017722934	
    Loss 10509.924873367	
    Loss 8944.3033451958	
    Loss 8642.842053591	
    Loss 10877.543917592	
    Loss 10159.637707743	
    Loss 13935.110817124	
    Loss 10772.359092246	
    Loss 12046.514606601	
    Loss 11687.866092425	
    Loss 8877.8823718367	
    Loss 8453.2421936911	
    Loss 9695.9210181467	
    Loss 8912.8526356026	
    Loss 8996.6550982176	
    Loss 9315.8803175082	
    Loss 10289.311892524	
    Loss 9847.4921444386	
    Loss 9197.3900006862	
    Loss 9657.2808548485	
    Loss 9180.6856420576	
    Loss 10676.669490623	
    Loss 8622.3328016514	
    Loss 9723.1698281627	
    Loss 9739.655098424	
    Loss 9118.7738404192	
    Loss 9703.9313110399	
    Loss 8810.6851921503	
    Loss 8342.5266344287	
    Loss 9144.5882853005	
    Loss 8983.4010984584	
    Loss 8956.49464814	
    Loss 11145.558760112	
    Loss 9292.0028441617	
    Loss 8853.608322979	
    Loss 9014.021425818	
    Loss 10149.087651349	
    Loss 11174.29783887	
    Loss 11235.162066067	
    Loss 8544.4379166836	
    Loss 10772.692167617	
    Loss 8664.7056515968	
    Loss 9111.8879183416	
    Loss 8896.8606596003	
    Loss 9395.4130838534	
    Loss 9810.9683356509	
    Loss 10295.51384954	
    Loss 9755.7300556271	
    Loss 9464.0203971221	
    Loss 8673.2957909866	
    Loss 8639.6584489583	
    Loss 8933.5004037626	
    Loss 9285.1654677257	
    Loss 10130.747575195	
    Loss 8904.8969833036	
    Loss 11158.883247842	
    Loss 9343.8600798977	
    Loss 9081.5164250831	
    Loss 9165.623122232	
    Loss 8597.595203965	
    Loss 10596.006698223	
    Loss 10985.133612378	
    Loss 9013.9754997985	
    Loss 10366.470035006	
    Loss 10043.128331989	
    Loss 9369.7911579915	
    Loss 8029.2683034583	
    Loss 8551.3131249713	
    Loss 10437.769416231	
    Loss 9539.0353619787	
    Loss 11940.181195983	
    Loss 9998.2124254243	
    Loss 10716.553651657	
    Loss 10914.660682084	
    Loss 10040.727961498	
    Loss 7920.3930270402	
    Loss 8557.9480495225	
    Loss 10483.663202123	
    Loss 8809.2821183635	
    Loss 8288.9006908381	
    Loss 11658.157233016	
    Loss 10367.33349086	
    Loss 9441.4192325044	
    Loss 8710.8306977137	
    Loss 8487.8200200887	
    Loss 8367.7028469173	
    Loss 8438.7496436234	
    Loss 8460.009742945	
    Loss 10822.277667745	
    Loss 9457.3595150704	
    Loss 8831.8493173575	
    Loss 12589.406066131	
    Loss 9525.1200693657	
    Loss 11731.996273082	
    Loss 10746.80878134	
    Loss 11702.661453095	
    Loss 10556.551519475	
    Loss 13426.640561167	
    Loss 8911.0603098895	
    Loss 10761.216902435	
    Loss 11171.133289311	
    Loss 9557.7928453114	
    Loss 10724.616949749	
    Loss 11994.163250714	
    Loss 10048.493482677	
    Loss 9795.6408375072	
    Loss 9548.452733566	
Epoch 2	
     0
     0
  1093
  5689
     0
   894
   408
     0
 22350
     0
 61805
    27
   199
    23
   865
 33540
  1152
  3763
[torch.DoubleTensor of size 18]

Validation accuracy:	0.033040483126973	
Grad norm	10.283454497561	
    Loss 12274.425486746	
    Loss 11239.507163695	
    Loss 8172.2623197073	
    Loss 11327.386650786	
    Loss 10382.209707531	
    Loss 10215.5328146	
    Loss 10562.697842938	
    Loss 13051.563757216	
    Loss 10950.836613795	
    Loss 10405.386178354	
    Loss 10958.63307322	
    Loss 10563.494087832	
    Loss 10538.796373727	
    Loss 11803.230330409	
    Loss 9773.806146187	
    Loss 8112.0268134855	
    Loss 9447.2723611941	
    Loss 11632.171350479	
    Loss 11718.616356112	
    Loss 9128.6904816058	
    Loss 9230.2548870959	
    Loss 9389.9068916946	
    Loss 11840.110728293	
    Loss 9565.6043392137	
    Loss 10999.872590714	
    Loss 10204.954579853	
    Loss 9767.7534097883	
    Loss 9271.5656678696	
    Loss 9809.3887198667	
    Loss 8343.8966261393	
    Loss 8326.1155354591	
    Loss 11114.424713536	
    Loss 8792.5787694568	
    Loss 9813.6699431392	
    Loss 8621.9620715425	
    Loss 9989.7058241488	
    Loss 8780.2158222927	
    Loss 9669.4508659114	
    Loss 9068.2942771665	
    Loss 9721.4953045592	
    Loss 11415.766423926	
    Loss 8419.7120229423	
    Loss 11168.229851624	
    Loss 8012.4187352298	
    Loss 10220.319732232	
    Loss 10700.316523057	
    Loss 8844.9753400745	
    Loss 12980.434868903	
    Loss 8807.7623028962	
    Loss 10011.945691939	
    Loss 11696.801725064	
    Loss 8772.9704365748	
    Loss 9320.8068838198	
    Loss 10565.318913608	
    Loss 10655.632361775	
    Loss 8470.4836957005	
    Loss 11728.428085907	
    Loss 9482.7625258763	
    Loss 10412.73184601	
    Loss 8367.8116734444	
    Loss 9321.8053696583	
    Loss 9730.424400345	
    Loss 9804.924518057	
    Loss 9432.8552361458	
    Loss 9655.2322189766	
    Loss 9944.8337351743	
    Loss 8902.4316034187	
    Loss 9451.3858549049	
    Loss 9550.4657018623	
    Loss 8853.4498010899	
    Loss 8221.3358117134	
    Loss 12717.709379878	
    Loss 8355.681067737	
    Loss 8574.4762653462	
    Loss 10215.906396067	
    Loss 10159.743702868	
    Loss 8993.6813749687	
    Loss 8298.064600598	
    Loss 11177.194419359	
    Loss 11488.726779739	
    Loss 9779.1585280307	
    Loss 9871.7211777469	
    Loss 8109.2254370656	
    Loss 9367.4518048068	
    Loss 11564.294370641	
    Loss 9827.3494376826	
    Loss 8926.4867710452	
    Loss 8784.1435672759	
    Loss 11296.408190767	
    Loss 10126.981057736	
    Loss 8055.8976943459	
    Loss 10544.677342738	
    Loss 9616.1593688605	
    Loss 8595.8083265476	
    Loss 8823.2501329885	
    Loss 11075.478924689	
    Loss 8787.6399756482	
    Loss 11008.227073991	
    Loss 10316.260104833	
    Loss 11761.79881684	
    Loss 8099.6207641292	
    Loss 10614.445380868	
    Loss 10636.62833905	
    Loss 12159.448635565	
    Loss 11162.384077523	
    Loss 9256.2382779376	
    Loss 10170.084871921	
    Loss 9932.927904917	
    Loss 10924.990541353	
    Loss 11114.665819079	
    Loss 8856.0115195828	
    Loss 9515.6822012442	
    Loss 9677.99345599	
    Loss 8429.8063694607	
    Loss 8961.1598346875	
    Loss 11904.640171995	
    Loss 10506.669723974	
    Loss 10401.470323532	
    Loss 9526.4004425061	
    Loss 9645.2043711273	
    Loss 9002.6414806612	
    Loss 7941.3021492095	
    Loss 9499.23039257	
    Loss 10188.617758561	
    Loss 9159.8882857936	
    Loss 10344.546461058	
    Loss 10253.850101947	
    Loss 8709.8560781165	
    Loss 9200.3627625788	
    Loss 9247.4072159882	
    Loss 8107.6662576512	
    Loss 8530.6047747359	
    Loss 10587.053365085	
    Loss 10354.279742459	
    Loss 10287.146111283	
    Loss 10655.305245413	
    Loss 10102.236796969	
    Loss 9017.675122147	
    Loss 9843.4275320201	
    Loss 10728.591535012	
    Loss 11351.145537106	
    Loss 11801.795152375	
    Loss 10280.566867989	
Epoch 3	
   2782
      0
      0
     18
      1
     40
     19
 106768
  21237
      0
     11
      6
    738
    129
     33
      0
     26
[torch.DoubleTensor of size 17]

Validation accuracy:	0.079206117989803	
Grad norm	10.849935279033	
    Loss 8864.0903732167	
    Loss 13217.351496355	
    Loss 8930.3865036534	
    Loss 10921.635121206	
    Loss 11478.301194353	
    Loss 9348.8221624177	
    Loss 10440.452899652	
    Loss 10934.529418799	
    Loss 8615.8278467444	
    Loss 8253.1004680834	
    Loss 10449.868482648	
    Loss 9029.4132376146	
    Loss 11091.840269041	
    Loss 11829.915520196	
    Loss 8573.1109483992	
    Loss 10214.145535424	
    Loss 8946.6720039243	
    Loss 10128.82276379	
    Loss 11515.918743633	
    Loss 10708.177875972	
    Loss 9076.517018397	
    Loss 10606.806462323	
    Loss 9113.0715393959	
    Loss 9063.5248943813	
    Loss 8926.489497994	
    Loss 10914.405879012	
    Loss 9633.603513008	
    Loss 9494.7416881138	
    Loss 8711.0875179605	
    Loss 8770.9810607829	
    Loss 10010.21936708	
    Loss 9280.9639304108	
    Loss 8540.2129964212	
    Loss 9898.6361811747	
    Loss 9174.9671631691	
    Loss 9338.7588077889	
    Loss 9509.9962436973	
    Loss 9074.4408054172	
    Loss 10480.109096979	
    Loss 8596.613407177	
    Loss 8987.3719043574	
    Loss 8334.744374163	
    Loss 9489.5993932897	
    Loss 11564.242774125	
    Loss 9930.0135443546	
    Loss 10395.861203589	
    Loss 11531.241310604	
    Loss 9479.2888284437	
    Loss 11720.321703355	
    Loss 10022.968759303	
    Loss 9733.9858755216	
    Loss 8770.8351595425	
    Loss 9221.4854399861	
    Loss 10892.830270828	
    Loss 11825.377746859	
    Loss 8666.1289421869	
    Loss 11934.153871645	
    Loss 10107.905008019	
    Loss 9406.6420559082	
    Loss 8637.6517915752	
    Loss 9558.8935206918	
    Loss 8923.2663962261	
    Loss 9037.0108456993	
    Loss 9601.1284651966	
    Loss 9796.5644127354	
    Loss 9427.1490367827	
    Loss 11551.913648792	
    Loss 10285.223754457	
    Loss 9719.6672798073	
    Loss 11718.651890878	
    Loss 9731.344957071	
    Loss 8709.263596476	
    Loss 9318.8496278534	
    Loss 9698.0764942217	
    Loss 9930.4016910031	
    Loss 10364.010966264	
    Loss 8591.5747089841	
    Loss 9499.3526068182	
    Loss 11893.823378704	
    Loss 8266.6522677074	
    Loss 9791.913199017	
    Loss 13470.663957007	
    Loss 11880.72477427	
    Loss 8632.3342129677	
    Loss 8259.211281763	
    Loss 8102.9384655305	
    Loss 11254.512418663	
    Loss 8668.9034223357	
    Loss 9607.3074502694	
    Loss 9968.7781950641	
    Loss 8501.5489513092	
    Loss 12862.072018502	
    Loss 8046.9744821061	
    Loss 10160.751563404	
    Loss 10644.118066691	
    Loss 8720.8528194718	
    Loss 11790.911769226	
    Loss 10706.120179555	
    Loss 12153.570641666	
    Loss 10335.796728788	
    Loss 9679.5633220969	
    Loss 10938.656916581	
    Loss 8856.0672426054	
    Loss 9729.559567228	
    Loss 9870.0176998959	
    Loss 9074.2320102004	
    Loss 8892.0844240873	
    Loss 8419.0417883165	
    Loss 9622.4304938691	
    Loss 9395.9489992602	
    Loss 9353.8366023133	
    Loss 9811.3201913932	
    Loss 9153.2351894576	
    Loss 8946.0491887854	
    Loss 9113.6055380449	
    Loss 8770.4410980119	
    Loss 10407.798218205	
    Loss 9746.2549635717	
    Loss 11459.496460179	
    Loss 9939.8552400966	
    Loss 8502.6204882621	
    Loss 11723.563120536	
    Loss 10820.208427544	
    Loss 10090.85276181	
    Loss 9551.1218478526	
    Loss 11141.636430546	
    Loss 9889.3951370273	
    Loss 9124.5757753972	
    Loss 8581.5965993481	
    Loss 10614.170363998	
    Loss 12714.256543739	
    Loss 11345.531032106	
    Loss 9269.5465472812	
    Loss 9261.2240379972	
    Loss 9023.9627246674	
    Loss 11831.031209478	
    Loss 8830.5257416186	
    Loss 9559.1673628005	
    Loss 12178.061954188	
    Loss 8475.7303718021	
    Loss 10746.131747296	
    Loss 11206.878547518	
    Loss 11464.261887712	
Epoch 4	
     0
   105
   168
 47235
 23910
  4913
  5945
   685
     0
     0
     0
  1413
  3908
  1109
  4257
 22557
  8353
  7250
[torch.DoubleTensor of size 18]

Validation accuracy:	0.042523974265598	
Grad norm	9.9583612683802	
    Loss 10455.180675117	
    Loss 9856.4374389679	
    Loss 10096.434160161	
    Loss 9440.8955408449	
    Loss 10892.795872346	
    Loss 8525.6718053086	
    Loss 10823.749623445	
    Loss 10454.089416237	
    Loss 9795.2315185669	
    Loss 10386.323714209	
    Loss 9020.744003879	
    Loss 8724.4459315046	
    Loss 10852.648174463	
    Loss 8760.8086271808	
    Loss 8917.5229077746	
    Loss 9607.3360569195	
    Loss 12657.030740508	
    Loss 11809.88527402	
    Loss 11610.251461142	
    Loss 9594.3586450928	
    Loss 9211.2631289784	
    Loss 10348.17763119	
    Loss 11039.344969801	
    Loss 11114.670474672	
    Loss 9703.3883673545	
    Loss 10162.154519831	
    Loss 9758.9448265754	
    Loss 10524.611195178	
    Loss 9180.7901303226	
    Loss 9046.1612007396	
    Loss 8716.4937974969	
    Loss 10665.124100215	
    Loss 9649.1973528484	
    Loss 9075.5670287264	
    Loss 9648.3406471111	
    Loss 10018.713415072	
    Loss 10735.772375571	
    Loss 10461.2304499	
    Loss 8915.4470932717	
    Loss 9121.939703395	
    Loss 11262.633855627	
    Loss 9287.5530638227	
    Loss 13583.359882501	
    Loss 9896.8834252807	
    Loss 11164.028896606	
    Loss 10847.494142323	
    Loss 12288.919199657	
    Loss 12000.319344802	
    Loss 10048.137714031	
    Loss 11763.126398115	
    Loss 12260.492402138	
    Loss 9427.4408017517	
    Loss 12322.27400777	
    Loss 10115.564186959	
    Loss 9669.5487873653	
    Loss 10603.988650441	
    Loss 10665.566323279	
    Loss 9054.4918391713	
    Loss 9039.8892567914	
    Loss 8588.1781757841	
    Loss 10755.246234334	
    Loss 11001.715868341	
    Loss 9952.2335069104	
    Loss 10477.573098561	
    Loss 10593.357015335	
    Loss 9909.8953594238	
    Loss 11800.101613242	
    Loss 8666.7029775764	
    Loss 11113.56874218	
    Loss 10681.735626201	
    Loss 11764.470832814	
    Loss 10088.738231158	
    Loss 11085.482767538	
    Loss 10076.281646099	
    Loss 10670.131390267	
    Loss 10433.598088979	
    Loss 10354.117010123	
    Loss 8764.5474598795	
    Loss 9706.3948759791	
    Loss 8387.3463174129	
    Loss 10960.332174314	
    Loss 8938.6968591955	
    Loss 10501.345913168	
    Loss 8068.1405380501	
    Loss 9930.6003743771	
    Loss 9869.3689260557	
    Loss 8207.3723758139	
    Loss 8950.1899505435	
    Loss 10260.282914793	
    Loss 11136.787059063	
    Loss 8809.1091880414	
    Loss 9440.6989785002	
    Loss 9163.7719391308	
    Loss 9545.8121030896	
    Loss 11141.001402251	
    Loss 11266.76874838	
    Loss 9747.8729418273	
    Loss 8370.4402727957	
    Loss 8582.6044362661	
    Loss 12859.444441849	
    Loss 10329.692756127	
    Loss 9434.3391956996	
    Loss 8521.9472028698	
    Loss 8218.6764543867	
    Loss 10228.097149392	
    Loss 10070.324550556	
    Loss 8691.2756021551	
    Loss 12521.057595298	
    Loss 10943.203374022	
    Loss 9873.9411117398	
    Loss 9380.3882144284	
    Loss 8373.6814663345	
    Loss 10884.593794798	
    Loss 9016.2113589178	
    Loss 8764.5854799952	
    Loss 11506.436287069	
    Loss 9245.2210173834	
    Loss 10530.909142481	
    Loss 8569.1348734297	
    Loss 11004.721976578	
    Loss 8740.796833768	
    Loss 8405.3482874976	
    Loss 8229.0485374239	
    Loss 10365.955233538	
    Loss 9310.691557736	
    Loss 9738.7568323451	
    Loss 11846.531359634	
    Loss 9327.1798330841	
    Loss 8994.0402618849	
    Loss 13866.150652203	
    Loss 10275.826856244	
    Loss 9386.2803644889	
    Loss 9686.7038031795	
    Loss 11330.142770304	
    Loss 9337.4594872642	
    Loss 8763.6187058831	
    Loss 10825.621408217	
    Loss 8945.1217063457	
    Loss 9962.722612291	
    Loss 8640.1211181648	
    Loss 10306.545831968	
    Loss 9450.4691012901	
    Loss 11808.973206332	
Epoch 5	
      0
      0
      0
     59
      0
      0
     55
      0
 121410
   2158
   1808
      0
     14
      0
      0
   6304
[torch.DoubleTensor of size 16]

Validation accuracy:	0.13410415149308	
Grad norm	10.225883307065	
    Loss 9615.6537755449	
    Loss 11654.492396351	
    Loss 9887.9263993378	
    Loss 10956.55213348	
    Loss 11593.6010089	
    Loss 8744.5210172063	
    Loss 8538.9906991419	
    Loss 17455.024432695	
    Loss 8460.5446480608	
    Loss 8999.102446807	
    Loss 9553.3973960277	
    Loss 8476.2890475594	
    Loss 8936.9394142695	
    Loss 12099.679697063	
    Loss 10727.629235568	
    Loss 9513.0445116934	
    Loss 9956.2493382454	
    Loss 11822.349205124	
    Loss 9851.7537787719	
