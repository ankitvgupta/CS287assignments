[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.5	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1623
 2124
 1881
 4163
 1681
 1828
 2531
 2580
 2852
 1881
 1074
 1733
 7686
 7221
 2094
 2800
 2671
 3839
 1519
 1135
 4246
 2563
 1940
 3477
 3264
 1897
 3083
 3898
 2894
 1205
 3425
  652
 5309
 3319
 2601
 1327
 1691
 2693
  729
 4882
 2365
 7316
 3624
 2194
 6298
[torch.DoubleTensor of size 45]

Validation accuracy:	0.020059480456421	
Grad norm	0	
    Loss 11396400.101793	
    Loss 10624025.740831	
    Loss 9904027.3383942	
    Loss 9232833.3393026	
    Loss 8607126.9951311	
    Loss 8023833.7147271	
    Loss 7480069.7927429	
    Loss 6973166.345719	
    Loss 6500615.6193551	
    Loss 6060090.7581224	
    Loss 5649420.9416057	
    Loss 5266579.5205503	
    Loss 4909689.6473177	
    Loss 4576989.9862157	
    Loss 4266832.6874147	
    Loss 3977696.253438	
    Loss 3708155.2452461	
    Loss 3456880.7771793	
    Loss 3222635.7718722	
    Loss 3004264.6966279	
    Loss 2800690.1279843	
    Loss 2610914.8708503	
    Loss 2434001.7014985	
    Loss 2269074.9979148	
    Loss 2115328.3322295	
    Loss 1972002.1656987	
    Loss 1838385.2910884	
    Loss 1713826.8704762	
    Loss 1597707.3587636	
    Loss 1489457.2539027	
    Loss 1388543.3571025	
    Loss 1294468.2991007	
    Loss 1206769.9410752	
    Loss 1125014.3814064	
    Loss 1048796.6405238	
    Loss 977745.78142238	
    Loss 911510.57576358	
    Loss 849764.54821553	
    Loss 792201.20616606	
    Loss 738541.59938049	
    Loss 688516.80121138	
    Loss 641881.59768548	
    Loss 598406.91191862	
    Loss 557878.38778598	
    Loss 520097.646991	
    Loss 484876.00944058	
    Loss 452040.87588677	
    Loss 421430.75318758	
    Loss 392894.68190136	
    Loss 366293.30423467	
    Loss 341494.28599397	
    Loss 318375.19945075	
    Loss 296823.49725692	
    Loss 276732.31393747	
    Loss 258002.45508567	
    Loss 240542.01557759	
    Loss 224265.11666046	
    Loss 209092.26408015	
    Loss 194946.42720784	
    Loss 181758.94775005	
    Loss 169465.49539052	
    Loss 158004.4984399	
    Loss 147320.06872195	
    Loss 137359.31655141	
    Loss 128074.16271236	
    Loss 119417.5920419	
    Loss 111348.29566465	
    Loss 103825.91299797	
    Loss 96813.016453304	
    Loss 90275.624086111	
    Loss 84181.076882634	
    Loss 78499.746679824	
Epoch 2	
 88540
    98
     0
   334
    39
     0
     0
    14
 42399
   384
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10443220441855	
Grad norm	214.08081156555	
    Loss 76871.426304605	
    Loss 71685.64853925	
    Loss 66851.420823724	
    Loss 62344.570059596	
    Loss 58142.875149971	
    Loss 54226.458600623	
    Loss 50574.438999164	
    Loss 47169.755267168	
    Loss 43996.937291642	
    Loss 41039.247515659	
    Loss 38281.098153904	
    Loss 35710.490231214	
    Loss 33313.550686026	
    Loss 31078.558875086	
    Loss 28995.521113516	
    Loss 27053.939377582	
    Loss 25244.1645161	
    Loss 23556.496936343	
    Loss 21982.747386938	
    Loss 20516.536567092	
    Loss 19148.9094706	
    Loss 17874.74878814	
    Loss 16686.815779674	
    Loss 15578.937238808	
    Loss 14546.254377794	
    Loss 13583.238891042	
    Loss 12685.029005067	
    Loss 11848.968845022	
    Loss 11069.732166466	
    Loss 10342.453409067	
    Loss 9664.682005861	
    Loss 9032.4114161803	
    Loss 8443.8862487497	
    Loss 7894.8607929234	
    Loss 7382.602196334	
    Loss 6904.8613965046	
    Loss 6459.8180069401	
    Loss 6044.8209136914	
    Loss 5658.1083723905	
    Loss 5298.0944485113	
    Loss 4962.2992168824	
    Loss 4648.6473949146	
    Loss 4356.7519651087	
    Loss 4083.8349071147	
    Loss 3830.1228374886	
    Loss 3594.3951614817	
    Loss 3373.9991183128	
    Loss 3167.7282884773	
    Loss 2975.9740682029	
    Loss 2797.0090799648	
    Loss 2630.5857647858	
    Loss 2475.2411675915	
    Loss 2330.5292024515	
    Loss 2195.7694711727	
    Loss 2070.2603350004	
    Loss 1952.4096308002	
    Loss 1842.9779799337	
    Loss 1742.2072036964	
    Loss 1647.5595082588	
    Loss 1558.5998007666	
    Loss 1476.4217501109	
    Loss 1399.1712906353	
    Loss 1327.3339377873	
    Loss 1260.0387654012	
    Loss 1197.8062323583	
    Loss 1139.0579284543	
    Loss 1084.7261456081	
    Loss 1034.3544539155	
    Loss 987.27561576485	
    Loss 943.20286041057	
    Loss 902.36278835056	
    Loss 864.1989646877	
Epoch 3	
 108029
      0
      0
      0
      0
      0
      0
      0
  23779
[torch.DoubleTensor of size 9]

Validation accuracy:	0.084562393784899	
Grad norm	22.55458844669	
    Loss 852.86958313343	
    Loss 818.1953355694	
    Loss 786.15639698777	
    Loss 755.88017749244	
    Loss 727.80570254743	
    Loss 701.83070881275	
    Loss 676.90008345363	
    Loss 653.25654547821	
    Loss 632.27163249138	
    Loss 613.05870756628	
    Loss 594.28130950316	
    Loss 577.43094852999	
    Loss 561.1929921614	
    Loss 545.51746028342	
    Loss 531.50204497982	
    Loss 518.63908810217	
    Loss 506.80435893512	
    Loss 495.31480575768	
    Loss 484.07430074217	
    Loss 474.73906449868	
    Loss 465.36780886198	
    Loss 457.2051451008	
    Loss 449.51534502237	
    Loss 441.98566051994	
    Loss 435.01374799664	
    Loss 428.12556655255	
    Loss 421.46562090898	
    Loss 416.29960027698	
    Loss 411.82222837639	
    Loss 406.73279158845	
    Loss 402.28823142155	
    Loss 397.71868918987	
    Loss 394.30123874511	
    Loss 390.7321489012	
    Loss 387.09455803102	
    Loss 383.36327613401	
    Loss 380.2608727811	
    Loss 377.18761083549	
    Loss 374.58406227631	
    Loss 372.50378331265	
    Loss 370.46742814728	
    Loss 367.9998862204	
    Loss 366.18988496827	
    Loss 363.67498481935	
    Loss 361.96345177742	
    Loss 361.27771106947	
    Loss 360.00520695333	
    Loss 357.9920375252	
    Loss 356.66082422721	
    Loss 355.17000307654	
    Loss 354.21457357306	
    Loss 353.16075614995	
    Loss 352.24307004416	
    Loss 351.54897991898	
    Loss 351.04528873195	
    Loss 349.65778122064	
    Loss 348.80670951125	
    Loss 349.26586814681	
    Loss 349.02085182187	
    Loss 348.03838827461	
    Loss 347.9056776798	
    Loss 347.13959726109	
    Loss 346.62578417199	
    Loss 345.82081484178	
    Loss 345.54985931539	
    Loss 344.57665187455	
    Loss 344.08954123858	
    Loss 343.91993612308	
    Loss 343.64602796553	
    Loss 343.17083963914	
    Loss 343.01196380278	
    Loss 342.74164869039	
Epoch 4	
 108180
      0
      0
      0
      0
      0
      0
      0
  23628
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083796127700898	
Grad norm	7.2817103119395	
    Loss 342.2633375547	
    Loss 342.17530632592	
    Loss 342.38762793736	
    Loss 342.17056290105	
    Loss 342.14490034423	
    Loss 342.29282180866	
    Loss 341.73761808909	
    Loss 340.79873458133	
    Loss 340.96473148821	
    Loss 341.48833318707	
    Loss 341.11510441858	
    Loss 341.41669308073	
    Loss 341.17545868482	
    Loss 340.40916024428	
    Loss 340.29505784817	
    Loss 340.38275851893	
    Loss 340.61490680471	
    Loss 340.37985624395	
    Loss 339.62403859751	
    Loss 340.08354397375	
    Loss 339.85046178258	
    Loss 340.18922877534	
    Loss 340.43015620113	
    Loss 340.29862762302	
    Loss 340.21806556153	
    Loss 339.74884609262	
    Loss 339.08904233522	
    Loss 339.49559644423	
    Loss 340.22646675104	
    Loss 339.9865942419	
    Loss 340.06988223353	
    Loss 339.72257409788	
    Loss 340.23585429759	
    Loss 340.33026519746	
    Loss 340.11838447803	
    Loss 339.56850154533	
    Loss 339.43735668294	
    Loss 339.12762923313	
    Loss 339.10756264684	
    Loss 339.42528241438	
    Loss 339.62992663869	
    Loss 339.25573774598	
    Loss 339.39485120645	
    Loss 338.69564197915	
    Loss 338.67094266514	
    Loss 339.56663877437	
    Loss 339.76858652245	
    Loss 339.12941637513	
    Loss 339.07810530753	
    Loss 338.77771439516	
    Loss 338.93377840791	
    Loss 338.91824502747	
    Loss 338.96598486779	
    Loss 339.17260937217	
    Loss 339.51073468129	
    Loss 338.90272109831	
    Loss 338.77879427298	
    Loss 339.91657553651	
    Loss 340.30623004145	
    Loss 339.91344393064	
    Loss 340.33270713607	
    Loss 340.08102534028	
    Loss 340.04855117946	
    Loss 339.69214471722	
    Loss 339.83748171615	
    Loss 339.25349823348	
    Loss 339.12808841995	
    Loss 339.29596832598	
    Loss 339.33690940315	
    Loss 339.15252860241	
    Loss 339.26772041146	
    Loss 339.250373438	
Epoch 5	
 108163
      0
      0
      0
      0
      0
      0
      0
  23645
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083871995630007	
Grad norm	6.2194067034929	
    Loss 338.84398012326	
    Loss 338.98662104677	
    Loss 339.41465563961	
    Loss 339.39838977213	
    Loss 339.56191265587	
    Loss 339.88424771825	
    Loss 339.49331779709	
    Loss 338.70608454255	
    Loss 339.0121690014	
    Loss 339.66782845909	
    Loss 339.41832518699	
    Loss 339.83460417704	
    Loss 339.70106780175	
    Loss 339.03487400336	
    Loss 339.01415690155	
    Loss 339.18820641272	
    Loss 339.50042554355	
    Loss 339.34036470697	
    Loss 338.65389818954	
    Loss 339.17968462704	
    Loss 339.0089004893	
    Loss 339.40437551203	
    Loss 339.69869883214	
    Loss 339.61725401602	
    Loss 339.58294965908	
    Loss 339.15643186075	
    Loss 338.53772446083	
    Loss 338.98086427015	
    Loss 339.74690070564	
    Loss 339.53932982909	
    Loss 339.65330844834	
    Loss 339.33475598444	
    Loss 339.87438348625	
    Loss 339.99325778773	
    Loss 339.80505332132	
    Loss 339.27623483745	
    Loss 339.16520724002	
    Loss 338.87368713047	
    Loss 338.87116291085	
    Loss 339.20446007969	
    Loss 339.42403439184	
    Loss 339.06413146262	
    Loss 339.21630714203	
    Loss 338.5291840322	
    Loss 338.51533498859	
    Loss 339.42184998625	
    Loss 339.63385537884	
    Loss 339.00405007293	
    Loss 338.96133236879	
    Loss 338.66876586791	
    Loss 338.83228195747	
    Loss 338.82382767595	
    Loss 338.87801505551	
    Loss 339.09067069594	
    Loss 339.43460276897	
    Loss 338.83160337275	
    Loss 338.71237512462	
    Loss 339.85460759247	
    Loss 340.24854580046	
    Loss 339.85960845012	
    Loss 340.28263290207	
    Loss 340.03444214078	
    Loss 340.00537074069	
    Loss 339.65211538072	
    Loss 339.80024575342	
    Loss 339.21895752955	
    Loss 339.09597519857	
    Loss 339.26613253954	
    Loss 339.30920952928	
    Loss 339.12661376966	
    Loss 339.24369610653	
    Loss 339.22791316805	
Epoch 6	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083856822044185	
Grad norm	6.1638851358326	
    Loss 338.82192428431	
    Loss 338.96597634772	
    Loss 339.39537965772	
    Loss 339.38037776053	
    Loss 339.54523407737	
    Loss 339.86865537042	
    Loss 339.47886566342	
    Loss 338.69257735958	
    Loss 338.99943938338	
    Loss 339.65593685566	
    Loss 339.40727665828	
    Loss 339.82427629787	
    Loss 339.69148496806	
    Loss 339.02596241362	
    Loss 339.00587011008	
    Loss 339.18044431753	
    Loss 339.49312372897	
    Loss 339.33351393785	
    Loss 338.64742564648	
    Loss 339.17369327021	
    Loss 339.00339493968	
    Loss 339.39921575988	
    Loss 339.69390853424	
    Loss 339.61282863605	
    Loss 339.57883094331	
    Loss 339.15256543618	
    Loss 338.53419637613	
    Loss 338.97751180276	
    Loss 339.74379608299	
    Loss 339.5364169021	
    Loss 339.65062179006	
    Loss 339.33229424622	
    Loss 339.87209332135	
    Loss 339.99111814646	
    Loss 339.80312568387	
    Loss 339.27442215014	
    Loss 339.16354256452	
    Loss 338.87211483682	
    Loss 338.86972244693	
    Loss 339.20308311535	
    Loss 339.4227477447	
    Loss 339.06296019529	
    Loss 339.21521971716	
    Loss 338.52816775679	
    Loss 338.51435459555	
    Loss 339.42095944758	
    Loss 339.63304392413	
    Loss 339.00331305941	
    Loss 338.96065084522	
    Loss 338.66812276087	
    Loss 338.83168818789	
    Loss 338.8232887722	
    Loss 338.87751791481	
    Loss 339.09021193776	
    Loss 339.43419531143	
    Loss 338.83121323457	
    Loss 338.71200229316	
    Loss 339.85425712345	
    Loss 340.24822540429	
    Loss 339.85930519029	
    Loss 340.28235910831	
    Loss 340.03419468066	
    Loss 340.00515996053	
    Loss 339.65193655986	
    Loss 339.80008563632	
    Loss 339.21882200112	
    Loss 339.09585625642	
    Loss 339.26602960497	
    Loss 339.30912226165	
    Loss 339.12652524438	
    Loss 339.24362366208	
    Loss 339.22784029865	
Epoch 7	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1622127341322	
    Loss 338.82184789484	
    Loss 338.96589833603	
    Loss 339.39530435478	
    Loss 339.38030475047	
    Loss 339.54517542177	
    Loss 339.86859733437	
    Loss 339.47881809075	
    Loss 338.69253019029	
    Loss 338.9993842971	
    Loss 339.65588319077	
    Loss 339.40723002555	
    Loss 339.82423020417	
    Loss 339.69144588278	
    Loss 339.02592808361	
    Loss 339.00583974293	
    Loss 339.18041294525	
    Loss 339.49308948183	
    Loss 339.33347837822	
    Loss 338.64738567199	
    Loss 339.17365930983	
    Loss 339.00336938969	
    Loss 339.39918961499	
    Loss 339.69388584483	
    Loss 339.61281059892	
    Loss 339.57881466659	
    Loss 339.15254810457	
    Loss 338.53418631026	
    Loss 338.977497247	
    Loss 339.74378399037	
    Loss 339.53640394053	
    Loss 339.65061190803	
    Loss 339.33228847375	
    Loss 339.87208827583	
    Loss 339.99111292525	
    Loss 339.80312615894	
    Loss 339.27442117698	
    Loss 339.16354365772	
    Loss 338.8721140992	
    Loss 338.86972370468	
    Loss 339.20308161366	
    Loss 339.42274607333	
    Loss 339.06296097663	
    Loss 339.2152207142	
    Loss 338.5281683735	
    Loss 338.51435258555	
    Loss 339.42095954456	
    Loss 339.63304546097	
    Loss 339.0033160971	
    Loss 338.96065399342	
    Loss 338.66812503415	
    Loss 338.83169076335	
    Loss 338.8232922643	
    Loss 338.87752166804	
    Loss 339.09021575737	
    Loss 339.43420047805	
    Loss 338.83121730835	
    Loss 338.71200539255	
    Loss 339.85425984403	
    Loss 340.24822842711	
    Loss 339.85930764981	
    Loss 340.28236210512	
    Loss 340.03419810356	
    Loss 340.00516476556	
    Loss 339.65194243845	
    Loss 339.80009165781	
    Loss 339.21882871622	
    Loss 339.09586314258	
    Loss 339.26603665066	
    Loss 339.30912950318	
    Loss 339.12653143554	
    Loss 339.24363019182	
    Loss 339.22784594982	
Epoch 8	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621978228483	
    Loss 338.82185302716	
    Loss 338.9659025662	
    Loss 339.39530808025	
    Loss 339.38030803584	
    Loss 339.5451792634	
    Loss 339.86860065206	
    Loss 339.4788216953	
    Loss 338.69253332095	
    Loss 338.99938631733	
    Loss 339.6558848649	
    Loss 339.40723188116	
    Loss 339.82423169843	
    Loss 339.69144760204	
    Loss 339.02592987841	
    Loss 339.00584154295	
    Loss 339.18041436909	
    Loss 339.49309042594	
    Loss 339.33347895752	
    Loss 338.64738566993	
    Loss 339.17365956334	
    Loss 339.0033700978	
    Loss 339.39919007826	
    Loss 339.69388641476	
    Loss 339.61281137123	
    Loss 339.57881543045	
    Loss 339.15254864359	
    Loss 338.53418729904	
    Loss 338.97749774931	
    Loss 339.74378456346	
    Loss 339.53640432731	
    Loss 339.65061243832	
    Loss 339.33228924269	
    Loss 339.8720890173	
    Loss 339.99111356584	
    Loss 339.80312718253	
    Loss 339.27442200672	
    Loss 339.16354458946	
    Loss 338.87211481828	
    Loss 338.86972452688	
    Loss 339.20308216886	
    Loss 339.42274656666	
    Loss 339.06296162923	
    Loss 339.2152213401	
    Loss 338.52816892447	
    Loss 338.51435289415	
    Loss 339.42095999702	
    Loss 339.63304599556	
    Loss 339.00331672806	
    Loss 338.96065459975	
    Loss 338.66812554421	
    Loss 338.83169127431	
    Loss 338.82329282392	
    Loss 338.87752223452	
    Loss 339.09021630778	
    Loss 339.43420111889	
    Loss 338.83121785164	
    Loss 338.71200584524	
    Loss 339.85426025663	
    Loss 340.24822885014	
    Loss 339.85930801557	
    Loss 340.28236250194	
    Loss 340.03419852493	
    Loss 340.00516528943	
    Loss 339.65194303832	
    Loss 339.8000922617	
    Loss 339.2188293679	
    Loss 339.09586380276	
    Loss 339.26603731566	
    Loss 339.309130176	
    Loss 339.12653201899	
    Loss 339.24363079298	
    Loss 339.22784647505	
Epoch 9	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.162196984619	
    Loss 338.82185350945	
    Loss 338.96590297056	
    Loss 339.39530843892	
    Loss 339.3803083578	
    Loss 339.54517962773	
    Loss 339.86860097068	
    Loss 339.47882203257	
    Loss 338.69253361685	
    Loss 338.9993865205	
    Loss 339.65588503585	
    Loss 339.40723206549	
    Loss 339.82423184965	
    Loss 339.69144777012	
    Loss 339.02593005207	
    Loss 339.00584171462	
    Loss 339.1804145081	
    Loss 339.49309052601	
    Loss 339.333479026	
    Loss 338.64738569066	
    Loss 339.17365960236	
    Loss 339.00337017088	
    Loss 339.39919012983	
    Loss 339.69388647397	
    Loss 339.61281144512	
    Loss 339.57881550298	
    Loss 339.15254869736	
    Loss 338.53418738805	
    Loss 338.97749779837	
    Loss 339.74378461658	
    Loss 339.5364043642	
    Loss 339.65061248592	
    Loss 339.33228930891	
    Loss 339.87208908093	
    Loss 339.9911136205	
    Loss 339.80312726757	
    Loss 339.27442207526	
    Loss 339.16354466585	
    Loss 338.87211487684	
    Loss 338.86972459346	
    Loss 339.20308221415	
    Loss 339.42274660674	
    Loss 339.06296168241	
    Loss 339.21522139093	
    Loss 338.52816896874	
    Loss 338.51435291903	
    Loss 339.42096003394	
    Loss 339.63304603883	
    Loss 339.00331677936	
    Loss 338.96065464861	
    Loss 338.66812558522	
    Loss 338.83169131527	
    Loss 338.82329286858	
    Loss 338.87752228005	
    Loss 339.09021635182	
    Loss 339.43420117011	
    Loss 338.83121789541	
    Loss 338.71200588188	
    Loss 339.8542602903	
    Loss 340.24822888463	
    Loss 339.8593080454	
    Loss 340.28236253419	
    Loss 340.03419855923	
    Loss 340.00516533206	
    Loss 339.65194308699	
    Loss 339.80009231077	
    Loss 339.21882942081	
    Loss 339.0958638565	
    Loss 339.26603736969	
    Loss 339.30913023056	
    Loss 339.12653206642	
    Loss 339.24363084153	
    Loss 339.22784651749	
Epoch 10	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969158249	
    Loss 338.82185354845	
    Loss 338.96590300322	
    Loss 339.39530846784	
    Loss 339.38030838397	
    Loss 339.54517965737	
    Loss 339.86860099663	
    Loss 339.47882205994	
    Loss 338.69253364088	
    Loss 338.99938653702	
    Loss 339.65588504966	
    Loss 339.40723208043	
    Loss 339.82423186181	
    Loss 339.6914477837	
    Loss 339.0259300662	
    Loss 339.00584172854	
    Loss 339.18041451934	
    Loss 339.4930905342	
    Loss 339.3334790316	
    Loss 338.64738569245	
    Loss 339.17365960555	
    Loss 339.00337017673	
    Loss 339.39919013389	
    Loss 339.69388647864	
    Loss 339.61281145094	
    Loss 339.57881550871	
    Loss 339.1525487016	
    Loss 338.53418739511	
    Loss 338.97749780221	
    Loss 339.74378462067	
    Loss 339.53640436695	
    Loss 339.65061248953	
    Loss 339.33228931401	
    Loss 339.87208908582	
    Loss 339.99111362464	
    Loss 339.80312727417	
    Loss 339.2744220805	
    Loss 339.16354467172	
    Loss 338.87211488125	
    Loss 338.86972459852	
    Loss 339.20308221753	
    Loss 339.42274660971	
    Loss 339.06296168647	
    Loss 339.21522139481	
    Loss 338.52816897206	
    Loss 338.51435292083	
    Loss 339.42096003675	
    Loss 339.63304604214	
    Loss 339.00331678336	
    Loss 338.9606546524	
    Loss 338.66812558837	
    Loss 338.83169131842	
    Loss 338.82329287204	
    Loss 338.8775222836	
    Loss 339.09021635525	
    Loss 339.43420117412	
    Loss 338.83121789886	
    Loss 338.71200588475	
    Loss 339.85426029297	
    Loss 340.24822888736	
    Loss 339.85930804776	
    Loss 340.28236253674	
    Loss 340.03419856197	
    Loss 340.00516533548	
    Loss 339.65194309089	
    Loss 339.80009231471	
    Loss 339.21882942507	
    Loss 339.09586386083	
    Loss 339.26603737404	
    Loss 339.30913023496	
    Loss 339.12653207024	
    Loss 339.24363084542	
    Loss 339.2278465209	
Epoch 11	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969101795	
    Loss 338.82185355158	
    Loss 338.96590300584	
    Loss 339.39530847015	
    Loss 339.38030838607	
    Loss 339.54517965978	
    Loss 339.86860099873	
    Loss 339.47882206214	
    Loss 338.69253364282	
    Loss 338.99938653836	
    Loss 339.65588505077	
    Loss 339.40723208163	
    Loss 339.82423186278	
    Loss 339.69144778479	
    Loss 339.02593006734	
    Loss 339.00584172965	
    Loss 339.18041452024	
    Loss 339.49309053487	
    Loss 339.33347903205	
    Loss 338.6473856926	
    Loss 339.1736596058	
    Loss 339.00337017719	
    Loss 339.39919013421	
    Loss 339.693886479	
    Loss 339.61281145139	
    Loss 339.57881550915	
    Loss 339.15254870193	
    Loss 338.53418739567	
    Loss 338.9774978025	
    Loss 339.74378462098	
    Loss 339.53640436716	
    Loss 339.6506124898	
    Loss 339.3322893144	
    Loss 339.87208908621	
    Loss 339.99111362495	
    Loss 339.80312727468	
    Loss 339.27442208089	
    Loss 339.16354467217	
    Loss 338.87211488157	
    Loss 338.8697245989	
    Loss 339.20308221778	
    Loss 339.42274660993	
    Loss 339.06296168678	
    Loss 339.21522139511	
    Loss 338.52816897231	
    Loss 338.51435292095	
    Loss 339.42096003696	
    Loss 339.63304604239	
    Loss 339.00331678367	
    Loss 338.96065465269	
    Loss 338.66812558862	
    Loss 338.83169131867	
    Loss 338.8232928723	
    Loss 338.87752228387	
    Loss 339.09021635551	
    Loss 339.43420117443	
    Loss 338.83121789913	
    Loss 338.71200588498	
    Loss 339.85426029318	
    Loss 340.24822888758	
    Loss 339.85930804795	
    Loss 340.28236253695	
    Loss 340.03419856219	
    Loss 340.00516533575	
    Loss 339.65194309121	
    Loss 339.80009231503	
    Loss 339.21882942541	
    Loss 339.09586386118	
    Loss 339.26603737439	
    Loss 339.30913023531	
    Loss 339.12653207055	
    Loss 339.24363084574	
    Loss 339.22784652117	
Epoch 12	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969097141	
    Loss 338.82185355183	
    Loss 338.96590300605	
    Loss 339.39530847033	
    Loss 339.38030838624	
    Loss 339.54517965997	
    Loss 339.8686009989	
    Loss 339.47882206232	
    Loss 338.69253364298	
    Loss 338.99938653846	
    Loss 339.65588505086	
    Loss 339.40723208173	
    Loss 339.82423186286	
    Loss 339.69144778488	
    Loss 339.02593006743	
    Loss 339.00584172975	
    Loss 339.18041452031	
    Loss 339.49309053492	
    Loss 339.33347903208	
    Loss 338.64738569261	
    Loss 339.17365960582	
    Loss 339.00337017722	
    Loss 339.39919013423	
    Loss 339.69388647904	
    Loss 339.61281145142	
    Loss 339.57881550918	
    Loss 339.15254870196	
    Loss 338.53418739571	
    Loss 338.97749780253	
    Loss 339.743784621	
    Loss 339.53640436718	
    Loss 339.65061248982	
    Loss 339.33228931443	
    Loss 339.87208908624	
    Loss 339.99111362498	
    Loss 339.80312727473	
    Loss 339.27442208092	
    Loss 339.16354467221	
    Loss 338.8721148816	
    Loss 338.86972459894	
    Loss 339.2030822178	
    Loss 339.42274660995	
    Loss 339.0629616868	
    Loss 339.21522139514	
    Loss 338.52816897233	
    Loss 338.51435292096	
    Loss 339.42096003698	
    Loss 339.63304604241	
    Loss 339.00331678369	
    Loss 338.96065465272	
    Loss 338.66812558864	
    Loss 338.83169131869	
    Loss 338.82329287233	
    Loss 338.8775222839	
    Loss 339.09021635553	
    Loss 339.43420117445	
    Loss 338.83121789915	
    Loss 338.71200588499	
    Loss 339.8542602932	
    Loss 340.2482288876	
    Loss 339.85930804796	
    Loss 340.28236253697	
    Loss 340.0341985622	
    Loss 340.00516533577	
    Loss 339.65194309123	
    Loss 339.80009231505	
    Loss 339.21882942544	
    Loss 339.09586386121	
    Loss 339.26603737442	
    Loss 339.30913023534	
    Loss 339.12653207058	
    Loss 339.24363084576	
    Loss 339.2278465212	
Epoch 13	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969096758	
    Loss 338.82185355185	
    Loss 338.96590300606	
    Loss 339.39530847035	
    Loss 339.38030838626	
    Loss 339.54517965999	
    Loss 339.86860099891	
    Loss 339.47882206233	
    Loss 338.69253364299	
    Loss 338.99938653847	
    Loss 339.65588505087	
    Loss 339.40723208174	
    Loss 339.82423186286	
    Loss 339.69144778489	
    Loss 339.02593006744	
    Loss 339.00584172975	
    Loss 339.18041452032	
    Loss 339.49309053492	
    Loss 339.33347903208	
    Loss 338.64738569261	
    Loss 339.17365960582	
    Loss 339.00337017723	
    Loss 339.39919013423	
    Loss 339.69388647904	
    Loss 339.61281145142	
    Loss 339.57881550919	
    Loss 339.15254870196	
    Loss 338.53418739571	
    Loss 338.97749780253	
    Loss 339.743784621	
    Loss 339.53640436718	
    Loss 339.65061248982	
    Loss 339.33228931444	
    Loss 339.87208908624	
    Loss 339.99111362498	
    Loss 339.80312727473	
    Loss 339.27442208093	
    Loss 339.16354467221	
    Loss 338.8721148816	
    Loss 338.86972459894	
    Loss 339.2030822178	
    Loss 339.42274660995	
    Loss 339.06296168681	
    Loss 339.21522139514	
    Loss 338.52816897233	
    Loss 338.51435292096	
    Loss 339.42096003698	
    Loss 339.63304604241	
    Loss 339.00331678369	
    Loss 338.96065465272	
    Loss 338.66812558864	
    Loss 338.83169131869	
    Loss 338.82329287233	
    Loss 338.8775222839	
    Loss 339.09021635553	
    Loss 339.43420117446	
    Loss 338.83121789915	
    Loss 338.712005885	
    Loss 339.8542602932	
    Loss 340.2482288876	
    Loss 339.85930804796	
    Loss 340.28236253697	
    Loss 340.0341985622	
    Loss 340.00516533577	
    Loss 339.65194309123	
    Loss 339.80009231505	
    Loss 339.21882942544	
    Loss 339.09586386121	
    Loss 339.26603737442	
    Loss 339.30913023534	
    Loss 339.12653207058	
    Loss 339.24363084576	
    Loss 339.2278465212	
Epoch 14	
 108168
      0
      0
      0
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969096755	
    Loss 338.82185355185	
    Loss 338.96590300606	
    Loss 339.39530847035	
    Loss 339.38030838626	
    Loss 339.54517965999	
    Loss 339.86860099891	
    Loss 339.47882206233	
    Loss 338.69253364299	
    Loss 338.99938653847	
    Loss 339.65588505087	
    Loss 339.40723208174	
