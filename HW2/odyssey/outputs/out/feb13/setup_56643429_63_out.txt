[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	50	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4015
 2219
 1944
 2781
 1687
 3023
 1874
 3245
 4261
 6303
 1615
 1657
 3323
 1938
 2532
 3666
 6322
 3613
 4406
  933
  784
 2804
 1647
 1334
 1183
 4404
 1008
 4493
 1379
 2808
 2498
 2062
 2596
 1642
 2433
 4043
 1865
 2144
 4053
 2614
 3855
 4636
 4218
 6532
 3416
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02055262199563	
Grad norm	0	
    Loss 22781583.15947	
    Loss 7120.8267163454	
    Loss 9951.9324541083	
    Loss 10730.541143016	
    Loss 5051.3401134218	
    Loss 8364.421612208	
    Loss 4965.9539689992	
    Loss 6362.321980337	
    Loss 4471.9504741187	
    Loss 9167.0336118562	
    Loss 5433.0744478758	
    Loss 12434.064480836	
    Loss 9200.3294232757	
    Loss 6229.2882576072	
    Loss 7001.3600574171	
    Loss 3486.1663753855	
    Loss 4949.6075291316	
    Loss 3590.0487769811	
    Loss 4697.2191799566	
    Loss 10575.09573173	
    Loss 4502.175821364	
    Loss 7478.0181103118	
    Loss 6658.343539231	
    Loss 11062.317579003	
    Loss 12358.794172871	
    Loss 4884.0947535591	
    Loss 8733.9596335241	
    Loss 5548.2193226363	
    Loss 4414.0080214568	
    Loss 6103.8854917322	
    Loss 3625.6921955733	
    Loss 4105.4314417679	
    Loss 3067.9124304396	
    Loss 4086.5467619712	
    Loss 4857.2305476588	
    Loss 8928.138056711	
    Loss 7236.421238664	
    Loss 5045.2720706002	
    Loss 11333.055096387	
    Loss 4671.8163904611	
    Loss 3597.1756885987	
    Loss 5284.9894372903	
    Loss 10322.861968394	
    Loss 3902.0690433072	
    Loss 8036.1468423058	
    Loss 4272.0088954333	
    Loss 6370.5335246916	
    Loss 5150.9393123961	
    Loss 3096.7476347908	
    Loss 4321.3906410063	
    Loss 3810.1847961645	
    Loss 6637.7174819572	
    Loss 4044.4622649617	
    Loss 7184.3241376494	
    Loss 5058.2974361066	
    Loss 5505.5974904459	
    Loss 5209.2488189023	
    Loss 3247.6804327222	
    Loss 12214.01526748	
    Loss 6067.5674976908	
    Loss 3746.6165883639	
    Loss 9659.2344897165	
    Loss 4464.106467018	
    Loss 7436.503250829	
    Loss 9603.7099731359	
    Loss 11856.730016207	
    Loss 7387.4242684821	
    Loss 6045.9276520029	
    Loss 6493.6396657115	
    Loss 3427.7627217125	
    Loss 6903.4086548752	
    Loss 6573.4215393441	
Epoch 2	
      0
      0
      0
 112716
  17480
      0
      0
      0
      0
   1323
      6
     60
      0
      0
     65
      0
      0
    104
     54
[torch.DoubleTensor of size 19]

Validation accuracy:	0.058274156348628	
Grad norm	10.441080711155	
    Loss 8333.0676808151	
    Loss 5150.4730726048	
    Loss 8432.1490794705	
    Loss 7093.0779430965	
    Loss 9072.2420581134	
    Loss 6257.2728743723	
    Loss 9848.5989282638	
    Loss 8962.7317872454	
    Loss 5894.5804911345	
    Loss 4516.6370503884	
    Loss 5051.9489912865	
    Loss 6424.1606141647	
    Loss 2869.6186347377	
    Loss 5203.2088588041	
    Loss 7937.3344975806	
    Loss 8261.9457873248	
    Loss 4622.2426242499	
    Loss 5476.0938678896	
    Loss 10091.565359025	
    Loss 3548.7558195753	
    Loss 3809.3403198488	
    Loss 9141.4377297022	
    Loss 11167.796082212	
    Loss 8225.1673669904	
    Loss 5070.1873440173	
    Loss 14223.980605325	
    Loss 3584.3608093292	
    Loss 6895.1113086806	
    Loss 6028.1815284179	
    Loss 4053.9552433888	
    Loss 3789.1545923936	
    Loss 3111.8104852703	
    Loss 5051.3860937736	
    Loss 3355.6365084738	
    Loss 8348.4505876153	
    Loss 4453.6976073707	
    Loss 8318.8954298732	
    Loss 3165.0077503255	
    Loss 2772.992478154	
    Loss 8821.3843508194	
    Loss 4081.3764876102	
    Loss 11319.192829939	
    Loss 6979.8566069765	
    Loss 6961.6764658758	
    Loss 7743.4528575039	
    Loss 4478.8245965976	
    Loss 5793.7275785984	
    Loss 5336.8374704522	
    Loss 4810.401123413	
    Loss 9873.1178049714	
    Loss 7282.6664955974	
    Loss 8944.5453763702	
    Loss 5383.5822431114	
    Loss 8743.0549184146	
    Loss 9133.1866774803	
    Loss 6241.202437251	
    Loss 8260.7989159966	
    Loss 4396.5602079133	
    Loss 15191.864632345	
    Loss 8419.9598721711	
    Loss 4431.380487995	
    Loss 4479.2516872476	
    Loss 7187.7069374163	
    Loss 5161.6680561477	
    Loss 5599.7466809415	
    Loss 6158.2623219988	
    Loss 5155.8032549462	
    Loss 7197.3169740156	
    Loss 6427.1404218647	
    Loss 4569.529741068	
    Loss 6972.3897606639	
    Loss 6402.3680615223	
Epoch 3	
     0
    12
  1308
 35035
 53217
     0
     0
     0
 41047
     0
   754
    15
     0
     0
     0
     7
    24
     1
   388
[torch.DoubleTensor of size 19]

Validation accuracy:	0.088773063850449	
Grad norm	11.008920260518	
    Loss 12369.494194116	
    Loss 4181.2956261277	
    Loss 4088.5403939024	
    Loss 10048.277670705	
    Loss 7124.9714669655	
    Loss 6205.8413698897	
    Loss 3301.8741404362	
    Loss 4909.9927696385	
    Loss 5682.9589095431	
    Loss 3087.417281572	
    Loss 6027.2113804051	
    Loss 6215.6665670274	
    Loss 3457.5547503919	
    Loss 7397.218520748	
    Loss 7314.4272376078	
    Loss 9050.1561470387	
    Loss 6508.9918564632	
    Loss 4191.0438030925	
    Loss 12777.58584651	
    Loss 3938.9724586985	
    Loss 7688.4348594915	
    Loss 4676.7725615301	
    Loss 7780.0413807331	
    Loss 3600.1895934142	
    Loss 9739.3814330181	
    Loss 8119.1829423766	
    Loss 5438.2883554762	
    Loss 6198.9747577071	
    Loss 5771.2074514696	
    Loss 5948.6334311445	
    Loss 7945.2785942089	
    Loss 12829.62446252	
    Loss 9043.6436497943	
    Loss 5356.0236322235	
    Loss 7745.0396378527	
    Loss 6568.2773093722	
    Loss 5810.3400424324	
    Loss 8057.3495174418	
    Loss 9738.6667717729	
    Loss 2891.2260292396	
    Loss 7743.8089270288	
    Loss 15458.692748392	
    Loss 5048.3801785103	
    Loss 9478.0587148681	
    Loss 3516.8095766141	
    Loss 4810.2615097794	
    Loss 4767.0693054622	
    Loss 8919.6587718225	
    Loss 4028.3795740392	
    Loss 6481.1643930173	
    Loss 7549.1741310981	
    Loss 7059.335461409	
    Loss 5834.4094549982	
    Loss 10072.021419336	
    Loss 6876.4413256444	
    Loss 5808.0146935559	
    Loss 6479.0512850268	
    Loss 6122.4976926367	
    Loss 8047.388789445	
    Loss 7841.9174498468	
    Loss 3866.2920561433	
    Loss 4430.2570853283	
    Loss 6386.5930181071	
    Loss 4265.4117252479	
    Loss 5308.0990448279	
    Loss 7697.9038286423	
    Loss 7634.1527874792	
    Loss 5984.105714235	
    Loss 6113.8447180582	
    Loss 8144.7297261009	
    Loss 10760.310644813	
    Loss 5290.1600521153	
Epoch 4	
     0
     0
     0
     0
     0
     0
    13
     0
 23252
     0
  6851
  1806
     0
     0
 17746
 54209
   289
 27642
[torch.DoubleTensor of size 18]

Validation accuracy:	0.025286780772032	
Grad norm	10.470453506074	
    Loss 5502.0523924027	
    Loss 4253.6160653517	
    Loss 6343.9994463513	
    Loss 2933.0702823911	
    Loss 5459.4623600367	
    Loss 5085.0004143066	
    Loss 5518.5924183389	
    Loss 7596.4005497194	
    Loss 5192.931801321	
    Loss 5577.2875593108	
    Loss 6765.2115211067	
    Loss 7136.4308745471	
    Loss 4785.5815028609	
    Loss 4331.528878773	
    Loss 9976.9596879449	
    Loss 3954.6065297232	
    Loss 3713.1361016271	
    Loss 6500.2932960096	
    Loss 4048.5964393459	
    Loss 9016.461551762	
    Loss 8029.60003069	
    Loss 7506.6593135507	
    Loss 6902.6801919999	
    Loss 4856.7279432963	
    Loss 3007.411248873	
    Loss 4003.1544988991	
    Loss 6073.7208660031	
    Loss 6807.5977971894	
    Loss 4240.3741128328	
    Loss 3210.0154091513	
    Loss 3643.5506805729	
    Loss 4895.875932435	
    Loss 6210.9628801446	
    Loss 7131.136073069	
    Loss 5450.616891355	
    Loss 6896.3825362147	
    Loss 7942.8566001293	
    Loss 9153.2293628424	
    Loss 8129.7103299759	
    Loss 7283.1232567185	
    Loss 4965.5900703974	
    Loss 8583.3520106262	
    Loss 4274.7230482329	
    Loss 10478.614118623	
    Loss 4026.0375369653	
    Loss 9446.1618498758	
    Loss 4935.1072210226	
    Loss 3112.5361086457	
    Loss 4046.75338234	
    Loss 7405.7595010166	
    Loss 6542.842367765	
    Loss 6804.6211161578	
    Loss 6556.7876862385	
    Loss 5134.6178191077	
    Loss 4121.4033924769	
    Loss 3847.5089557888	
    Loss 8632.1951001128	
    Loss 4236.3497982872	
    Loss 11659.142666725	
    Loss 5227.7706593987	
    Loss 6823.0035081625	
    Loss 3261.3369347334	
    Loss 6940.4538299479	
    Loss 7171.4014225487	
    Loss 3773.5257861584	
    Loss 6347.3804760018	
    Loss 4351.3438345694	
    Loss 6177.7266715556	
    Loss 4694.8990162186	
    Loss 9085.3868730703	
    Loss 5019.8469949673	
    Loss 10604.169204429	
Epoch 5	
     0
     0
     0
     0
     0
     0
     0
 13150
     0
     0
 75689
 20557
     0
     0
  1510
 11371
  1291
  5486
  2754
[torch.DoubleTensor of size 19]

Validation accuracy:	0.0075943797038116	
Grad norm	10.770407351262	
    Loss 7054.7987585063	
    Loss 4964.7663493147	
    Loss 7433.9771887471	
    Loss 10223.308203156	
    Loss 8047.6444146729	
    Loss 6786.508287906	
    Loss 3609.4185788709	
    Loss 9178.3972003326	
    Loss 8048.23739396	
    Loss 6683.6280228317	
    Loss 7856.3387773316	
    Loss 4842.8243085865	
    Loss 4149.2051339229	
    Loss 5221.3042753649	
    Loss 5224.6922701167	
    Loss 9035.3173202017	
    Loss 6928.8590436013	
    Loss 6098.2892062052	
    Loss 7210.7434603437	
    Loss 9467.9970581697	
    Loss 5525.1549093984	
    Loss 7607.5143264906	
    Loss 7031.0067849183	
    Loss 6381.187650006	
    Loss 11035.965974757	
    Loss 6822.8824067451	
    Loss 7000.1387200634	
    Loss 5260.9385665231	
    Loss 6773.8931291553	
    Loss 4357.5054991074	
    Loss 2853.6942859935	
    Loss 8806.0170809104	
    Loss 6981.2462206796	
    Loss 7184.0341326801	
    Loss 5259.0446788797	
    Loss 4379.4580867611	
    Loss 7677.701265138	
    Loss 6973.2461185755	
    Loss 5245.3425896554	
    Loss 6020.6096847309	
    Loss 3154.5383599502	
    Loss 3735.4875394898	
    Loss 5865.3740152359	
    Loss 12008.949656568	
    Loss 5648.9604287979	
    Loss 7423.6174255845	
    Loss 5014.0544423458	
    Loss 3757.8232081309	
    Loss 5704.6484650709	
    Loss 4802.6731140869	
    Loss 4981.1457539074	
    Loss 4668.5442000144	
    Loss 5112.6414671399	
    Loss 5989.4003422651	
    Loss 4462.6484857296	
    Loss 6376.7693247143	
    Loss 4502.8639622856	
    Loss 7524.8777183317	
    Loss 11134.683422821	
    Loss 11793.289400247	
    Loss 8829.9026967705	
    Loss 2537.2131880826	
    Loss 11240.427321575	
    Loss 7446.6075984422	
    Loss 7211.0922643262	
    Loss 6363.0303611952	
    Loss 5731.4783869577	
    Loss 3239.8379770351	
    Loss 4442.1940841311	
    Loss 3691.2097785464	
    Loss 5388.7061593021	
    Loss 3964.4819392101	
Epoch 6	
     0
  9803
     0
     0
 79075
     0
     0
 26754
     0
 15281
     0
   407
     0
     0
     0
   203
     0
   285
[torch.DoubleTensor of size 18]

Validation accuracy:	0.058577628065064	
Grad norm	10.84611212931	
    Loss 11035.048848339	
    Loss 4164.6666544655	
    Loss 6592.9690461321	
    Loss 2978.2830341588	
    Loss 5946.8995947784	
    Loss 4658.9944021018	
    Loss 3411.9354315137	
    Loss 5331.4042647137	
    Loss 5874.3462892747	
    Loss 4998.2356720428	
    Loss 10517.842733285	
    Loss 3628.9536314377	
    Loss 3518.2355050939	
    Loss 4491.5455535479	
    Loss 4689.0327997152	
    Loss 3168.1858423171	
    Loss 2968.7074818093	
    Loss 4053.6794520831	
    Loss 5608.9787256075	
    Loss 6095.9232326572	
    Loss 6272.6349217415	
    Loss 7461.9969921897	
    Loss 4263.1415633398	
    Loss 3765.7704157315	
    Loss 6933.3582396351	
    Loss 6395.8377568235	
    Loss 9227.252635007	
    Loss 5837.8342171647	
    Loss 5274.4673158685	
    Loss 5163.2959865784	
    Loss 5810.9499613218	
    Loss 5648.8605387858	
    Loss 4193.0073067925	
    Loss 8569.1142459922	
    Loss 6410.8310312313	
    Loss 6600.307846755	
    Loss 8072.8550184082	
    Loss 2732.7081513651	
    Loss 4810.9050274034	
    Loss 4161.7339380856	
    Loss 3332.6991813715	
    Loss 6331.6521789249	
    Loss 6621.0479809468	
    Loss 7933.4758568545	
    Loss 6834.6213416394	
    Loss 5456.8871885166	
    Loss 5468.6236355921	
    Loss 5512.8615611548	
    Loss 6522.2671207116	
    Loss 5385.7816416244	
    Loss 7297.2354712999	
    Loss 11037.150933144	
    Loss 9302.069995933	
    Loss 11109.774247298	
    Loss 4517.5443578254	
    Loss 3475.7974650896	
    Loss 5180.1376495922	
    Loss 3946.7761355792	
    Loss 9151.2385129614	
    Loss 5121.729771033	
    Loss 4682.1775196861	
    Loss 4118.8907688551	
    Loss 6426.6969618534	
    Loss 5446.6566831366	
    Loss 4121.6183865252	
    Loss 6708.3717458296	
    Loss 5308.0136032231	
    Loss 5757.7635562248	
    Loss 5324.432472989	
    Loss 6476.0651522001	
    Loss 4974.5871890197	
    Loss 6304.4020862988	
Epoch 7	
     0
 21555
     0
     0
 87483
     0
     0
 22769
     0
     0
     0
     0
     0
     0
     0
     0
     0
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.079373027433843	
Grad norm	10.123861606882	
    Loss 6578.2712231658	
    Loss 6714.6030168323	
    Loss 6416.9079350714	
    Loss 3991.0063384079	
    Loss 2939.8709186047	
    Loss 5719.9699799696	
    Loss 9559.2109753263	
    Loss 7877.4880363907	
    Loss 4672.509769295	
    Loss 8215.434569679	
    Loss 2995.5012067991	
    Loss 7002.006749762	
    Loss 3450.805235495	
    Loss 3107.1758692572	
    Loss 5284.2872992985	
    Loss 8951.7428816318	
    Loss 4346.9345458041	
    Loss 2803.0027452356	
    Loss 4341.7110476208	
    Loss 9995.6514441875	
    Loss 8735.715179601	
    Loss 10205.447664338	
    Loss 8113.9963865611	
    Loss 3574.9310179266	
    Loss 7758.8092543461	
    Loss 14114.049293536	
    Loss 6319.0215170197	
    Loss 7182.0075205526	
    Loss 5348.9706822879	
    Loss 3688.9295112937	
    Loss 7267.9249817104	
    Loss 9268.0723327492	
    Loss 9082.0403584737	
    Loss 5775.3367649799	
    Loss 3348.4580679276	
    Loss 6234.2261162895	
    Loss 4185.1840959902	
    Loss 3571.4403321455	
    Loss 7342.9218443269	
    Loss 3819.8455183168	
    Loss 4028.5829348978	
    Loss 4214.0813348696	
    Loss 4695.9032371079	
    Loss 7181.7406391542	
    Loss 8616.0826929044	
    Loss 4500.7345397663	
    Loss 4749.3036845384	
    Loss 3770.2482300998	
    Loss 6855.24495989	
    Loss 4025.6786130684	
    Loss 3988.6841570443	
    Loss 6080.9024568231	
    Loss 7565.9858602998	
    Loss 6954.8548187922	
    Loss 9535.5581102208	
    Loss 4001.3487583877	
    Loss 3484.3071322269	
    Loss 6222.0271522152	
    Loss 8558.4280965427	
    Loss 6647.3987305217	
    Loss 3370.2722929748	
    Loss 3430.8893125367	
    Loss 5344.99568468	
    Loss 6823.770934481	
    Loss 8135.7172805846	
    Loss 8680.529864958	
    Loss 6931.0495265884	
    Loss 7260.0984361931	
    Loss 5327.5156051802	
    Loss 8918.1599740266	
    Loss 10458.45002998	
    Loss 8830.6493231996	
Epoch 8	
      0
      0
      0
      0
   3618
      0
      0
   6108
      0
 106925
  13884
   1194
      0
      0
      2
      0
      2
     74
      1
[torch.DoubleTensor of size 19]

Validation accuracy:	0.090396637533382	
Grad norm	10.924297855489	
    Loss 11442.04874985	
    Loss 9259.0819198916	
    Loss 8841.4704826427	
    Loss 3238.6493253279	
    Loss 5203.4381394316	
    Loss 5697.9891315631	
    Loss 6825.2921162786	
    Loss 4490.8393564275	
    Loss 7333.1471446341	
    Loss 4292.9022589538	
    Loss 4246.868239564	
    Loss 5244.860683035	
    Loss 3918.3548652914	
    Loss 7032.1363743151	
    Loss 6925.4638543568	
    Loss 6677.898716254	
    Loss 7470.9072614985	
    Loss 3559.7719753865	
    Loss 12066.3701502	
    Loss 7643.8000143858	
    Loss 2828.4362129538	
    Loss 5283.1541714732	
    Loss 9075.3793321936	
    Loss 9496.3124179452	
    Loss 4753.9891562428	
    Loss 7020.1016637099	
    Loss 9701.4856086687	
    Loss 10679.165567205	
    Loss 4019.5257572489	
    Loss 2773.5682001742	
    Loss 4393.7096688513	
    Loss 6289.1993725532	
    Loss 4838.3236604201	
    Loss 4611.3590537847	
    Loss 8850.0428652886	
    Loss 4482.2698868824	
    Loss 7935.0176748491	
    Loss 3542.5532326367	
    Loss 6205.8505055037	
    Loss 7788.7484147208	
    Loss 3595.660994633	
    Loss 11252.33344829	
    Loss 9487.8997905443	
    Loss 8705.9318902387	
    Loss 5738.2915919678	
    Loss 5068.1420629365	
    Loss 3947.04294371	
    Loss 4044.3249935093	
    Loss 4771.1809909649	
    Loss 5277.6937216596	
    Loss 4687.2419886038	
    Loss 3419.0635051865	
    Loss 4987.1815065297	
    Loss 2896.7575337931	
    Loss 6685.4640632249	
    Loss 7031.3960825336	
    Loss 6528.4729442604	
    Loss 6574.3839793205	
    Loss 7931.7177300354	
    Loss 4843.8554128583	
    Loss 8011.9729904243	
    Loss 3851.3731675689	
    Loss 5751.6616542031	
    Loss 4663.4616935372	
    Loss 5684.949093468	
    Loss 6476.2128901423	
    Loss 6227.6538154302	
    Loss 8416.7534919775	
    Loss 3618.3883474745	
    Loss 4578.2826867682	
    Loss 6865.4116952446	
    Loss 7401.7837989495	
Epoch 9	
     0
     0
     0
     0
 51478
     0
     0
 30568
 49762
[torch.DoubleTensor of size 9]

Validation accuracy:	0.067112770089828	
Grad norm	10.226293030976	
    Loss 6684.3447989618	
    Loss 8395.5883023766	
    Loss 7050.3407558837	
    Loss 5265.4898714079	
    Loss 7607.5094442021	
    Loss 4174.3058208962	
    Loss 6725.9504110283	
    Loss 5283.1132486817	
    Loss 7706.3079542536	
    Loss 5434.7625286163	
    Loss 5978.1194677189	
    Loss 11378.727953238	
    Loss 4209.776997072	
    Loss 5516.830889338	
    Loss 5178.6475291788	
    Loss 6818.8109786418	
    Loss 3302.2176758259	
    Loss 4101.2244290138	
    Loss 6535.8219640936	
    Loss 3131.3703327469	
    Loss 3516.2982513972	
    Loss 9824.7824269134	
    Loss 4916.7731152058	
    Loss 4321.0422114386	
    Loss 4677.9605657876	
