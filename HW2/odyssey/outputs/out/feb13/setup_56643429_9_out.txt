[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.25	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3285
 2442
 1284
 3419
 2196
 1530
 2433
 2210
 1727
  963
 4192
 2102
 1737
 5634
 3464
 1149
 1114
 6608
 3531
 2717
 2125
 3344
 4280
 3522
 3005
 2502
 1959
  998
  717
 3531
 2659
 4349
 1687
 3995
 6154
 5127
 1610
 3504
 1457
  647
 3273
 3487
 3535
 8441
 2163
[torch.DoubleTensor of size 45]

Validation accuracy:	0.019232520029133	
Grad norm	0	
    Loss 2275775.736267	
    Loss 2267757.1082956	
    Loss 2259774.9101848	
    Loss 2251821.8598954	
    Loss 2243897.1565095	
    Loss 2236002.2762163	
    Loss 2228134.2325274	
    Loss 2220296.1267792	
    Loss 2212485.1557383	
    Loss 2204701.6999959	
    Loss 2196945.5110198	
    Loss 2189218.3706576	
    Loss 2181518.7010563	
    Loss 2173846.514567	
    Loss 2166201.7643397	
    Loss 2158585.34238	
    Loss 2150993.4967441	
    Loss 2143428.7683377	
    Loss 2135891.8130293	
    Loss 2128381.3404545	
    Loss 2120896.1783498	
    Loss 2113438.5068075	
    Loss 2106007.2922416	
    Loss 2098602.5989407	
    Loss 2091225.5720096	
    Loss 2083874.2587308	
    Loss 2076549.3556109	
    Loss 2069249.8204548	
    Loss 2061975.3230429	
    Loss 2054727.0109734	
    Loss 2047503.0926572	
    Loss 2040304.8018163	
    Loss 2033130.868184	
    Loss 2025984.7760233	
    Loss 2018862.3218891	
    Loss 2011766.6032862	
    Loss 2004694.9791137	
    Loss 1997648.5539057	
    Loss 1990626.2761676	
    Loss 1983629.0522139	
    Loss 1976657.1219266	
    Loss 1969709.2528004	
    Loss 1962785.834334	
    Loss 1955887.9361288	
    Loss 1949013.1386198	
    Loss 1942163.8154235	
    Loss 1935336.9038041	
    Loss 1928534.8699392	
    Loss 1921757.1145025	
    Loss 1915004.0300461	
    Loss 1908273.8234709	
    Loss 1901566.63807	
    Loss 1894884.2459513	
    Loss 1888223.6596157	
    Loss 1881588.171547	
    Loss 1874974.1754659	
    Loss 1868384.9636147	
    Loss 1861818.036269	
    Loss 1855276.399813	
    Loss 1848755.8634677	
    Loss 1842259.9437516	
    Loss 1835787.0880424	
    Loss 1829336.3440591	
    Loss 1822908.1765041	
    Loss 1816501.7443669	
    Loss 1810118.9180627	
    Loss 1803758.6808883	
    Loss 1797420.2574611	
    Loss 1791104.1936492	
    Loss 1784810.955862	
    Loss 1778540.4506147	
    Loss 1772291.6924662	
    Loss 1766065.2253106	
    Loss 1759861.1034475	
    Loss 1753678.6604149	
    Loss 1747517.4280217	
    Loss 1741377.6284143	
    Loss 1735259.4085185	
    Loss 1729162.5556596	
    Loss 1723086.960838	
    Loss 1717033.2306402	
    Loss 1711000.2245509	
    Loss 1704989.5034797	
    Loss 1698999.3763814	
    Loss 1693030.149346	
    Loss 1687082.6620945	
    Loss 1681156.5785519	
    Loss 1675250.5763443	
    Loss 1669364.5237621	
    Loss 1663499.1837222	
    Loss 1657653.9169267	
    Loss 1651830.4278542	
    Loss 1646027.3036091	
    Loss 1640244.635599	
    Loss 1634483.8588626	
    Loss 1628742.4611942	
    Loss 1623019.7537007	
    Loss 1617318.6564164	
    Loss 1611636.7616551	
    Loss 1605974.1387626	
    Loss 1600332.22652	
    Loss 1594710.1611143	
    Loss 1589107.3480449	
    Loss 1583524.9853489	
    Loss 1577961.4755491	
    Loss 1572419.3824341	
    Loss 1566895.3043403	
    Loss 1561391.1144863	
    Loss 1555905.8554936	
    Loss 1550440.5642348	
    Loss 1544993.4070526	
    Loss 1539565.7471435	
    Loss 1534158.651663	
    Loss 1528768.8683332	
    Loss 1523398.0349846	
    Loss 1518046.2213236	
    Loss 1512713.85461	
    Loss 1507399.8890653	
    Loss 1502104.5136684	
    Loss 1496827.6048983	
    Loss 1491568.5998516	
    Loss 1486329.2964205	
    Loss 1481109.219361	
    Loss 1475906.7786978	
    Loss 1470722.1292707	
    Loss 1465556.4305356	
    Loss 1460408.2793283	
    Loss 1455277.8286854	
    Loss 1450165.1480018	
    Loss 1445071.3160829	
    Loss 1439995.8974281	
    Loss 1434938.4976996	
    Loss 1429899.5383321	
    Loss 1424877.6952232	
    Loss 1419873.0280674	
    Loss 1414885.1661363	
    Loss 1409915.0902713	
    Loss 1404962.9055427	
    Loss 1400027.404945	
    Loss 1395109.5023182	
    Loss 1390210.4680355	
    Loss 1385326.963543	
    Loss 1380460.8928707	
Epoch 2	
 63617
  6046
  2360
   975
 14023
   203
   187
 10380
 23310
  6624
  2136
   153
   155
   183
   214
   882
   177
   183
[torch.DoubleTensor of size 18]

Validation accuracy:	0.094243141539209	
Grad norm	203.39938100302	
    Loss 1377550.5414287	
    Loss 1372712.2464127	
    Loss 1367890.5093357	
    Loss 1363085.1188992	
    Loss 1358297.4631279	
    Loss 1353527.1815005	
    Loss 1348773.1572232	
    Loss 1344035.6674796	
    Loss 1339314.752689	
    Loss 1334610.8736472	
    Loss 1329922.4381388	
    Loss 1325252.1577218	
    Loss 1320598.0895932	
    Loss 1315960.7005577	
    Loss 1311339.229401	
    Loss 1306734.4197662	
    Loss 1302144.4897718	
    Loss 1297570.5806065	
    Loss 1293013.6923014	
    Loss 1288472.3725179	
    Loss 1283945.9493941	
    Loss 1279436.0234372	
    Loss 1274941.9219263	
    Loss 1270463.9151987	
    Loss 1266002.8620073	
    Loss 1261557.3958866	
    Loss 1257127.6896672	
    Loss 1252713.3932802	
    Loss 1248313.7864904	
    Loss 1243930.0470905	
    Loss 1239561.0231513	
    Loss 1235207.1647366	
    Loss 1230868.2914746	
    Loss 1226545.6456422	
    Loss 1222237.2919356	
    Loss 1217945.3816036	
    Loss 1213667.8082576	
    Loss 1209405.6472849	
    Loss 1205157.9249823	
    Loss 1200925.6134511	
    Loss 1196708.1000511	
    Loss 1192504.968495	
    Loss 1188317.004017	
    Loss 1184144.7020509	
    Loss 1179985.5976358	
    Loss 1175842.2605705	
    Loss 1171712.3237713	
    Loss 1167597.4164055	
    Loss 1163497.4321997	
    Loss 1159412.3097628	
    Loss 1155340.9919417	
    Loss 1151283.1023388	
    Loss 1147240.8548505	
    Loss 1143211.1939958	
    Loss 1139196.7121592	
    Loss 1135194.8233941	
    Loss 1131208.3790453	
    Loss 1127235.4349928	
    Loss 1123277.6797626	
    Loss 1119332.3703126	
    Loss 1115402.2662927	
    Loss 1111486.1451041	
    Loss 1107583.0162356	
    Loss 1103693.7048299	
    Loss 1099817.4486744	
    Loss 1095955.5662373	
    Loss 1092107.1676598	
    Loss 1088272.0081877	
    Loss 1084450.3767598	
    Loss 1080642.2343239	
    Loss 1076848.280994	
    Loss 1073067.4123047	
    Loss 1069300.1419477	
    Loss 1065545.9912458	
    Loss 1061805.2973795	
    Loss 1058077.1515556	
    Loss 1054362.0088262	
    Loss 1050660.0256628	
    Loss 1046970.6631831	
    Loss 1043294.1909244	
    Loss 1039630.9164169	
    Loss 1035980.1016821	
    Loss 1032342.9145338	
    Loss 1028718.070738	
    Loss 1025105.9395442	
    Loss 1021507.1357404	
    Loss 1017921.2893863	
    Loss 1014347.3099293	
    Loss 1010785.4556884	
    Loss 1007236.0006521	
    Loss 1003698.7049057	
    Loss 1000174.5732387	
    Loss 996663.05064417	
    Loss 993163.79216967	
    Loss 989677.59812688	
    Loss 986203.25246762	
    Loss 982740.00896774	
    Loss 979290.02429107	
    Loss 975851.54479421	
    Loss 972424.67498787	
    Loss 969010.29172047	
    Loss 965607.85497436	
    Loss 962217.0372845	
    Loss 958838.67131844	
    Loss 955471.63352024	
    Loss 952117.68538648	
    Loss 948774.83037658	
    Loss 945443.70359098	
    Loss 942123.98423556	
    Loss 938816.56149646	
    Loss 935519.94990175	
    Loss 932235.10998271	
    Loss 928962.73287941	
    Loss 925700.75470479	
    Loss 922450.44486719	
    Loss 919211.38621352	
    Loss 915984.28686156	
    Loss 912768.18814439	
    Loss 909563.3614736	
    Loss 906369.60469997	
    Loss 903186.78296103	
    Loss 900016.01690239	
    Loss 896856.74002926	
    Loss 893708.19477411	
    Loss 890570.20125694	
    Loss 887443.63108779	
    Loss 884327.69223356	
    Loss 881222.58359222	
    Loss 878128.13337588	
    Loss 875045.09108116	
    Loss 871973.24041465	
    Loss 868912.3208992	
    Loss 865862.6592013	
    Loss 862823.26437746	
    Loss 859794.16073425	
    Loss 856775.09416349	
    Loss 853766.88709562	
    Loss 850769.44630569	
    Loss 847782.21880306	
    Loss 844805.61661071	
    Loss 841840.63012457	
    Loss 838884.93155179	
    Loss 835939.6936002	
Epoch 3	
 79381
  3975
  1035
   249
 11632
    40
    38
  7506
 22095
  4418
   936
    33
    27
    24
    59
   296
    35
    29
[torch.DoubleTensor of size 18]

Validation accuracy:	0.096875758679291	
Grad norm	159.38342061908	
    Loss 834178.19390395	
    Loss 831249.80620406	
    Loss 828331.38687405	
    Loss 825422.68092612	
    Loss 822524.86963642	
    Loss 819637.49740505	
    Loss 816760.12731452	
    Loss 813892.44393317	
    Loss 811034.94774065	
    Loss 808187.92068946	
    Loss 805350.13846925	
    Loss 802523.37354116	
    Loss 799706.41897476	
    Loss 796899.54776361	
    Loss 794102.23238064	
    Loss 791315.05296809	
    Loss 788536.8024673	
    Loss 785768.26053522	
    Loss 783010.00739855	
    Loss 780261.16638664	
    Loss 777521.23741908	
    Loss 774791.28292498	
    Loss 772070.910733	
    Loss 769360.3644124	
    Loss 766660.15121104	
    Loss 763969.39414026	
    Loss 761288.12787479	
    Loss 758616.26739335	
    Loss 755953.21320479	
    Loss 753299.66647096	
    Loss 750655.05244787	
    Loss 748019.58485452	
    Loss 745393.21382896	
    Loss 742776.59969229	
    Loss 740168.5581848	
    Loss 737570.60286162	
    Loss 734981.29620687	
    Loss 732401.2559912	
    Loss 729830.03849765	
    Loss 727268.15256058	
    Loss 724715.11105537	
    Loss 722170.73835976	
    Loss 719635.61197636	
    Loss 717110.12947162	
    Loss 714592.49839577	
    Loss 712084.45347033	
    Loss 709584.44620981	
    Loss 707093.57034967	
    Loss 704611.79612921	
    Loss 702139.01778901	
    Loss 699674.61534848	
    Loss 697218.22539094	
    Loss 694771.43838077	
    Loss 692332.13706048	
    Loss 689902.07859159	
    Loss 687479.45587534	
    Loss 685066.25136577	
    Loss 682661.33372096	
    Loss 680265.64858356	
    Loss 677877.27512664	
    Loss 675498.20296079	
    Loss 673127.69699635	
    Loss 670764.81025998	
    Loss 668410.32611214	
    Loss 666063.81386209	
    Loss 663725.98234523	
    Loss 661396.38957588	
    Loss 659074.77486678	
    Loss 656761.34925147	
    Loss 654456.0251338	
    Loss 652159.39110375	
    Loss 649870.65580192	
    Loss 647590.21417842	
    Loss 645317.64537499	
    Loss 643053.25115434	
    Loss 640796.40866687	
    Loss 638547.3890838	
    Loss 636306.38997963	
    Loss 634072.94120788	
    Loss 631847.31459701	
    Loss 629629.67053791	
    Loss 627419.5556865	
    Loss 625217.65356286	
    Loss 623023.22325881	
    Loss 620836.55205683	
    Loss 618657.98627982	
    Loss 616487.31307071	
    Loss 614323.71446223	
    Loss 612167.46371216	
    Loss 610018.70117016	
    Loss 607877.28731894	
    Loss 605743.80436781	
    Loss 603618.09463787	
    Loss 601499.79002087	
    Loss 599389.31543575	
    Loss 597286.07705014	
    Loss 595189.45683148	
    Loss 593100.87918667	
    Loss 591019.29105539	
    Loss 588944.70695708	
    Loss 586877.6874732	
    Loss 584817.82622851	
    Loss 582765.00968257	
    Loss 580719.78566747	
    Loss 578681.40013996	
    Loss 576650.99433973	
    Loss 574627.36627964	
    Loss 572610.7065895	
    Loss 570600.95010482	
    Loss 568598.67280577	
    Loss 566602.95080887	
    Loss 564614.29729721	
    Loss 562633.2436538	
    Loss 560658.43542537	
    Loss 558690.78811415	
    Loss 556729.83935345	
    Loss 554776.20384088	
    Loss 552829.18128423	
    Loss 550888.95849062	
    Loss 548955.39227119	
    Loss 547028.50434231	
    Loss 545109.01489888	
    Loss 543196.35628961	
    Loss 541290.25195265	
    Loss 539390.43720426	
    Loss 537497.50757729	
    Loss 535611.0589018	
    Loss 533731.2069464	
    Loss 531857.81118477	
    Loss 529991.26125275	
    Loss 528131.47197314	
    Loss 526278.35821433	
    Loss 524432.08012395	
    Loss 522592.04627171	
    Loss 520758.17119076	
    Loss 518930.31914391	
    Loss 517109.04487565	
    Loss 515294.2970514	
    Loss 513485.78635819	
    Loss 511683.69883722	
    Loss 509888.74380067	
    Loss 508099.36340457	
    Loss 506316.22898305	
Epoch 4	
 93790
  2164
   349
    56
  8565
     2
     2
  4902
 19279
  2299
   323
     3
     3
     1
     4
    60
     3
     3
[torch.DoubleTensor of size 18]

Validation accuracy:	0.098992473901432	
Grad norm	125.15304344638	
    Loss 505249.78308479	
    Loss 503476.91073015	
    Loss 501710.04406488	
    Loss 499948.99628771	
    Loss 498194.60364285	
    Loss 496446.48507597	
    Loss 494704.49912568	
    Loss 492968.2503561	
    Loss 491238.23791049	
    Loss 489514.61936541	
    Loss 487796.54317181	
    Loss 486085.15738401	
    Loss 484379.71040976	
    Loss 482680.34061639	
    Loss 480986.74151136	
    Loss 479299.34597218	
    Loss 477617.29006758	
    Loss 475941.11444292	
    Loss 474271.17844262	
    Loss 472606.88708229	
    Loss 470947.98515278	
    Loss 469295.1231971	
    Loss 467648.05189037	
    Loss 466006.97897888	
    Loss 464372.19093164	
    Loss 462743.13425648	
    Loss 461119.79736741	
    Loss 459502.22403364	
    Loss 457889.93323233	
    Loss 456283.32669237	
    Loss 454682.16394582	
    Loss 453086.50652292	
    Loss 451496.36141264	
    Loss 449912.13091878	
    Loss 448333.0165688	
    Loss 446760.10135398	
    Loss 445192.39749334	
    Loss 443630.25379017	
    Loss 442073.52035225	
    Loss 440522.41636366	
    Loss 438976.62756983	
    Loss 437436.08022647	
    Loss 435901.11495457	
    Loss 434372.10287586	
    Loss 432847.81348848	
    Loss 431329.33315996	
    Loss 429815.65806851	
    Loss 428307.53713085	
    Loss 426804.96488534	
    Loss 425307.83159119	
    Loss 423815.76230376	
    Loss 422328.52684881	
    Loss 420847.13795695	
    Loss 419370.23299174	
    Loss 417898.98399266	
    Loss 416432.13272292	
    Loss 414970.97985237	
    Loss 413514.93147024	
    Loss 412064.52511116	
    Loss 410618.40933893	
    Loss 409177.94989381	
    Loss 407742.77363312	
    Loss 406312.056408	
    Loss 404886.41382769	
    Loss 403465.67642402	
    Loss 402050.17295909	
    Loss 400639.74356471	
    Loss 399234.09361056	
    Loss 397833.40410915	
    Loss 396437.61370929	
    Loss 395047.09536768	
    Loss 393661.34131844	
    Loss 392280.6519063	
    Loss 390904.71526815	
    Loss 389533.72240161	
    Loss 388167.30554453	
    Loss 386805.57334588	
    Loss 385448.71452186	
    Loss 384096.40151321	
    Loss 382748.83706029	
    Loss 381406.09325911	
    Loss 380067.92251697	
    Loss 378734.6625445	
    Loss 377405.94509601	
    Loss 376081.97534598	
    Loss 374762.89609923	
    Loss 373448.63546544	
    Loss 372138.61266394	
    Loss 370833.04230774	
    Loss 369532.0057951	
    Loss 368235.42163727	
    Loss 366943.58823794	
    Loss 365656.54955061	
    Loss 364373.99018569	
    Loss 363096.12284502	
    Loss 361822.67772823	
    Loss 360553.18701353	
    Loss 359288.53519362	
    Loss 358028.16751693	
    Loss 356772.02838368	
    Loss 355520.48595382	
    Loss 354273.21766762	
    Loss 353030.23342023	
    Loss 351791.86964589	
    Loss 350557.66001454	
    Loss 349328.28805836	
    Loss 348103.05314116	
    Loss 346881.97740397	
    Loss 345665.07509284	
    Loss 344452.67109805	
    Loss 343244.29001707	
    Loss 342040.13541376	
    Loss 340840.64696072	
    Loss 339644.91171201	
    Loss 338453.55573724	
    Loss 337266.20171743	
    Loss 336083.29368575	
    Loss 334904.38231511	
    Loss 333729.54835238	
    Loss 332558.72299399	
    Loss 331391.97808267	
    Loss 330229.78693347	
    Loss 329071.65435954	
    Loss 327917.51955301	
    Loss 326767.14303616	
    Loss 325620.91787303	
    Loss 324478.643099	
    Loss 323340.38662251	
    Loss 322206.06035025	
    Loss 321075.82300267	
    Loss 319949.65131645	
    Loss 318827.57577703	
    Loss 317709.63403516	
    Loss 316595.52037395	
    Loss 315485.09229047	
    Loss 314378.28046197	
    Loss 313275.4399646	
    Loss 312176.56551883	
    Loss 311081.49746936	
    Loss 309990.31436947	
    Loss 308903.51226649	
    Loss 307820.05951399	
    Loss 306740.32962289	
Epoch 5	
 106564
    924
     86
      7
   5280
      0
      0
   2595
  15169
   1101
     73
      0
      0
      0
      0
      9
[torch.DoubleTensor of size 16]

Validation accuracy:	0.098484158776402	
Grad norm	98.532496939875	
    Loss 306094.57836505	
    Loss 305021.10739801	
    Loss 303951.25503291	
    Loss 302884.90968966	
    Loss 301822.61134892	
    Loss 300764.07184474	
    Loss 299709.29310249	
    Loss 298657.94007727	
    Loss 297610.3993767	
    Loss 296566.74358147	
    Loss 295526.40324034	
    Loss 294490.13180469	
    Loss 293457.44641038	
    Loss 292428.42828194	
    Loss 291402.90489253	
    Loss 290381.19356753	
    Loss 289362.67575726	
    Loss 288347.70789916	
    Loss 287336.53573164	
    Loss 286328.72024959	
    Loss 285324.19542533	
    Loss 284323.33271315	
    Loss 283325.95789626	
    Loss 282332.24936109	
    Loss 281342.35389094	
    Loss 280355.93175369	
    Loss 279372.95310165	
    Loss 278393.52108104	
    Loss 277417.25416114	
    Loss 276444.38302185	
    Loss 275474.84633674	
    Loss 274508.60641812	
    Loss 273545.70338371	
    Loss 272586.40155382	
    Loss 271630.14289982	
    Loss 270677.70479461	
    Loss 269728.39799848	
    Loss 268782.41708982	
    Loss 267839.76870863	
    Loss 266900.50992972	
    Loss 265964.45429176	
    Loss 265031.57858179	
    Loss 264102.03387474	
    Loss 263176.18690876	
    Loss 262253.19810174	
    Loss 261333.72446911	
    Loss 260417.11210842	
    Loss 259503.87746315	
    Loss 258594.0338212	
    Loss 257687.46516726	
    Loss 256783.9529666	
    Loss 255883.3805525	
    Loss 254986.34214388	
    Loss 254092.00513995	
    Loss 253201.14613447	
    Loss 252312.89374418	
    Loss 251428.06389093	
    Loss 250546.38562658	
    Loss 249668.17373572	
    Loss 248792.46950766	
    Loss 247920.19059117	
    Loss 247051.19055342	
    Loss 246184.79291528	
    Loss 245321.43111631	
    Loss 244461.11926544	
    Loss 243603.95840483	
    Loss 242749.94069629	
    Loss 241898.77077183	
    Loss 241050.60703025	
    Loss 240205.41682387	
    Loss 239363.40373618	
    Loss 238524.25871966	
    Loss 237688.21208605	
    Loss 236855.04204504	
    Loss 236024.85841038	
    Loss 235197.46635798	
    Loss 234372.86292126	
    Loss 233551.21356454	
    Loss 232732.31635684	
    Loss 231916.30488705	
    Loss 231103.1927198	
    Loss 230292.87291861	
    Loss 229485.46861351	
    Loss 228680.83395493	
    Loss 227879.11506275	
    Loss 227080.321308	
    Loss 226284.47317764	
    Loss 225491.17806669	
    Loss 224700.57573325	
    Loss 223912.72730445	
    Loss 223127.59093312	
    Loss 222345.27292701	
    Loss 221565.92513537	
    Loss 220789.28758411	
    Loss 220015.45403374	
    Loss 219244.32758027	
    Loss 218475.56964612	
    Loss 217709.69719364	
    Loss 216946.47426028	
    Loss 216185.80494578	
    Loss 215427.9364967	
    Loss 214672.61135625	
    Loss 213919.89915446	
    Loss 213169.99051492	
    Loss 212422.62587589	
    Loss 211678.17357382	
    Loss 210936.24994276	
    Loss 210196.81671321	
    Loss 209459.89928545	
    Loss 208725.65131262	
    Loss 207993.9065414	
    Loss 207264.67268963	
    Loss 206538.33113605	
    Loss 205814.24647347	
    Loss 205092.83286932	
    Loss 204373.81683451	
    Loss 203657.48203446	
    Loss 202943.57730363	
    Loss 202232.09931503	
    Loss 201523.03709024	
    Loss 200816.473981	
    Loss 200112.72405851	
    Loss 199411.38383309	
    Loss 198712.47569787	
    Loss 198015.81648346	
    Loss 197321.66328604	
    Loss 196629.906681	
    Loss 195940.60619709	
    Loss 195253.71350256	
    Loss 194569.2465445	
    Loss 193887.21322433	
    Loss 193207.70734942	
    Loss 192530.69137397	
    Loss 191856.03836662	
    Loss 191183.59108314	
    Loss 190513.31188725	
    Loss 189845.42600092	
    Loss 189179.94966296	
    Loss 188516.8007059	
    Loss 187856.00331499	
    Loss 187197.899357	
    Loss 186541.8096039	
    Loss 185887.93684514	
Epoch 6	
 116782
    278
     10
      0
   2672
      0
      0
   1062
  10676
    320
      8
[torch.DoubleTensor of size 11]

Validation accuracy:	0.099622177713037	
Grad norm	77.827852255893	
    Loss 185496.87581094	
    Loss 184846.82066171	
    Loss 184198.9506226	
    Loss 183553.20425157	
    Loss 182909.90529742	
    Loss 182268.84776471	
    Loss 181630.10234296	
    Loss 180993.41786818	
    Loss 180359.06280647	
    Loss 179727.05813631	
    Loss 179097.02677695	
    Loss 178469.47308384	
    Loss 177844.07537011	
    Loss 177220.89833513	
    Loss 176599.84170186	
    Loss 175981.1372204	
    Loss 175364.34460208	
    Loss 174749.68724739	
    Loss 174137.34736353	
    Loss 173526.97972156	
    Loss 172918.64524057	
    Loss 172312.53455637	
    Loss 171708.51459908	
    Loss 171106.7400136	
    Loss 170507.27082553	
    Loss 169909.90556217	
    Loss 169314.60968359	
    Loss 168721.5097457	
    Loss 168130.30203916	
    Loss 167541.12060739	
    Loss 166953.99722201	
    Loss 166368.83069003	
    Loss 165785.68143754	
    Loss 165204.73356933	
    Loss 164625.58894464	
    Loss 164048.80558771	
    Loss 163473.89914865	
    Loss 162900.96837844	
    Loss 162330.10933961	
    Loss 161761.28333182	
    Loss 161194.39104124	
    Loss 160629.43740648	
    Loss 160066.43933771	
    Loss 159505.75952618	
    Loss 158946.82039853	
    Loss 158390.0044556	
    Loss 157834.88266238	
    Loss 157281.81493671	
    Loss 156730.8285754	
    Loss 156181.80100006	
    Loss 155634.61681995	
    Loss 155089.2353324	
    Loss 154545.97039538	
    Loss 154004.34657079	
    Loss 153464.87371093	
    Loss 152926.95100208	
    Loss 152391.0626457	
    Loss 151857.1265255	
    Loss 151325.33391325	
    Loss 150794.99699808	
    Loss 150266.725164	
    Loss 149740.50391335	
    Loss 149215.79322208	
    Loss 148692.87271307	
    Loss 148171.87844812	
    Loss 147652.77623646	
    Loss 147135.63361544	
    Loss 146620.17694257	
    Loss 146106.5427882	
    Loss 145594.71583375	
    Loss 145084.79357875	
    Loss 144576.59224648	
    Loss 144070.29015721	
    Loss 143565.73187075	
    Loss 143062.97663811	
    Loss 142561.9409574	
    Loss 142062.54897573	
    Loss 141564.94467231	
    Loss 141069.01630317	
    Loss 140574.84589427	
    Loss 140082.40676167	
    Loss 139591.68381807	
    Loss 139102.67672371	
    Loss 138615.36289958	
    Loss 138129.85035563	
    Loss 137646.06439101	
    Loss 137164.08143977	
    Loss 136683.64961226	
    Loss 136204.83971785	
    Loss 135727.71095511	
    Loss 135252.24755152	
    Loss 134778.42971519	
    Loss 134306.46826201	
    Loss 133836.14565855	
    Loss 133367.4878778	
    Loss 132900.49172609	
    Loss 132434.91948478	
    Loss 131971.04683259	
    Loss 131508.83118466	
    Loss 131048.15670554	
    Loss 130589.19375339	
    Loss 130131.73813433	
    Loss 129675.88184953	
    Loss 129221.72036621	
    Loss 128769.13455421	
    Loss 128318.28471777	
    Loss 127868.98431547	
    Loss 127421.18058332	
    Loss 126974.88495949	
    Loss 126530.14324857	
    Loss 126086.98781241	
    Loss 125645.30949264	
    Loss 125205.44750414	
    Loss 124766.94391864	
    Loss 124330.06589119	
    Loss 123894.62821226	
    Loss 123460.78811755	
    Loss 123028.43619703	
    Loss 122597.51384607	
    Loss 122168.04814815	
    Loss 121740.12091251	
    Loss 121313.94047097	
    Loss 120889.18691147	
    Loss 120465.90818241	
    Loss 120043.97632999	
    Loss 119623.56110999	
    Loss 119204.58512385	
    Loss 118787.11975953	
    Loss 118371.14465302	
    Loss 117956.5928379	
    Loss 117543.48836881	
    Loss 117131.95538402	
    Loss 116721.91401962	
    Loss 116313.34275577	
    Loss 115906.0928079	
    Loss 115500.13828445	
    Loss 115095.62329256	
    Loss 114692.56750838	
    Loss 114290.94223727	
    Loss 113890.7432112	
    Loss 113492.20125012	
    Loss 113094.87292528	
    Loss 112698.86451889	
Epoch 7	
 123823
     65
      1
      0
   1019
      0
      0
    337
   6488
     74
      1
[torch.DoubleTensor of size 11]

Validation accuracy:	0.10034292303957	
Grad norm	61.721242651564	
    Loss 112462.01789445	
    Loss 112068.33434696	
    Loss 111675.97748556	
    Loss 111284.91220835	
    Loss 110895.31499596	
    Loss 110507.04489427	
    Loss 110120.20381011	
    Loss 109734.61244118	
    Loss 109350.44530016	
    Loss 108967.69342093	
    Loss 108586.10684589	
    Loss 108206.02934266	
    Loss 107827.23935218	
    Loss 107449.80204354	
    Loss 107073.65382685	
    Loss 106698.96359295	
    Loss 106325.42050714	
    Loss 105953.15676669	
    Loss 105582.31500507	
    Loss 105212.60873212	
    Loss 104844.18035137	
    Loss 104477.10521165	
    Loss 104111.27367843	
    Loss 103746.82114399	
    Loss 103383.75497544	
    Loss 103021.96116961	
    Loss 102661.40769829	
    Loss 102302.22716193	
    Loss 101944.17442228	
    Loss 101587.32784808	
    Loss 101231.76628459	
    Loss 100877.34924881	
    Loss 100524.14927598	
    Loss 100172.30088781	
    Loss 99821.516003004	
    Loss 99472.193783964	
    Loss 99123.995263872	
    Loss 98776.955134552	
    Loss 98431.223606996	
    Loss 98086.701609662	
    Loss 97743.348356594	
    Loss 97401.186400624	
    Loss 97060.143075458	
    Loss 96720.571782781	
    Loss 96382.070621744	
    Loss 96044.847415935	
    Loss 95708.619306459	
    Loss 95373.640887853	
    Loss 95039.948247384	
    Loss 94707.414195545	
    Loss 94375.992140938	
    Loss 94045.686683545	
    Loss 93716.631221555	
    Loss 93388.586270571	
    Loss 93061.881497187	
    Loss 92736.104167767	
    Loss 92411.51531634	
    Loss 92088.141965637	
    Loss 91766.105340621	
    Loss 91444.905972905	
    Loss 91124.943498118	
    Loss 90806.276698508	
    Loss 90488.478207682	
    Loss 90171.710678505	
    Loss 89856.187879924	
    Loss 89541.795822713	
    Loss 89228.635524221	
    Loss 88916.464913845	
    Loss 88605.399554321	
    Loss 88295.430494441	
    Loss 87986.59886222	
    Loss 87678.794150867	
    Loss 87372.156098029	
    Loss 87066.576494872	
    Loss 86762.084532743	
    Loss 86458.66829681	
    Loss 86156.204532845	
    Loss 85854.821207101	
    Loss 85554.465867556	
    Loss 85255.186557783	
    Loss 84956.928142995	
    Loss 84659.734289459	
    Loss 84363.537649024	
    Loss 84068.379251928	
    Loss 83774.344221211	
    Loss 83481.305100867	
    Loss 83189.371679085	
    Loss 82898.393401706	
    Loss 82608.384942142	
    Loss 82319.41096587	
    Loss 82031.471742033	
    Loss 81744.46754246	
    Loss 81458.640830321	
    Loss 81173.803687589	
    Loss 80889.942443537	
    Loss 80607.105572227	
    Loss 80325.130324151	
    Loss 80044.13693087	
    Loss 79764.19549699	
    Loss 79485.182505269	
    Loss 79207.218788827	
    Loss 78930.143578093	
    Loss 78654.052072781	
    Loss 78378.980020068	
    Loss 78104.900160528	
    Loss 77831.838634588	
    Loss 77559.730063672	
    Loss 77288.526144487	
    Loss 77018.217218053	
    Loss 76748.784543944	
    Loss 76480.382602463	
    Loss 76212.836200079	
    Loss 75946.451243611	
    Loss 75680.885599557	
    Loss 75416.306716505	
    Loss 75152.596218192	
    Loss 74889.818718041	
    Loss 74627.961546849	
    Loss 74366.933376805	
    Loss 74106.784344448	
    Loss 73847.591000407	
    Loss 73589.490065034	
    Loss 73332.228807863	
    Loss 73075.858127272	
    Loss 72820.294734699	
    Loss 72565.653815556	
    Loss 72311.860753336	
    Loss 72059.005902549	
    Loss 71807.08844393	
    Loss 71555.990821225	
    Loss 71305.745363375	
    Loss 71056.487629482	
    Loss 70808.115813861	
    Loss 70560.66793073	
    Loss 70314.014187919	
    Loss 70068.129834423	
    Loss 69823.108746224	
    Loss 69578.964485029	
    Loss 69335.708017459	
    Loss 69093.320456427	
    Loss 68851.952119333	
    Loss 68611.318371076	
    Loss 68371.471130971	
Epoch 8	
 128110
      9
      0
      0
    304
      0
      0
     71
   3310
      4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10010773245933	
Grad norm	49.192926254969	
    Loss 68228.012211881	
    Loss 67989.574725707	
    Loss 67751.951151986	
    Loss 67515.116985006	
    Loss 67279.151733356	
    Loss 67043.963240436	
    Loss 66809.666044482	
    Loss 66576.134072385	
    Loss 66343.474513294	
    Loss 66111.661385763	
    Loss 65880.528541119	
    Loss 65650.313325913	
    Loss 65420.858900327	
    Loss 65192.236213211	
    Loss 64964.399550599	
    Loss 64737.471108403	
    Loss 64511.234421671	
    Loss 64285.76093775	
    Loss 64061.16635302	
    Loss 63837.201656835	
    Loss 63614.060843799	
    Loss 63391.746139355	
    Loss 63170.161496988	
    Loss 62949.427031563	
    Loss 62729.51863068	
    Loss 62510.37701144	
    Loss 62291.975769619	
    Loss 62074.445957318	
    Loss 61857.585699122	
    Loss 61641.440214601	
    Loss 61426.111225816	
    Loss 61211.434781071	
    Loss 60997.490366865	
    Loss 60784.380077116	
    Loss 60571.893430111	
    Loss 60360.313666753	
    Loss 60149.40745695	
    Loss 59939.164541198	
    Loss 59729.769030481	
    Loss 59521.081809622	
    Loss 59313.101714866	
    Loss 59105.863235424	
    Loss 58899.235687616	
    Loss 58693.559038759	
    Loss 58488.550351237	
    Loss 58284.305840664	
    Loss 58080.639139551	
    Loss 57877.733583325	
    Loss 57675.629825166	
    Loss 57474.198147456	
    Loss 57273.438489406	
    Loss 57073.379863761	
    Loss 56874.042127302	
    Loss 56675.340359315	
    Loss 56477.482164453	
    Loss 56280.185200516	
    Loss 56083.561381714	
    Loss 55887.700103863	
    Loss 55692.681946093	
    Loss 55498.140060935	
    Loss 55304.330713033	
    Loss 55111.351710784	
    Loss 54918.863799407	
    Loss 54726.94505994	
    Loss 54535.857163613	
    Loss 54345.4393356	
    Loss 54155.804147524	
    Loss 53966.743107715	
    Loss 53778.352462591	
    Loss 53590.622251767	
    Loss 53403.569853628	
    Loss 53217.124496863	
    Loss 53031.397375311	
    Loss 52846.313426373	
    Loss 52661.884388961	
    Loss 52478.145061413	
    Loss 52294.940926401	
    Loss 52112.389069968	
    Loss 51930.474101947	
    Loss 51749.223440624	
    Loss 51568.561294514	
    Loss 51388.568772709	
    Loss 51209.144345577	
    Loss 51030.358722405	
    Loss 50852.284179252	
    Loss 50674.75983418	
    Loss 50497.915417015	
    Loss 50321.671902437	
    Loss 50146.000758417	
    Loss 49970.974448772	
    Loss 49796.602366346	
    Loss 49622.735540434	
    Loss 49449.632625928	
    Loss 49277.125630429	
    Loss 49105.177096447	
    Loss 48933.864608317	
    Loss 48763.079641158	
    Loss 48592.843162842	
    Loss 48423.288377747	
    Loss 48254.29097075	
    Loss 48085.939305765	
    Loss 47918.107257093	
    Loss 47750.884636926	
    Loss 47584.270762215	
    Loss 47418.296989557	
    Loss 47252.903975752	
    Loss 47088.100357701	
    Loss 46923.848968649	
    Loss 46760.119243379	
    Loss 46596.855359118	
    Loss 46434.284404874	
    Loss 46272.196281329	
    Loss 46110.869606906	
    Loss 45950.039330614	
    Loss 45789.805747683	
    Loss 45630.098597168	
    Loss 45470.91692263	
    Loss 45312.312754418	
    Loss 45154.17683091	
    Loss 44996.572549405	
    Loss 44839.570775997	
    Loss 44683.258991343	
    Loss 44527.435382615	
    Loss 44372.14623228	
    Loss 44217.341582534	
    Loss 44063.103309215	
    Loss 43909.346604251	
    Loss 43756.181337345	
    Loss 43603.62042139	
    Loss 43451.516680584	
    Loss 43299.906424366	
    Loss 43148.926542836	
    Loss 42998.465187046	
    Loss 42848.59246108	
    Loss 42699.200942491	
    Loss 42550.261821847	
    Loss 42401.837784111	
    Loss 42253.932365637	
    Loss 42106.586768155	
    Loss 41959.775196992	
    Loss 41813.589142618	
    Loss 41667.851969091	
    Loss 41522.584938252	
Epoch 9	
 130251
      0
      0
      0
     50
      0
      0
     10
   1497
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099963583394028	
Grad norm	39.448998708657	
    Loss 41435.684282763	
    Loss 41291.264377354	
    Loss 41147.352867705	
    Loss 41003.927076479	
    Loss 40861.003537863	
    Loss 40718.525876013	
    Loss 40576.612277926	
    Loss 40435.173652803	
    Loss 40294.271185875	
    Loss 40153.869467596	
    Loss 40013.85811573	
    Loss 39874.401992024	
    Loss 39735.387720716	
    Loss 39596.892379587	
    Loss 39458.878707032	
    Loss 39321.433708365	
    Loss 39184.410111195	
    Loss 39047.839573306	
    Loss 38911.818325409	
    Loss 38776.121426979	
    Loss 38640.975509765	
    Loss 38506.336452853	
    Loss 38372.116007829	
    Loss 38238.422524922	
    Loss 38105.213184001	
    Loss 37972.46293253	
    Loss 37840.151715296	
    Loss 37708.406233163	
    Loss 37577.056310871	
    Loss 37446.126686337	
    Loss 37315.729447071	
    Loss 37185.687103021	
    Loss 37056.08295852	
    Loss 36926.996878347	
    Loss 36798.274050545	
    Loss 36670.112574463	
    Loss 36542.357114879	
    Loss 36414.964912673	
    Loss 36288.141695344	
    Loss 36161.720725501	
    Loss 36035.728225414	
    Loss 35910.207053982	
    Loss 35784.989563533	
    Loss 35660.400557074	
    Loss 35536.238665331	
    Loss 35412.528256927	
    Loss 35289.147666977	
    Loss 35166.231602808	
    Loss 35043.822404594	
    Loss 34921.78932861	
    Loss 34800.164321517	
    Loss 34678.987861373	
    Loss 34558.209272823	
    Loss 34437.844772167	
    Loss 34318.018889084	
    Loss 34198.539742351	
    Loss 34079.419728372	
    Loss 33960.783232353	
    Loss 33842.686762883	
    Loss 33724.858211008	
    Loss 33607.45464757	
    Loss 33490.593632836	
    Loss 33374.002968898	
    Loss 33257.701095163	
    Loss 33141.978358382	
    Loss 33026.646744243	
    Loss 32911.819503234	
    Loss 32797.319628442	
    Loss 32683.226411857	
    Loss 32569.52478242	
    Loss 32456.227522125	
    Loss 32343.283260428	
    Loss 32230.782110403	
    Loss 32118.672156026	
    Loss 32006.955223739	
    Loss 31895.697198273	
    Loss 31784.722469129	
    Loss 31674.141162551	
    Loss 31563.96016948	
    Loss 31454.195367701	
    Loss 31344.755603954	
    Loss 31235.746015319	
    Loss 31127.049112871	
    Loss 31018.746939011	
    Loss 30910.905122203	
    Loss 30803.341289392	
    Loss 30696.197162622	
    Loss 30589.443822614	
    Loss 30483.020302277	
    Loss 30377.007088136	
    Loss 30271.418161049	
    Loss 30166.074925826	
    Loss 30061.243631222	
    Loss 29956.769454279	
    Loss 29852.600380479	
    Loss 29748.830796971	
    Loss 29645.39086096	
    Loss 29542.238275461	
    Loss 29439.539869184	
    Loss 29337.17405881	
    Loss 29235.207488699	
    Loss 29133.541176074	
    Loss 29032.256578147	
    Loss 28931.331254784	
    Loss 28830.83334505	
    Loss 28730.649333772	
    Loss 28630.832960198	
    Loss 28531.359403311	
    Loss 28432.180018498	
    Loss 28333.218522524	
    Loss 28234.743842678	
    Loss 28136.529260507	
    Loss 28038.830891762	
    Loss 27941.436001495	
    Loss 27844.399591387	
    Loss 27747.685215339	
    Loss 27651.246212464	
    Loss 27555.177656408	
    Loss 27459.361196874	
    Loss 27363.867568302	
    Loss 27268.762566105	
    Loss 27174.09937937	
    Loss 27079.713702503	
    Loss 26985.645616722	
    Loss 26891.869056419	
    Loss 26798.446061957	
    Loss 26705.277024355	
    Loss 26612.489147854	
    Loss 26520.105111201	
    Loss 26427.960782583	
    Loss 26336.094098612	
    Loss 26244.638397926	
    Loss 26153.478371598	
    Loss 26062.700301058	
    Loss 25972.218797574	
    Loss 25881.99879629	
    Loss 25792.083490097	
    Loss 25702.465584334	
    Loss 25613.20951685	
    Loss 25524.286854969	
    Loss 25435.745793737	
    Loss 25347.484001791	
    Loss 25259.505400116	
Epoch 10	
 131231
      0
      0
      0
      6
      0
      0
      0
    571
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099827021121631	
Grad norm	31.873776066655	
    Loss 25206.860703457	
    Loss 25119.382123473	
    Loss 25032.230441031	
    Loss 24945.380627086	
    Loss 24858.808368699	
    Loss 24772.483371992	
    Loss 24686.523326496	
    Loss 24600.864043719	
    Loss 24515.53595235	
    Loss 24430.49865368	
    Loss 24345.679369778	
    Loss 24261.192391709	
    Loss 24176.954520522	
    Loss 24093.047625206	
    Loss 24009.438399135	
    Loss 23926.187015071	
    Loss 23843.197223142	
    Loss 23760.474914013	
    Loss 23678.1017399	
    Loss 23595.86821961	
    Loss 23514.021742576	
    Loss 23432.488783832	
    Loss 23351.185385359	
    Loss 23270.21106319	
    Loss 23189.511870196	
    Loss 23109.084353249	
    Loss 23028.914364821	
    Loss 22949.123649877	
    Loss 22869.565210083	
    Loss 22790.250667779	
    Loss 22711.296152036	
    Loss 22632.516654763	
    Loss 22553.997460767	
    Loss 22475.802653882	
    Loss 22397.817867913	
    Loss 22320.178299476	
    Loss 22242.786829487	
    Loss 22165.57676114	
    Loss 22088.767768179	
    Loss 22012.173479087	
    Loss 21935.839517939	
    Loss 21859.814661552	
    Loss 21783.907920033	
    Loss 21708.429099515	
    Loss 21633.233502299	
    Loss 21558.298808164	
    Loss 21483.547394616	
    Loss 21409.080448621	
    Loss 21334.940260081	
    Loss 21260.995981473	
    Loss 21187.301946188	
    Loss 21113.902640313	
    Loss 21040.703861395	
    Loss 20967.788260572	
    Loss 20895.223453973	
    Loss 20822.880546071	
    Loss 20750.705363143	
    Loss 20678.841301553	
    Loss 20607.33035446	
    Loss 20535.969049859	
    Loss 20464.844786352	
    Loss 20394.084346682	
    Loss 20323.46590226	
    Loss 20252.966279951	
    Loss 20182.892494394	
    Loss 20113.040797831	
    Loss 20043.520207736	
    Loss 19974.181326829	
    Loss 19905.089129083	
    Loss 19836.221600234	
    Loss 19767.597023662	
    Loss 19699.171750886	
    Loss 19631.020301439	
    Loss 19563.106945919	
    Loss 19495.428029101	
    Loss 19428.070557358	
    Loss 19360.844318307	
    Loss 19293.855226516	
    Loss 19227.123046113	
    Loss 19160.657892137	
    Loss 19094.356967565	
    Loss 19028.341052736	
    Loss 18962.486369873	
    Loss 18896.876936589	
    Loss 18831.575384435	
    Loss 18766.385289937	
    Loss 18701.45560978	
    Loss 18636.792530126	
    Loss 18572.310965398	
    Loss 18508.098835347	
    Loss 18444.171857828	
    Loss 18380.334019698	
    Loss 18316.854523492	
    Loss 18253.587147295	
    Loss 18190.470881164	
    Loss 18127.610040889	
    Loss 18064.962248226	
    Loss 18002.444753235	
    Loss 17940.241003665	
    Loss 17878.233668878	
    Loss 17816.474580313	
    Loss 17754.885711787	
    Loss 17693.539933823	
    Loss 17632.402023736	
    Loss 17571.56294282	
    Loss 17510.874897007	
    Loss 17450.419101379	
    Loss 17390.182049539	
    Loss 17330.100437694	
    Loss 17270.087230811	
    Loss 17210.434625254	
    Loss 17150.909586564	
    Loss 17091.749763446	
    Loss 17032.778585279	
    Loss 16974.020327564	
