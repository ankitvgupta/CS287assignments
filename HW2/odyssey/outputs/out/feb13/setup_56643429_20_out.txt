[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	10	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1148
 2561
 2213
  963
 6323
 5419
 4358
 3268
 1544
 3549
 1066
 1665
 1107
 5216
  941
 4810
 6144
 1436
 4639
 4447
 3628
 3139
 1777
 1902
 2506
 3655
 2892
 1609
 1110
 4013
 1971
 3369
  881
 1625
 4711
 1119
 8087
 2783
 4377
 2334
 2829
 1797
 2514
 1329
 3034
[torch.DoubleTensor of size 45]

Validation accuracy:	0.023799769361496	
Grad norm	0	
    Loss 2279312.0654901	
    Loss 1720309.3696416	
    Loss 1298540.4693598	
    Loss 980228.53631885	
    Loss 739984.92484164	
    Loss 558648.78531379	
    Loss 421786.16635499	
    Loss 318475.27741679	
    Loss 240490.91211286	
    Loss 181620.94178994	
    Loss 137187.32387059	
    Loss 103641.53648527	
    Loss 78320.941237302	
    Loss 59204.745545642	
    Loss 44779.364433006	
    Loss 33885.410560991	
    Loss 25664.846774227	
    Loss 19458.148422583	
    Loss 14770.566196879	
    Loss 11234.848456362	
    Loss 8564.6740570081	
    Loss 6544.7278897027	
    Loss 5024.8378305732	
    Loss 3873.7942904408	
    Loss 3007.1047139103	
    Loss 2350.555952036	
    Loss 1852.2340059893	
    Loss 1483.224365086	
    Loss 1200.4872996309	
    Loss 988.99220809817	
    Loss 825.78219534089	
    Loss 704.49983214058	
    Loss 616.30630535433	
    Loss 545.79510686084	
    Loss 491.37314155208	
    Loss 453.49185332281	
    Loss 421.72069733256	
    Loss 397.69219251258	
    Loss 381.51165799985	
    Loss 369.89195511878	
    Loss 359.28571929961	
    Loss 351.71007447781	
    Loss 350.65493959786	
    Loss 342.28390509179	
    Loss 338.8314304366	
    Loss 339.94799202304	
    Loss 336.75960621899	
    Loss 330.34296420243	
    Loss 333.30016170693	
    Loss 331.36131276713	
    Loss 331.70793675377	
    Loss 332.68259478046	
    Loss 336.23039969063	
    Loss 335.8176974885	
    Loss 335.14236200311	
    Loss 332.1176547476	
    Loss 329.26966255055	
    Loss 334.62669162334	
    Loss 333.5916676928	
    Loss 333.20649865616	
    Loss 332.67354853857	
    Loss 331.78215790067	
    Loss 331.99302760963	
    Loss 332.42762813838	
    Loss 333.66720709741	
    Loss 332.86454553338	
    Loss 326.66603620979	
    Loss 328.15563129861	
    Loss 329.20766440335	
    Loss 331.51263108027	
    Loss 330.69121949147	
    Loss 333.22227417287	
Epoch 2	
 76984
  5935
     0
 17418
  8579
     0
     0
  2023
 15608
  5261
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082779497450838	
Grad norm	6.5470180607276	
    Loss 328.42417040015	
    Loss 331.54213264226	
    Loss 334.26068558491	
    Loss 330.50014325625	
    Loss 332.91254993829	
    Loss 332.52216107804	
    Loss 329.04306616308	
    Loss 328.03226798223	
    Loss 329.97781482606	
    Loss 330.79366083205	
    Loss 332.53639683805	
    Loss 333.51527843197	
    Loss 332.96787960502	
    Loss 329.71143882734	
    Loss 331.65055600197	
    Loss 331.27166601741	
    Loss 332.95024770396	
    Loss 333.8471284285	
    Loss 333.76394685878	
    Loss 334.82051301982	
    Loss 334.74924693154	
    Loss 332.39583924951	
    Loss 334.84288107024	
    Loss 333.34829278423	
    Loss 333.80101904783	
    Loss 331.82615127266	
    Loss 328.00692142727	
    Loss 332.56568189067	
    Loss 331.93707542843	
    Loss 333.00369060031	
    Loss 330.34601794624	
    Loss 330.42831691983	
    Loss 333.80296313627	
    Loss 332.45981973015	
    Loss 330.3547910908	
    Loss 332.02730432589	
    Loss 330.01821089314	
    Loss 328.43945406531	
    Loss 329.25438871679	
    Loss 330.46636136894	
    Loss 329.48987215146	
    Loss 329.21449252911	
    Loss 333.64508024756	
    Loss 329.43945721907	
    Loss 329.14477557322	
    Loss 332.62723459222	
    Loss 331.22942602865	
    Loss 326.17060174835	
    Loss 330.13911328834	
    Loss 328.97606949466	
    Loss 329.91440996485	
    Loss 331.33922745301	
    Loss 335.21682423812	
    Loss 335.06002929884	
    Loss 334.57285712323	
    Loss 331.68958377273	
    Loss 328.94608662861	
    Loss 334.38406321232	
    Loss 333.41031444215	
    Loss 333.07056928046	
    Loss 332.5734806391	
    Loss 331.70908986275	
    Loss 331.93953282852	
    Loss 332.38801434932	
    Loss 333.64048280196	
    Loss 332.84631214885	
    Loss 326.65438319916	
    Loss 328.14837825812	
    Loss 329.2040728356	
    Loss 331.51199083852	
    Loss 330.69272461432	
    Loss 333.22584065261	
Epoch 3	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.5347224888989	
    Loss 328.42801879313	
    Loss 331.54480597348	
    Loss 334.26258011105	
    Loss 330.50176217683	
    Loss 332.91365740868	
    Loss 332.52308035153	
    Loss 329.04354882701	
    Loss 328.03273577084	
    Loss 329.97811630664	
    Loss 330.79396976214	
    Loss 332.53652471249	
    Loss 333.51543281079	
    Loss 332.96799786221	
    Loss 329.71155823359	
    Loss 331.65062367319	
    Loss 331.2717173088	
    Loss 332.95023782526	
    Loss 333.84710886649	
    Loss 333.76395673588	
    Loss 334.82049049722	
    Loss 334.74919040755	
    Loss 332.39582513813	
    Loss 334.84287554511	
    Loss 333.34829609723	
    Loss 333.80100477411	
    Loss 331.82612911048	
    Loss 328.00689887493	
    Loss 332.56566936369	
    Loss 331.93707484877	
    Loss 333.00368354693	
    Loss 330.34600594232	
    Loss 330.42830772287	
    Loss 333.80295261319	
    Loss 332.45981011339	
    Loss 330.35478495425	
    Loss 332.02730236936	
    Loss 330.01820990519	
    Loss 328.43945254008	
    Loss 329.25438889601	
    Loss 330.46636258349	
    Loss 329.48987172063	
    Loss 329.21449187992	
    Loss 333.64507889143	
    Loss 329.43945621443	
    Loss 329.14477531707	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163615	
    Loss 330.13911282262	
    Loss 328.97606914544	
    Loss 329.91440996582	
    Loss 331.33922795121	
    Loss 335.21682466155	
    Loss 335.06002991825	
    Loss 334.57285770565	
    Loss 331.68958427646	
    Loss 328.94608702056	
    Loss 334.38406354643	
    Loss 333.41031475144	
    Loss 333.07056954784	
    Loss 332.57348095824	
    Loss 331.70909019139	
    Loss 331.93953311672	
    Loss 332.38801459989	
    Loss 333.64048310414	
    Loss 332.84631243697	
    Loss 326.65438348657	
    Loss 328.14837852915	
    Loss 329.20407310823	
    Loss 331.51199111929	
    Loss 330.69272489174	
    Loss 333.22584094512	
Epoch 4	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.5347224644815	
    Loss 328.42801908256	
    Loss 331.54480618418	
    Loss 334.26258026549	
    Loss 330.50176230057	
    Loss 332.91365749859	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310415	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 5	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 6	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 7	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886652	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 8	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 9	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 10	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886652	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 11	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 12	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886652	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 13	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 14	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 15	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 16	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 17	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 18	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 19	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Epoch 20	
 76982
  5936
     0
 17418
  8580
     0
     0
  2023
 15607
  5262
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082764323865016	
Grad norm	6.534722464482	
    Loss 328.42801908257	
    Loss 331.54480618419	
    Loss 334.2625802655	
    Loss 330.50176230057	
    Loss 332.9136574986	
    Loss 332.52308042296	
    Loss 329.04354887254	
    Loss 328.03273580896	
    Loss 329.97811633278	
    Loss 330.79396978513	
    Loss 332.53652472503	
    Loss 333.5154328225	
    Loss 332.96799787108	
    Loss 329.71155824147	
    Loss 331.65062367814	
    Loss 331.27171731214	
    Loss 332.95023782576	
    Loss 333.84710886651	
    Loss 333.76395673685	
    Loss 334.82049049676	
    Loss 334.74919040578	
    Loss 332.39582513782	
    Loss 334.84287554505	
    Loss 333.34829609744	
    Loss 333.80100477355	
    Loss 331.82612910968	
    Loss 328.00689887412	
    Loss 332.56566936324	
    Loss 331.93707484882	
    Loss 333.00368354675	
    Loss 330.34600594191	
    Loss 330.42830772257	
    Loss 333.80295261279	
    Loss 332.45981011302	
    Loss 330.35478495403	
    Loss 332.02730236927	
    Loss 330.01820990515	
    Loss 328.43945254001	
    Loss 329.25438889602	
    Loss 330.46636258354	
    Loss 329.48987172063	
    Loss 329.21449187989	
    Loss 333.64507889136	
    Loss 329.43945621439	
    Loss 329.14477531705	
    Loss 332.62723424009	
    Loss 331.22942569659	
    Loss 326.17060163614	
    Loss 330.13911282261	
    Loss 328.97606914543	
    Loss 329.91440996582	
    Loss 331.33922795122	
    Loss 335.21682466156	
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Done	
