[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	50	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1047
 2885
 1155
 1823
 3644
 3396
 5314
 3388
 3145
  952
 5689
 2559
 4113
 1949
 1943
 1972
 3154
 3046
 7599
  861
 4180
  940
 3075
 1257
 2781
 1235
 1980
 1982
 3759
 6323
 6312
 1915
 1334
 2144
 1860
 1914
 1021
 1335
 4940
  877
 1848
 4638
 1711
 6004
 6809
[torch.DoubleTensor of size 45]

Validation accuracy:	0.014983915999029	
Grad norm	0	
    Loss 2275738.3666726	
    Loss 557578.97915925	
    Loss 137804.47119298	
    Loss 35070.222914895	
    Loss 9916.0568761642	
    Loss 3894.2547939587	
    Loss 2233.0982139892	
    Loss 1930.4111169614	
    Loss 2335.3981821723	
    Loss 1919.3285685001	
    Loss 1854.0094230943	
    Loss 2463.6157065065	
    Loss 1896.5460351268	
    Loss 1810.3284226032	
    Loss 2027.0434017991	
    Loss 1896.2819775334	
    Loss 1732.5950967703	
    Loss 2307.7883283707	
    Loss 2322.0985587047	
    Loss 1826.5013095034	
    Loss 1985.2798954699	
    Loss 1976.9162588146	
    Loss 2179.7717099413	
    Loss 1805.9083388738	
    Loss 1888.5894159838	
    Loss 2378.8340610634	
    Loss 1888.7808954594	
    Loss 1970.8559318244	
    Loss 1846.808098294	
    Loss 1850.2642508588	
    Loss 1857.3121444164	
    Loss 1672.2773373774	
    Loss 1899.7965855268	
    Loss 1960.8065619597	
    Loss 2209.1414761658	
    Loss 1714.7001945861	
    Loss 1781.2306336301	
    Loss 1750.6600007057	
    Loss 2421.5608818377	
    Loss 2035.5976787374	
    Loss 1747.5785175303	
    Loss 2230.2088196829	
    Loss 2062.3685371142	
    Loss 1890.247727016	
    Loss 1860.3266691252	
    Loss 2138.0553152509	
    Loss 1929.6415238012	
    Loss 2678.8485607675	
    Loss 1917.8696384671	
    Loss 2080.7013388797	
    Loss 2132.0907873729	
    Loss 2223.4228726293	
    Loss 2040.3635653026	
    Loss 2018.7598289495	
    Loss 1921.1941135713	
    Loss 1943.4805163664	
    Loss 1848.8917441036	
    Loss 1784.3403066458	
    Loss 1773.8651455213	
    Loss 1941.2131579703	
    Loss 1771.7175340056	
    Loss 1761.9525853474	
    Loss 2342.2192389061	
    Loss 2013.0750835951	
    Loss 2085.7789533664	
    Loss 2823.8661183634	
    Loss 2011.9472874617	
    Loss 1817.9636556996	
    Loss 2060.708737372	
    Loss 2423.6437000162	
    Loss 1895.1409447917	
    Loss 1935.5902708848	
Epoch 2	
     0
     0
     0
     3
     0
     0
    33
 41403
 90255
     0
   102
     0
     0
     0
     0
     0
    11
     0
     1
[torch.DoubleTensor of size 19]

Validation accuracy:	0.11974235251275	
Grad norm	8.8849271545278	
    Loss 1865.0847042465	
    Loss 2101.3644340594	
    Loss 2279.2072514435	
    Loss 1854.6435152735	
    Loss 2313.7202208946	
    Loss 1847.6894708064	
    Loss 2346.6121125926	
    Loss 1760.474974229	
    Loss 2009.6802248468	
    Loss 1945.0248132131	
    Loss 1841.6871721066	
    Loss 1877.8221221693	
    Loss 1870.989376433	
    Loss 2132.8212621996	
    Loss 1817.3252598642	
    Loss 2018.4933976039	
    Loss 1899.2767151331	
    Loss 1941.73791006	
    Loss 2505.077504386	
    Loss 1848.3695439691	
    Loss 1846.6272795269	
    Loss 1891.4831619343	
    Loss 2195.1300689613	
    Loss 1996.8981167404	
    Loss 2037.4326568373	
    Loss 2185.8891433586	
    Loss 2227.8350712828	
    Loss 1859.4015236555	
    Loss 1783.1473785836	
    Loss 2053.8161047187	
    Loss 1778.1600650852	
    Loss 1786.3973529074	
    Loss 1857.4347727001	
    Loss 2176.1704840022	
    Loss 1866.7812248384	
    Loss 1889.3888651465	
    Loss 1748.6563819305	
    Loss 2059.8648055724	
    Loss 1914.084663602	
    Loss 1944.7864281093	
    Loss 1861.5591567882	
    Loss 2006.2862335887	
    Loss 2031.8246427887	
    Loss 1801.3691124681	
    Loss 2112.3952938909	
    Loss 2092.1312851044	
    Loss 1878.4883239533	
    Loss 2050.2722206578	
    Loss 1840.215199882	
    Loss 2029.6364215517	
    Loss 1939.3247489527	
    Loss 1956.7201651051	
    Loss 2030.8459799123	
    Loss 1985.9453776949	
    Loss 1895.325543199	
    Loss 2265.2858240735	
    Loss 1859.4962485191	
    Loss 1860.7874218356	
    Loss 2244.0948358041	
    Loss 1998.3986865644	
    Loss 2224.1097262997	
    Loss 1877.3407742534	
    Loss 2406.540451037	
    Loss 1800.1033814703	
    Loss 1932.5267798694	
    Loss 2236.4915803615	
    Loss 1872.0202995933	
    Loss 1689.4915519513	
    Loss 1844.8944378675	
    Loss 1848.3525767832	
    Loss 2046.179722692	
    Loss 1854.6102003525	
Epoch 3	
     0
  6955
 13179
     0
     0
     0
     1
     0
     0
     0
 58082
 16835
     0
     0
  2103
 14560
  2738
 14068
  3287
[torch.DoubleTensor of size 19]

Validation accuracy:	0.015636380189366	
Grad norm	9.471429611005	
    Loss 2104.1472400134	
    Loss 1941.7112624998	
    Loss 2339.0760208529	
    Loss 1794.5120995058	
    Loss 1808.5489632593	
    Loss 1771.6169267413	
    Loss 1910.1512785821	
    Loss 1836.7449365467	
    Loss 2153.0622656738	
    Loss 2003.1203949219	
    Loss 1920.2140087742	
    Loss 1928.9581366791	
    Loss 1759.4232203611	
    Loss 2041.505320344	
    Loss 2061.4535998775	
    Loss 1848.6843003071	
    Loss 1826.4724416803	
    Loss 1984.3327470378	
    Loss 2058.7462503057	
    Loss 1896.8667303298	
    Loss 2182.2216765952	
    Loss 2138.5710696498	
    Loss 2171.7954566015	
    Loss 1915.8624558743	
    Loss 2034.21749051	
    Loss 2467.6702854361	
    Loss 1736.5538720333	
    Loss 2103.0886312034	
    Loss 1828.5628553528	
    Loss 1707.5788175727	
    Loss 1969.7084592219	
    Loss 2058.2993007411	
    Loss 1734.1713741864	
    Loss 1987.1029913427	
    Loss 1879.2003918117	
    Loss 2106.7282323558	
    Loss 1837.4222677061	
    Loss 2031.7165012831	
    Loss 2152.4188372747	
    Loss 1728.1893191401	
    Loss 1877.5116669409	
    Loss 1917.3379609902	
    Loss 1910.6170296609	
    Loss 2136.0064659881	
    Loss 1777.3957253425	
    Loss 1808.2629229477	
    Loss 2088.6038035736	
    Loss 2301.9846621462	
    Loss 2299.2052606457	
    Loss 1951.4146503221	
    Loss 1890.0646392096	
    Loss 2101.0266697696	
    Loss 2208.6295439696	
    Loss 1857.5904751351	
    Loss 2020.1467123476	
    Loss 2006.5988348675	
    Loss 1915.2816281184	
    Loss 1788.428069483	
    Loss 2224.0897792765	
    Loss 1685.2626810719	
    Loss 1873.0996836845	
    Loss 1976.6152237461	
    Loss 1662.4065805176	
    Loss 1914.9685476528	
    Loss 1884.1793295018	
    Loss 2328.1513191476	
    Loss 2005.9139874092	
    Loss 1811.5564331664	
    Loss 1886.9795636362	
    Loss 2260.1966087529	
    Loss 1829.5977722413	
    Loss 1949.8941232882	
Epoch 4	
     0
 67351
     0
 26285
     0
     0
     0
 38145
     0
     0
    27
[torch.DoubleTensor of size 11]

Validation accuracy:	0.069722626851177	
Grad norm	9.0985750810704	
    Loss 2026.2734447688	
    Loss 1698.4782617137	
    Loss 1882.396623062	
    Loss 1857.5846747277	
    Loss 1864.6304183668	
    Loss 1750.5387513076	
    Loss 2009.3773742071	
    Loss 1858.6354888231	
    Loss 1877.2341595572	
    Loss 1828.3086548655	
    Loss 2127.5183380097	
    Loss 1971.3351009275	
    Loss 1818.861753782	
    Loss 1794.1374832965	
    Loss 2176.7452968844	
    Loss 2040.7577933879	
    Loss 1855.5858084362	
    Loss 1893.4084640448	
    Loss 2255.0795781355	
    Loss 1801.0147766691	
    Loss 1720.6775210227	
    Loss 1655.4533030933	
    Loss 2334.3422628743	
    Loss 1870.1720404088	
    Loss 2127.1677263069	
    Loss 1767.647153928	
    Loss 2393.9137828587	
    Loss 1974.7545845348	
    Loss 1770.7890550963	
    Loss 1718.9203367932	
    Loss 1706.6691462868	
    Loss 2059.5312778887	
    Loss 2057.4548646226	
    Loss 2028.0533738845	
    Loss 1768.3571990277	
    Loss 2040.6096371085	
    Loss 2094.7226276717	
    Loss 1688.801105646	
    Loss 1910.412483855	
    Loss 2016.3262857654	
    Loss 2057.4008890232	
    Loss 2417.4979570601	
    Loss 1921.7422721367	
    Loss 1677.5706743008	
    Loss 1782.5812189531	
    Loss 1834.3129246134	
    Loss 2187.5441945167	
    Loss 2016.5999423035	
    Loss 1862.2972142666	
    Loss 1939.9935379432	
    Loss 2080.3022470394	
    Loss 1975.9302396809	
    Loss 2053.3853281634	
    Loss 2435.5415157349	
    Loss 1839.9969331273	
    Loss 1866.456839738	
    Loss 1838.3356497544	
    Loss 1778.1873343581	
    Loss 2776.9990510323	
    Loss 1815.8028216462	
    Loss 1749.7435884724	
    Loss 1695.0921406688	
    Loss 2058.7669827348	
    Loss 2518.9667582933	
    Loss 2011.1243559027	
    Loss 2138.7024013395	
    Loss 1796.1439313163	
    Loss 1948.4599548271	
    Loss 1785.2652819009	
    Loss 1909.5361839299	
    Loss 2162.314697464	
    Loss 2344.9686989684	
Epoch 5	
     0
 21486
     0
 11593
 96195
     0
     0
     0
     0
     0
   501
     0
     0
     0
     2
   321
    57
  1648
     5
[torch.DoubleTensor of size 19]

Validation accuracy:	0.059434935663996	
Grad norm	9.1769314336705	
    Loss 1953.8924388181	
    Loss 2099.9387444347	
    Loss 1811.3665779784	
    Loss 2230.7928885905	
    Loss 2004.6090537892	
    Loss 1937.6615975881	
    Loss 2285.1941740854	
    Loss 1746.0386250686	
    Loss 2102.4717980092	
    Loss 2165.995404709	
    Loss 2150.7253057446	
    Loss 2239.6030658343	
    Loss 1796.8708911841	
    Loss 2038.5585573918	
    Loss 2082.0003005523	
    Loss 2115.0012029476	
    Loss 1845.310997189	
    Loss 1843.8960069782	
    Loss 1821.5721712384	
    Loss 1753.4346987225	
    Loss 1863.0015607372	
    Loss 1957.5666153735	
    Loss 1832.8215154088	
    Loss 1874.2635765849	
    Loss 1840.1829426785	
    Loss 1788.9065783562	
    Loss 2476.204534496	
    Loss 1691.1517374017	
    Loss 1971.2426575243	
    Loss 2035.6720285183	
    Loss 1909.546694748	
    Loss 2181.0169885085	
    Loss 2159.9860868244	
    Loss 1899.3302736757	
    Loss 2349.3652376915	
    Loss 1963.0076551779	
    Loss 1758.2638933883	
    Loss 1961.1882839478	
    Loss 2012.2791751167	
    Loss 1801.7601313354	
    Loss 2116.6740837641	
    Loss 2191.5383047669	
    Loss 1905.4837316289	
    Loss 2403.9846438469	
    Loss 1776.6938072514	
    Loss 2061.0390201207	
    Loss 1763.7254299193	
    Loss 2149.4772150351	
    Loss 2208.0102017138	
    Loss 2068.5340253911	
    Loss 2439.7241773875	
    Loss 1741.3787102884	
    Loss 2050.0709837649	
    Loss 1800.6544861439	
    Loss 1920.1800859288	
    Loss 1923.848658216	
    Loss 1943.4554978215	
    Loss 1928.2233080172	
    Loss 2407.8017787933	
    Loss 2029.3769526918	
    Loss 1939.0185739748	
    Loss 1807.2330453258	
    Loss 1786.9855457196	
    Loss 1930.5976322712	
    Loss 2100.8759833946	
    Loss 2310.8747146982	
    Loss 1698.4351343441	
    Loss 2140.4051105678	
    Loss 1946.5444513471	
    Loss 1803.7026489	
    Loss 2022.0585681423	
    Loss 1952.5398885613	
Epoch 6	
 18010
  2246
     2
   170
     0
     0
     0
 95661
     0
 15582
     1
    26
    47
     0
     0
    29
    32
     2
[torch.DoubleTensor of size 18]

Validation accuracy:	0.07765841223598	
Grad norm	9.0126268906043	
    Loss 1794.36298784	
    Loss 1951.8053410248	
    Loss 1832.6598509258	
    Loss 2149.51537646	
    Loss 1853.9960096273	
    Loss 1809.9979936193	
    Loss 2286.7175700814	
    Loss 2036.5649656213	
    Loss 2054.5028450491	
    Loss 1847.4339375689	
    Loss 2064.783728534	
    Loss 1941.2157012807	
    Loss 2250.7482629921	
    Loss 1725.9099805083	
    Loss 1927.287164996	
    Loss 1781.8397610942	
    Loss 2155.4481066208	
    Loss 1928.59255178	
    Loss 1952.5868358837	
    Loss 1890.8077320686	
    Loss 1908.7769075861	
    Loss 1887.7726553215	
    Loss 2042.174791465	
    Loss 1837.9556160939	
    Loss 2332.2191681042	
    Loss 2229.1073162259	
    Loss 2016.2410524289	
    Loss 2254.6481665636	
    Loss 1944.3988421861	
    Loss 1664.1755813407	
    Loss 1962.5586814404	
    Loss 1944.2559946084	
    Loss 1999.4588686343	
    Loss 2146.7881715792	
    Loss 2305.4461242619	
    Loss 1926.0803335328	
    Loss 2067.2840242025	
    Loss 1895.2012637537	
    Loss 2190.4853548726	
    Loss 1914.7355290657	
    Loss 2548.1213874813	
    Loss 2865.9648147393	
    Loss 2380.1144752571	
    Loss 1967.9230570211	
    Loss 1756.2880174215	
    Loss 1837.4826427257	
    Loss 1880.8417775562	
    Loss 2187.1437627489	
    Loss 2087.3406475884	
    Loss 1898.2846216239	
    Loss 2031.0515355371	
    Loss 2053.8333210538	
    Loss 2441.514896167	
    Loss 1857.6117571753	
    Loss 2087.3212436059	
    Loss 2118.8252525292	
    Loss 1947.0003327543	
    Loss 1885.2452695731	
    Loss 2097.0695800055	
    Loss 1879.7501378414	
    Loss 1770.5272259926	
    Loss 1893.2009419049	
    Loss 2120.3383750033	
    Loss 2269.544604721	
    Loss 1846.7145192688	
    Loss 2244.8166513026	
    Loss 1987.344236741	
    Loss 1737.5481447425	
    Loss 2305.8768307931	
    Loss 1876.0615980025	
    Loss 2262.6573242366	
    Loss 1979.4347420538	
Epoch 7	
     0
 97449
  9870
   276
     0
     0
     0
 11138
  5031
     0
  3741
  3655
     0
     0
   119
    19
   510
[torch.DoubleTensor of size 17]

Validation accuracy:	0.053024095654285	
Grad norm	9.2564379969959	
    Loss 1989.0148520004	
    Loss 2248.6939025303	
    Loss 1728.8411980402	
    Loss 2123.7766920705	
    Loss 1862.2907199232	
    Loss 1835.1586235312	
    Loss 1899.5781562609	
    Loss 2212.4151642487	
    Loss 2176.179579065	
    Loss 2430.3717466309	
    Loss 1955.6351467109	
    Loss 2150.8486878864	
    Loss 2215.8179040521	
    Loss 1739.897523273	
    Loss 1811.9137395833	
    Loss 2080.8481942711	
    Loss 1769.4822755643	
    Loss 1679.8784242038	
    Loss 2098.5003451712	
    Loss 2092.1932150263	
    Loss 2014.9473536948	
    Loss 1870.7596714683	
    Loss 2234.8433312669	
    Loss 2004.3642912413	
    Loss 1979.8821655706	
    Loss 1954.8416327099	
    Loss 1972.0601375656	
    Loss 1791.1278005807	
    Loss 1915.4588133313	
    Loss 2032.6934121548	
    Loss 1934.6841488349	
    Loss 2124.30272154	
    Loss 2337.4975151845	
    Loss 1820.4476844839	
    Loss 1772.0365967151	
    Loss 1893.7628649587	
    Loss 1806.8752387214	
    Loss 1962.0141076748	
    Loss 2199.5537684881	
    Loss 2040.7249002652	
    Loss 1761.3101069118	
    Loss 2196.9489247412	
    Loss 2012.449642383	
    Loss 1754.769219936	
    Loss 2043.3359963345	
    Loss 2141.1727335628	
    Loss 2199.3318552012	
    Loss 1917.2958799418	
    Loss 2010.1632550113	
    Loss 2241.0230774967	
    Loss 1883.8690487402	
    Loss 1913.9714481875	
    Loss 2113.825423357	
    Loss 1849.1724503172	
    Loss 1830.793989423	
    Loss 1936.5833441726	
    Loss 2223.4079657866	
    Loss 1937.2081664963	
    Loss 2222.2903598226	
    Loss 1762.4049108499	
    Loss 1756.367108988	
    Loss 2231.9095990912	
    Loss 1847.208002698	
    Loss 2251.2842847255	
    Loss 2355.0857462256	
    Loss 2167.7574468253	
    Loss 1798.3340008596	
    Loss 1995.9168549544	
    Loss 1666.273653082	
    Loss 1747.8815761522	
    Loss 2215.7101444453	
    Loss 1773.736831461	
Epoch 8	
    17
 10932
     0
     0
 63074
     0
     0
     0
     0
 57782
     1
     0
     0
     0
     0
     0
     0
     2
[torch.DoubleTensor of size 18]

Validation accuracy:	0.095092862345229	
Grad norm	9.3617020413835	
    Loss 1799.8625925027	
    Loss 2548.8197970571	
    Loss 1953.1847475823	
    Loss 1969.0442856482	
    Loss 1768.021637653	
    Loss 2196.6621805893	
    Loss 1778.7971218533	
    Loss 2333.8277018116	
    Loss 2004.8183510347	
    Loss 1959.7142119704	
    Loss 2013.6995423138	
    Loss 2044.1041933961	
    Loss 1953.0748375336	
    Loss 1736.6281406471	
    Loss 2032.5578795748	
    Loss 1961.7483688636	
    Loss 1953.8616647732	
    Loss 2039.0657238133	
    Loss 1899.3274051448	
    Loss 2186.7489295091	
    Loss 1765.4886859561	
    Loss 1840.9752311724	
    Loss 2090.1890760234	
    Loss 1690.6414320799	
    Loss 1809.7719946027	
    Loss 2080.1590411734	
    Loss 2040.9427454155	
    Loss 2086.9528470076	
    Loss 2341.9097326957	
    Loss 1862.3188497022	
    Loss 1696.0937426033	
    Loss 2340.7812789098	
    Loss 2190.6359044515	
    Loss 2405.9776373129	
    Loss 1943.7310098341	
    Loss 1873.9831937242	
    Loss 1981.191956697	
    Loss 1894.7135547229	
    Loss 1753.1142986104	
    Loss 1800.3931553461	
    Loss 1763.4822703914	
    Loss 2634.4756738242	
    Loss 1897.7656099996	
    Loss 2369.2183967218	
    Loss 1682.8402995238	
    Loss 2044.5813627323	
    Loss 2052.6659544672	
    Loss 2053.9743928331	
    Loss 1806.3836773083	
    Loss 1964.9782059498	
    Loss 1898.5861527805	
    Loss 2156.4768364788	
    Loss 2253.1430158544	
    Loss 1892.7305926809	
    Loss 1869.0323516965	
    Loss 2107.5410831135	
    Loss 1907.8825647769	
    Loss 2313.9254478964	
    Loss 2252.2591323998	
    Loss 1789.4251001106	
    Loss 1847.7885205718	
    Loss 1988.2212398178	
    Loss 2011.8339012642	
    Loss 2427.0913248407	
    Loss 2263.5001717224	
    Loss 1989.3868192407	
    Loss 1843.2437393556	
    Loss 1860.3607971316	
    Loss 1936.5803118811	
    Loss 1957.9309698835	
    Loss 2184.0808806527	
    Loss 1889.0437404367	
Epoch 9	
      0
      0
   1602
      0
      0
      0
     43
      0
 116538
      0
     47
   1516
     71
      0
    449
   2258
   2163
   6532
    476
      3
     85
      8
     17
[torch.DoubleTensor of size 23]

Validation accuracy:	0.13420277980092	
Grad norm	8.9794009257473	
    Loss 1764.5568230692	
    Loss 1809.2087836926	
    Loss 1779.0863049447	
    Loss 2021.7678590064	
    Loss 1977.0216841495	
    Loss 1992.9188532681	
    Loss 2204.9630834927	
    Loss 1838.6809166755	
    Loss 1849.6119052058	
    Loss 1894.6167664134	
    Loss 1830.3816501334	
    Loss 1817.7814047009	
    Loss 2347.3051856092	
    Loss 1877.6796713591	
    Loss 1996.0603808312	
    Loss 2199.8939418991	
    Loss 2060.1057592306	
    Loss 2075.1449794131	
    Loss 2001.3004534287	
    Loss 1971.7987261412	
    Loss 2004.3883845677	
    Loss 2349.6130322943	
    Loss 2072.169247748	
    Loss 1994.0815027111	
    Loss 1863.4673295829	
    Loss 1969.8276182022	
    Loss 1804.6672112174	
    Loss 2030.1859858404	
    Loss 1783.988240738	
    Loss 1712.8509672849	
    Loss 2279.5068056707	
    Loss 1843.6266749573	
    Loss 1961.0356787991	
    Loss 2074.0167510696	
    Loss 2100.8135891058	
    Loss 2537.9706634634	
    Loss 1899.4391787249	
    Loss 1972.7144823562	
    Loss 1817.2494884774	
    Loss 1864.5800998098	
    Loss 1746.2817539042	
    Loss 1865.2686791631	
    Loss 1779.3393221091	
    Loss 1961.4997432566	
    Loss 1902.1534456656	
    Loss 1701.9274827386	
    Loss 1825.7890817216	
    Loss 2122.6535964104	
    Loss 1942.5757035221	
    Loss 2054.5026002323	
    Loss 2411.6950169774	
    Loss 1754.2431260696	
    Loss 1899.400352697	
    Loss 2473.3688856072	
    Loss 1838.1869347391	
    Loss 2271.2783593305	
    Loss 1937.1835258903	
    Loss 1843.5457162058	
    Loss 1982.1935483203	
    Loss 1919.1875324226	
    Loss 1927.7342092018	
    Loss 1852.9249784772	
    Loss 1946.0473485128	
    Loss 2204.4623320085	
    Loss 2393.1683665209	
    Loss 2133.3044002195	
    Loss 1822.7300150463	
    Loss 1759.1858283104	
    Loss 1797.2006618652	
    Loss 2164.8863345076	
    Loss 2252.1789255921	
    Loss 2275.0846538305	
Epoch 10	
     0
 84068
   416
     0
     0
     0
     0
 30875
 12284
     0
   754
   656
     0
     0
   305
   216
    22
  2212
[torch.DoubleTensor of size 18]

Validation accuracy:	0.060216375333819	
Grad norm	9.4079913436776	
    Loss 2105.117514074	
    Loss 1975.208005393	
    Loss 1828.9019047621	
    Loss 1822.8482203183	
    Loss 1801.9772359786	
    Loss 1921.104538218	
    Loss 1709.1805349217	
    Loss 1858.8759576593	
    Loss 1930.7405252966	
    Loss 1924.1569610893	
    Loss 2103.4114728367	
    Loss 2278.2872259579	
    Loss 2129.3443009967	
    Loss 1813.8996213754	
    Loss 2093.1270697583	
    Loss 1818.4768007419	
    Loss 1730.1072307624	
    Loss 2202.3561333704	
    Loss 1900.8300872832	
    Loss 1811.4442779673	
    Loss 1922.5400209313	
    Loss 1980.532365471	
    Loss 2247.2832058694	
    Loss 1964.2719383482	
    Loss 1706.1239077814	
    Loss 1789.4291812659	
    Loss 2108.9622535019	
    Loss 1971.3855733128	
    Loss 2170.4866542469	
    Loss 1844.2940550838	
    Loss 1962.0719219317	
    Loss 2431.3413099962	
    Loss 1799.4156937894	
    Loss 1906.4260093511	
    Loss 1897.3552372885	
    Loss 2240.4379263619	
    Loss 1945.9253736857	
    Loss 2053.2335466811	
    Loss 2050.6331871001	
    Loss 1784.5784457066	
    Loss 2372.3871050533	
    Loss 1775.9648941376	
    Loss 1961.0801464944	
    Loss 2000.3184314531	
    Loss 2288.8080021137	
    Loss 1968.5789556051	
    Loss 2115.1919780968	
    Loss 1991.7789573884	
    Loss 1926.088478308	
    Loss 1852.8019675441	
    Loss 2009.5603316967	
    Loss 1907.6442627413	
    Loss 2452.6840815603	
    Loss 1963.9658702583	
    Loss 1983.3134599873	
    Loss 1857.5577678494	
    Loss 2560.6628306889	
    Loss 1798.5783070156	
    Loss 2986.269124464	
    Loss 1764.2570645698	
    Loss 2008.8058008044	
    Loss 1755.5303062601	
    Loss 2415.3237165112	
    Loss 2011.8842073434	
    Loss 1967.6535329983	
    Loss 2585.991323511	
    Loss 1716.6354000512	
    Loss 2143.6975994154	
    Loss 1968.459885133	
    Loss 1881.6906336581	
    Loss 2451.2598735214	
    Loss 1772.64435678	
Epoch 11	
 95543
 10465
     0
 25800
[torch.DoubleTensor of size 4]

Validation accuracy:	0.084175467346443	
Grad norm	9.7743403756821	
    Loss 2222.6505080916	
    Loss 1732.3842124812	
    Loss 1945.8113227195	
    Loss 2210.0418704433	
    Loss 1933.1745162725	
    Loss 1791.8686842863	
    Loss 1828.5468845272	
    Loss 1844.6561684361	
    Loss 2228.6844094882	
    Loss 1993.0875315173	
    Loss 1756.000229382	
    Loss 2528.0818962044	
    Loss 1986.1804898841	
    Loss 1771.5256584249	
    Loss 1913.797440583	
    Loss 1875.7449476703	
    Loss 1880.9496451638	
    Loss 1899.1127990904	
    Loss 2100.3752823349	
    Loss 1736.4909272919	
    Loss 2050.7582373533	
    Loss 2096.786987939	
    Loss 1888.3083260635	
    Loss 1883.9004610057	
    Loss 1847.6014415773	
    Loss 2321.0032637533	
    Loss 1952.4224628456	
    Loss 1841.1929488333	
    Loss 1855.2970620632	
    Loss 2484.0012916161	
    Loss 1913.477343092	
    Loss 2545.457850495	
    Loss 1871.5536519611	
    Loss 1825.7042249088	
    Loss 2063.7275218818	
    Loss 1777.6874563725	
    Loss 2014.7042136684	
    Loss 2116.3216524094	
    Loss 1889.1160467637	
    Loss 1944.9435105229	
    Loss 1776.4290425245	
    Loss 2177.6889364985	
    Loss 2294.2864741883	
    Loss 2035.3157849286	
    Loss 2078.243872598	
    Loss 2366.7527766352	
    Loss 1957.8893405311	
    Loss 1820.3505701013	
    Loss 1969.8640770054	
    Loss 2007.3420751758	
    Loss 2133.2635699077	
    Loss 1748.8055699218	
    Loss 2197.7872535979	
    Loss 1771.1289946702	
    Loss 1874.7238051037	
    Loss 1714.6236136402	
    Loss 1952.8655164246	
    Loss 1858.1142604824	
    Loss 1909.0370667032	
    Loss 2063.1193039909	
    Loss 2387.5325167153	
    Loss 1868.3035492129	
    Loss 1967.6978396566	
    Loss 1771.7371063465	
    Loss 2517.858871907	
    Loss 2052.9701767423	
    Loss 1798.5347710662	
    Loss 1659.6267937705	
    Loss 1774.4224606511	
    Loss 1872.1124498003	
    Loss 2053.9978685488	
    Loss 1956.338742736	
Epoch 12	
     0
     0
     0
     0
 55793
     0
     1
 55703
     0
 18672
     0
   325
     0
     0
     1
   157
    72
  1043
    41
[torch.DoubleTensor of size 19]

Validation accuracy:	0.061984098082059	
Grad norm	9.1428338979335	
    Loss 2080.8214013092	
    Loss 1812.3186067383	
    Loss 1781.8832220655	
    Loss 1957.6274324587	
    Loss 1760.4092581819	
    Loss 1639.8414460878	
    Loss 2182.7909489003	
    Loss 1919.4724368392	
    Loss 1848.6655140694	
    Loss 2255.5155105635	
    Loss 2233.7427981591	
    Loss 1981.9269004542	
    Loss 2351.9654164261	
    Loss 1724.132287885	
    Loss 1748.730373478	
    Loss 1983.1197926049	
    Loss 1964.0771883095	
    Loss 1887.9300047498	
    Loss 2098.4427607595	
    Loss 2032.9238466563	
    Loss 2192.2893792857	
    Loss 2179.9270592772	
    Loss 2159.7959575287	
    Loss 1875.1083443487	
    Loss 2128.5118786129	
    Loss 2544.409745399	
    Loss 1834.6783427826	
    Loss 1737.8998184387	
    Loss 2007.3090034083	
    Loss 1741.628028798	
    Loss 2162.1558571064	
    Loss 2127.0202184166	
    Loss 1724.2346989625	
    Loss 1846.8603609503	
    Loss 1925.9793493001	
    Loss 1786.3016827814	
    Loss 1750.3990526639	
    Loss 2075.2046400216	
    Loss 2083.2691842908	
    Loss 2363.6969024668	
    Loss 2200.8136884059	
    Loss 1883.8326936801	
    Loss 1725.2291718344	
    Loss 1934.529202719	
    Loss 1853.8707648843	
    Loss 2302.2925786372	
    Loss 2225.6946591551	
    Loss 1802.8610686572	
    Loss 2124.8315657278	
    Loss 1790.5377583531	
    Loss 2277.2252783966	
    Loss 1917.5041453682	
    Loss 1842.4188409835	
    Loss 1736.4719628738	
    Loss 2051.7286907869	
    Loss 2130.6466021304	
    Loss 2423.9826597938	
    Loss 1760.9940129671	
    Loss 3370.7227505856	
    Loss 2281.0569836943	
    Loss 1841.2878475982	
    Loss 1906.9998067875	
    Loss 1812.641254518	
    Loss 2041.4982278597	
    Loss 1782.0008183489	
    Loss 2337.2247235557	
    Loss 2324.8266165348	
    Loss 1901.3368930137	
    Loss 1908.2818995558	
    Loss 2085.807376942	
    Loss 2208.4261262405	
    Loss 1921.112210584	
Epoch 13	
     0
 92372
    16
 35524
  2980
     0
     0
   702
     0
   154
    57
     0
     0
     0
     0
     3
[torch.DoubleTensor of size 16]

Validation accuracy:	0.064009771789269	
Grad norm	10.059547979956	
    Loss 2594.3405276543	
    Loss 1769.791542076	
    Loss 1956.3993373322	
    Loss 2143.588860409	
    Loss 1792.8673987388	
    Loss 2058.0774003933	
    Loss 1803.3327136859	
    Loss 2210.4029079748	
    Loss 1945.2979750345	
    Loss 1990.3827802754	
    Loss 1870.7665809023	
    Loss 2225.2106527949	
    Loss 1878.5801687489	
    Loss 1837.9016529203	
    Loss 2210.274459038	
    Loss 2257.7678436878	
    Loss 1805.3706483128	
    Loss 2398.8509786268	
    Loss 1811.3106965103	
    Loss 1879.6992462988	
    Loss 1900.4199527746	
    Loss 2376.195718541	
    Loss 2350.7740277056	
    Loss 2096.235040095	
    Loss 1927.7773870645	
    Loss 1962.3400339504	
    Loss 1731.2347764087	
    Loss 1777.6350037941	
    Loss 1789.3853611855	
    Loss 1882.3746641175	
    Loss 1828.9011023659	
    Loss 1802.8092636396	
    Loss 1918.4107679694	
    Loss 2028.9356289714	
    Loss 2253.3514175825	
    Loss 2433.6881015357	
    Loss 2026.5855012061	
    Loss 2302.1854271188	
    Loss 1937.1509861113	
    Loss 1813.4126364549	
    Loss 1639.3861828117	
    Loss 2232.4462621724	
    Loss 2125.936134295	
    Loss 1903.8248873537	
    Loss 2117.7019425971	
    Loss 1768.5230819446	
    Loss 2175.6223486578	
    Loss 1842.913571224	
    Loss 1915.7841945813	
    Loss 1806.8891676893	
    Loss 2063.2242486724	
    Loss 1713.4007333683	
    Loss 2587.483572783	
    Loss 2030.8910113533	
    Loss 1780.8444471153	
    Loss 1734.257405829	
    Loss 1859.6577136298	
    Loss 1854.7160306537	
    Loss 1965.1386391726	
    Loss 1845.6513184895	
    Loss 1799.490118002	
    Loss 1985.8548767797	
    Loss 2340.1039349747	
    Loss 2250.3517154024	
    Loss 1862.6101712043	
    Loss 2585.8746286603	
    Loss 1953.2398944051	
    Loss 1789.8872241294	
    Loss 1967.7071637017	
    Loss 2358.7477080007	
    Loss 1976.4887333995	
    Loss 1992.929067506	
Epoch 14	
      0
   8578
      0
    143
      0
      0
      0
   4653
      0
 118319
      0
    115
[torch.DoubleTensor of size 12]

Validation accuracy:	0.091276705511046	
Grad norm	9.8966925174914	
    Loss 2290.0628263647	
    Loss 2141.7449168809	
    Loss 1870.1794905558	
    Loss 2218.2008859336	
    Loss 1968.4851435889	
    Loss 1931.8767414924	
    Loss 1973.5920179566	
    Loss 2127.8061415521	
    Loss 1925.7815498286	
    Loss 1688.9322302114	
    Loss 2355.9534236709	
    Loss 2273.2129927573	
    Loss 1973.0314687797	
    Loss 1900.3327496976	
    Loss 2053.982765102	
    Loss 1696.0242750326	
    Loss 1922.9594737486	
    Loss 1823.262845089	
    Loss 1961.6513545168	
    Loss 1983.9524246579	
    Loss 1877.0500856457	
    Loss 1936.21691161	
    Loss 2079.0488289244	
    Loss 1942.8483936849	
    Loss 1809.9579939224	
    Loss 2334.117058071	
    Loss 2096.8078434529	
    Loss 1784.9530163434	
    Loss 1825.805331064	
    Loss 1801.0367741627	
    Loss 1990.6323221794	
    Loss 2011.3150447447	
    Loss 1902.9489144989	
    Loss 1939.1864764693	
    Loss 2032.7158750464	
    Loss 1876.3590809457	
    Loss 1715.6369443434	
    Loss 1936.330028874	
    Loss 2047.5566957499	
    Loss 2146.8787456522	
    Loss 1891.3930608399	
    Loss 2592.9137605766	
    Loss 2033.1352451802	
    Loss 1777.7562662569	
    Loss 2071.4682921095	
    Loss 2107.3747454245	
    Loss 2265.3899966799	
    Loss 1760.4383994222	
    Loss 2040.3747769001	
    Loss 2137.3983803366	
    Loss 1753.4056104613	
    Loss 1967.5280262694	
    Loss 1933.7245104544	
    Loss 2211.9003855888	
    Loss 1984.338165379	
    Loss 1829.6158288397	
    Loss 1795.7974274103	
    Loss 2027.8392487329	
    Loss 2197.7696265593	
    Loss 2109.9722756161	
    Loss 1713.2002629908	
    Loss 1875.5924090373	
    Loss 2046.0720823357	
    Loss 2031.8970329715	
    Loss 2158.6008026642	
    Loss 1916.8885434471	
    Loss 1974.6863268901	
    Loss 1837.2180415248	
    Loss 2128.312875628	
    Loss 1859.2588922272	
    Loss 1949.6729227361	
    Loss 2163.1401028679	
Epoch 15	
   8086
 102691
      0
  13869
      0
      0
    744
      0
     32
      0
    258
    783
      7
      0
     35
   3297
    587
   1027
    390
      0
      0
      0
      2
[torch.DoubleTensor of size 23]

Validation accuracy:	0.064419458606458	
Grad norm	9.6058767225524	
    Loss 2215.3681385251	
    Loss 2265.3610931736	
    Loss 2341.4528610264	
    Loss 1912.2969105605	
    Loss 1843.3504459959	
    Loss 2098.3565343556	
    Loss 2366.0232336906	
    Loss 1736.0966414482	
    Loss 1836.0352671229	
    Loss 2004.7098977904	
    Loss 1960.2008226756	
    Loss 1864.118944015	
    Loss 1822.1080592809	
    Loss 2007.6926391041	
    Loss 2401.8892673911	
    Loss 1811.3856703987	
    Loss 2037.0581584449	
    Loss 1959.2849525188	
    Loss 1828.8062725834	
    Loss 2173.7219972846	
    Loss 1911.4745801526	
    Loss 1968.6216493712	
    Loss 1890.5797227967	
    Loss 2087.060195719	
    Loss 2314.3718561596	
    Loss 2286.5804628489	
    Loss 1967.1118438563	
    Loss 1777.01447558	
    Loss 2179.1768026464	
    Loss 2231.1726835367	
    Loss 1719.5359661164	
    Loss 1821.9718676824	
    Loss 1824.882398481	
    Loss 2170.4457272872	
    Loss 1963.0384638248	
    Loss 1704.4744855036	
    Loss 2420.4756931741	
    Loss 1864.3912908867	
    Loss 1674.4316492932	
    Loss 1816.1396422711	
    Loss 1797.4746338656	
    Loss 2411.9623299766	
    Loss 1856.0049710648	
    Loss 1830.9811354952	
    Loss 2224.6055439322	
    Loss 1819.3685974659	
    Loss 2013.5669774574	
    Loss 1935.6781622259	
    Loss 1698.1067294338	
    Loss 1771.6028018018	
    Loss 2155.8904459722	
    Loss 1937.4891931212	
    Loss 1863.2751441002	
    Loss 2111.5528105895	
    Loss 1837.0269690215	
    Loss 1853.3243046375	
    Loss 2115.3839783425	
    Loss 1945.520469103	
    Loss 2511.9833969364	
    Loss 1893.6713856144	
    Loss 2038.7918023124	
    Loss 1958.055097755	
    Loss 1769.3021065986	
    Loss 2425.7682294848	
    Loss 2200.0835996499	
    Loss 2476.8163646251	
    Loss 1738.3228655643	
    Loss 2121.6547248859	
    Loss 1952.2666736276	
    Loss 2128.3688615952	
    Loss 2051.0680685046	
    Loss 2002.6469227173	
Epoch 16	
     0
 22112
    48
     0
  8881
     0
     0
     0
 95679
  1539
  1470
     2
     0
     0
    14
     4
    45
  1812
   202
[torch.DoubleTensor of size 19]

Validation accuracy:	0.11144998786113	
Grad norm	9.5192673644037	
    Loss 2242.3838988673	
    Loss 1864.0506787212	
    Loss 1880.3720167373	
    Loss 2059.6695391568	
    Loss 2208.9369116678	
    Loss 2173.6134372545	
    Loss 2005.1131837737	
    Loss 2113.3394766494	
    Loss 2017.7467358098	
    Loss 2031.1277203377	
    Loss 2123.8011677824	
    Loss 2109.6695650017	
    Loss 2226.9436220172	
    Loss 2036.1881862205	
    Loss 1948.4173930295	
    Loss 1793.0839808788	
    Loss 2368.2533706844	
    Loss 1801.0148598317	
    Loss 1855.6778981967	
    Loss 2086.7731142871	
    Loss 1794.2296731422	
    Loss 2166.8153406226	
    Loss 2424.9959456876	
    Loss 2176.3336596289	
    Loss 2242.9077370447	
    Loss 2133.5446529211	
    Loss 2001.773763362	
    Loss 1878.9419389796	
    Loss 1798.034239441	
    Loss 1953.8272969647	
    Loss 2100.7243203157	
    Loss 2295.6794991311	
    Loss 2015.4309444511	
    Loss 2231.3416691388	
    Loss 2097.0093562113	
    Loss 1995.8512155369	
    Loss 2105.7085716336	
    Loss 1848.2545604287	
    Loss 2153.3495061487	
    Loss 1755.3999338091	
    Loss 1851.7098630938	
    Loss 2226.4847427502	
    Loss 1830.9219965146	
    Loss 2136.6430592462	
    Loss 2091.2078254004	
    Loss 2028.4785481879	
    Loss 1842.7446131853	
    Loss 2005.1233596507	
    Loss 2002.2694814077	
    Loss 1682.4510200973	
    Loss 1818.2905145424	
    Loss 1871.6024400044	
    Loss 2089.4840288093	
    Loss 2245.7645561105	
    Loss 1873.8672471197	
    Loss 1938.4129846284	
    Loss 2589.085683102	
    Loss 1670.0363763467	
    Loss 2716.9204918598	
    Loss 1880.0906863711	
    Loss 1965.3143456499	
    Loss 2189.2922295991	
    Loss 2276.6146213762	
    Loss 2330.3576144617	
    Loss 1792.9022798185	
    Loss 1869.4185464728	
    Loss 1799.4859200316	
    Loss 2004.2488156163	
    Loss 1698.7785103672	
    Loss 1725.1333308868	
    Loss 1852.1810286403	
    Loss 2126.792984968	
Epoch 17	
 46526
     0
     0
 83235
  2046
     0
     0
     0
     0
     0
     1
[torch.DoubleTensor of size 11]

Validation accuracy:	0.12088037144938	
Grad norm	9.3284361668895	
    Loss 1876.17566917	
    Loss 2140.0970226809	
    Loss 1924.9190365151	
    Loss 2173.7867895257	
    Loss 2067.6525558523	
    Loss 1759.5022238049	
    Loss 1969.1470084866	
    Loss 1956.119175652	
    Loss 1960.2410421998	
    Loss 1955.7798906263	
    Loss 2178.6289384849	
    Loss 1733.4040479638	
    Loss 2164.1220922444	
    Loss 1687.7803026289	
    Loss 2111.0926733306	
    Loss 2152.6916543145	
    Loss 2018.7430834353	
    Loss 1875.5877975003	
    Loss 1884.64108739	
    Loss 2026.8168724204	
    Loss 2034.2474679749	
    Loss 1876.9808173419	
    Loss 2073.3949089647	
    Loss 1857.5063894079	
    Loss 2033.5598600342	
    Loss 2072.9382396716	
    Loss 2059.5322741683	
    Loss 1948.6885787221	
    Loss 1932.6052283328	
    Loss 1871.1462501615	
    Loss 2015.2122294992	
    Loss 1777.676655126	
    Loss 1856.4427120821	
    Loss 1717.2177298067	
    Loss 2011.2583551718	
    Loss 1869.0950056155	
    Loss 2260.5016700684	
    Loss 1906.2384451433	
    Loss 1746.2947011594	
    Loss 1898.9166361191	
    Loss 2217.0655852831	
    Loss 1990.3698853292	
    Loss 2077.6421871398	
    Loss 1900.0986277501	
    Loss 2031.5928593405	
    Loss 2287.8574523556	
    Loss 1765.0681356134	
    Loss 1898.2635587137	
    Loss 1852.7159908696	
    Loss 1766.4043130087	
    Loss 2008.8612336623	
    Loss 1760.0274319074	
    Loss 1901.9950565304	
    Loss 1768.9934894506	
    Loss 2436.2811750532	
    Loss 2298.1729609698	
