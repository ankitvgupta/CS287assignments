[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.5	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2334
  860
 1292
 3527
 1211
 2097
 4080
 4297
 1920
 7879
 2114
 6689
 1980
 3227
 5140
 1180
 1321
 2491
 6443
 2564
 4774
 1053
 2227
 1309
 3789
 4824
 1537
 5466
 4396
 1098
 1190
 1379
 2025
 1016
 4524
 4672
 1905
 3757
 3890
  939
 2632
 5268
 2534
 1519
 1439
[torch.DoubleTensor of size 45]

Validation accuracy:	0.022259650400583	
Grad norm	0	
    Loss 11387835.998271	
    Loss 10995001.903879	
    Loss 10615758.692745	
    Loss 10249608.369248	
    Loss 9896093.2042003	
    Loss 9554763.4592867	
    Loss 9225223.6197534	
    Loss 8907041.657626	
    Loss 8599849.6132216	
    Loss 8303261.9568414	
    Loss 8016891.3850543	
    Loss 7740394.5417872	
    Loss 7473445.7815491	
    Loss 7215714.9699046	
    Loss 6966868.6252382	
    Loss 6726600.8274869	
    Loss 6494621.4426643	
    Loss 6270646.7123005	
    Loss 6054398.6168794	
    Loss 5845609.1780664	
    Loss 5644014.3016074	
    Loss 5449371.8969449	
    Loss 5261442.8362167	
    Loss 5079996.0606495	
    Loss 4904815.5449162	
    Loss 4735678.5278372	
    Loss 4572374.2006346	
    Loss 4414697.0463022	
    Loss 4262460.4122195	
    Loss 4115475.2829754	
    Loss 3973554.707914	
    Loss 3836536.8201426	
    Loss 3704239.8702189	
    Loss 3576506.7150166	
    Loss 3453176.4211268	
    Loss 3334102.3606834	
    Loss 3219134.2660531	
    Loss 3108136.9023454	
    Loss 3000963.6396265	
    Loss 2897483.7614748	
    Loss 2797574.4931175	
    Loss 2701107.3579588	
    Loss 2607969.9957037	
    Loss 2518051.7237579	
    Loss 2431227.8459502	
    Loss 2347397.1919853	
    Loss 2266461.9139212	
    Loss 2188319.1715515	
    Loss 2112870.1678436	
    Loss 2040024.9097295	
    Loss 1969692.7386326	
    Loss 1901780.651565	
    Loss 1836214.0264536	
    Loss 1772908.0515098	
    Loss 1711784.0361146	
    Loss 1652766.5445552	
    Loss 1595786.9929325	
    Loss 1540770.3932257	
    Loss 1487653.9495575	
    Loss 1436368.4545804	
    Loss 1386854.1894191	
    Loss 1339047.9091246	
    Loss 1292888.496851	
    Loss 1248320.9470476	
    Loss 1205289.0301162	
    Loss 1163742.7585733	
    Loss 1123627.118572	
    Loss 1084896.8980206	
    Loss 1047498.4517605	
    Loss 1011391.777224	
    Loss 976533.66817868	
    Loss 942877.77501428	
    Loss 910381.04707311	
    Loss 879006.13243711	
    Loss 848712.166713	
    Loss 819463.37523113	
    Loss 791223.29385873	
    Loss 763958.24161018	
    Loss 737631.43738337	
    Loss 712212.24179115	
    Loss 687669.2936043	
    Loss 663973.22606965	
    Loss 641095.77196819	
    Loss 619006.44241041	
    Loss 597677.80184819	
    Loss 577085.60132916	
    Loss 557203.23817754	
    Loss 538007.91494655	
    Loss 519471.70774418	
    Loss 501575.86143756	
    Loss 484296.79107042	
    Loss 467615.01184958	
    Loss 451509.77580391	
    Loss 435959.85414693	
    Loss 420944.42044465	
    Loss 406446.41086189	
    Loss 392449.39038543	
    Loss 378933.9332148	
    Loss 365884.31267729	
    Loss 353285.60794789	
    Loss 341120.86082071	
    Loss 329375.19737706	
    Loss 318034.86819446	
    Loss 307086.78018107	
    Loss 296515.37390487	
    Loss 286308.5274344	
    Loss 276454.21569491	
    Loss 266938.97781171	
    Loss 257752.63763898	
    Loss 248880.56208814	
    Loss 240315.962999	
    Loss 232046.03984018	
    Loss 224062.92615685	
    Loss 216354.91108397	
    Loss 208913.38058625	
    Loss 201728.30687609	
    Loss 194789.29199968	
    Loss 188090.80063134	
    Loss 181622.88552119	
    Loss 175377.39694459	
    Loss 169348.49794913	
    Loss 163527.84341489	
    Loss 157906.60728978	
    Loss 152479.83606328	
    Loss 147240.89866078	
    Loss 142181.02881764	
    Loss 137295.6300631	
    Loss 132577.8551932	
    Loss 128024.92955733	
    Loss 123627.9439484	
    Loss 119382.58695124	
    Loss 115283.98194369	
    Loss 111327.47164597	
    Loss 107506.63715379	
    Loss 103817.4458023	
    Loss 100255.81991822	
    Loss 96816.365371918	
    Loss 93495.197796724	
    Loss 90289.240485996	
    Loss 87194.568688366	
    Loss 84205.532923548	
    Loss 81320.225244228	
    Loss 78534.519451404	
Epoch 2	
 131112
      0
      0
      0
      0
      0
      0
      0
    696
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098689002184996	
Grad norm	109.57694985804	
    Loss 76908.744877665	
    Loss 74274.453893807	
    Loss 71732.206756472	
    Loss 69276.810655888	
    Loss 66906.032000112	
    Loss 64616.628901027	
    Loss 62406.859781084	
    Loss 60272.175290871	
    Loss 58212.627245696	
    Loss 56224.161044172	
    Loss 54303.894529148	
    Loss 52448.768553803	
    Loss 50657.754833647	
    Loss 48929.384605693	
    Loss 47259.859429222	
    Loss 45648.39938043	
    Loss 44093.095287934	
    Loss 42591.246792626	
    Loss 41141.290636445	
    Loss 39740.643628124	
    Loss 38388.521591674	
    Loss 37083.321531912	
    Loss 35822.679482818	
    Loss 34605.680240917	
    Loss 33430.006946648	
    Loss 32295.248618577	
    Loss 31199.665708919	
    Loss 30141.45994264	
    Loss 29120.279412589	
    Loss 28134.551319222	
    Loss 27183.025029143	
    Loss 26263.988239593	
    Loss 25376.232359368	
    Loss 24519.098361696	
    Loss 23691.657936619	
    Loss 22892.819970251	
    Loss 22120.881203575	
    Loss 21375.987913091	
    Loss 20657.382133324	
    Loss 19962.508601447	
    Loss 19292.255313328	
    Loss 18644.983729776	
    Loss 18019.279913141	
    Loss 17416.263717258	
    Loss 16834.234384941	
    Loss 16271.416967874	
    Loss 15728.608782518	
    Loss 15204.493405101	
    Loss 14698.457608239	
    Loss 14209.49288528	
    Loss 13737.165150922	
    Loss 13281.831940832	
    Loss 12840.435970167	
    Loss 12416.330705515	
    Loss 12006.577023298	
    Loss 11611.266604028	
    Loss 11229.034767524	
    Loss 10859.645848026	
    Loss 10503.862124161	
    Loss 10160.584877139	
    Loss 9828.4274980477	
    Loss 9507.8223348831	
    Loss 9198.2388372809	
    Loss 8898.6200240391	
    Loss 8610.0738729146	
    Loss 8331.4318209079	
    Loss 8062.1579135189	
    Loss 7802.6940232614	
    Loss 7551.4574044213	
    Loss 7308.8128657583	
    Loss 7074.947720564	
    Loss 6849.1634670241	
    Loss 6630.5770478336	
    Loss 6419.8323955184	
    Loss 6216.6613118962	
    Loss 6021.0528377842	
    Loss 5831.3852961687	
    Loss 5648.7147264909	
    Loss 5471.9860419489	
    Loss 5302.0169249416	
    Loss 5136.8639730063	
    Loss 4978.1315416805	
    Loss 4824.5318949194	
    Loss 4676.2447471843	
    Loss 4533.5450495162	
    Loss 4394.6945304247	
    Loss 4260.4083733356	
    Loss 4131.8241278337	
    Loss 4006.8033428504	
    Loss 3886.6907107821	
    Loss 3771.4902449033	
    Loss 3659.0389401526	
    Loss 3552.0526919556	
    Loss 3448.2717463731	
    Loss 3346.489693959	
    Loss 3248.9300343727	
    Loss 3155.1763029483	
    Loss 3064.1363269936	
    Loss 2976.5257966823	
    Loss 2891.9637380319	
    Loss 2810.3176673364	
    Loss 2731.4874968575	
    Loss 2655.3063866756	
    Loss 2582.1889825141	
    Loss 2511.4859610667	
    Loss 2443.3061225842	
    Loss 2377.0958484703	
    Loss 2313.5019069754	
    Loss 2251.9985545373	
    Loss 2190.8402395161	
    Loss 2133.6142207809	
    Loss 2077.6447767501	
    Loss 2024.643855904	
    Loss 1973.5858075973	
    Loss 1924.2624867743	
    Loss 1876.9566412379	
    Loss 1830.2212537215	
    Loss 1785.0658864181	
    Loss 1741.7345962426	
    Loss 1699.0364128205	
    Loss 1659.3206710032	
    Loss 1620.7928468733	
    Loss 1582.9550327379	
    Loss 1546.5859285162	
    Loss 1511.2982947747	
    Loss 1476.9333845759	
    Loss 1443.4244342834	
    Loss 1411.1504775906	
    Loss 1381.5337903185	
    Loss 1351.6120692979	
    Loss 1322.5110367356	
    Loss 1294.8802118873	
    Loss 1268.3907148276	
    Loss 1242.6911332115	
    Loss 1218.3171711341	
    Loss 1194.3634484909	
    Loss 1171.020993356	
    Loss 1148.2138141265	
    Loss 1126.6354859405	
    Loss 1106.5614505003	
    Loss 1086.3108288659	
    Loss 1067.5444763783	
    Loss 1048.9826557442	
Epoch 3	
 131764
      0
      0
      0
      0
      0
      0
      0
     44
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10005462490896	
Grad norm	14.023176260002	
    Loss 1037.6553516966	
    Loss 1019.4719953355	
    Loss 1003.1078570208	
    Loss 986.78001731027	
    Loss 970.6406392413	
    Loss 955.08382437227	
    Loss 940.48486133995	
    Loss 925.70865851862	
    Loss 912.27982421934	
    Loss 899.0829326158	
    Loss 886.74963869602	
    Loss 873.92877691057	
    Loss 861.54048784928	
    Loss 849.94208735446	
    Loss 838.30359159125	
    Loss 827.62967199596	
    Loss 817.646759787	
    Loss 808.10212643776	
    Loss 798.67228896717	
    Loss 788.99737707652	
    Loss 780.05795146498	
    Loss 771.94351981806	
    Loss 763.56184188475	
    Loss 755.53344261759	
    Loss 747.0526461766	
    Loss 739.10391696093	
    Loss 731.67416403461	
    Loss 724.09685423164	
    Loss 717.34096849983	
    Loss 710.86372180593	
    Loss 705.12754095323	
    Loss 698.83871162147	
    Loss 692.4914079109	
    Loss 686.50978843454	
    Loss 681.02119191088	
    Loss 675.49997425354	
    Loss 669.78137546708	
    Loss 664.43453939434	
    Loss 660.00726483827	
    Loss 654.87442999876	
    Loss 650.47333269607	
    Loss 646.21768696337	
    Loss 641.2500444002	
    Loss 637.25665105468	
    Loss 633.6834462481	
    Loss 629.62679787276	
    Loss 626.14903099648	
    Loss 622.75255771336	
    Loss 619.52758002304	
    Loss 615.8701887392	
    Loss 612.12101832744	
    Loss 609.4556537448	
    Loss 605.10436777238	
    Loss 602.83993705747	
    Loss 600.5398719597	
    Loss 598.60487097249	
    Loss 596.1657381713	
    Loss 593.50877998175	
    Loss 591.74686442536	
    Loss 590.34608632199	
    Loss 588.15244291546	
    Loss 586.11290229362	
    Loss 584.14439959952	
    Loss 581.56049778251	
    Loss 579.78153054005	
    Loss 578.04307068162	
    Loss 576.20040903453	
    Loss 574.82689180331	
    Loss 573.01255678857	
    Loss 570.99670824974	
    Loss 569.42094804797	
    Loss 567.90106264954	
    Loss 565.90195023404	
    Loss 564.19045387345	
    Loss 562.96452555294	
    Loss 562.2652400495	
    Loss 560.80920514279	
    Loss 559.76964535001	
    Loss 558.46179456617	
    Loss 557.93450923036	
    Loss 556.38092405085	
    Loss 555.60996922905	
    Loss 554.48653978028	
    Loss 553.42706180943	
    Loss 552.98394902774	
    Loss 551.35921662779	
    Loss 549.59723583827	
    Loss 548.88672437697	
    Loss 547.45890445166	
    Loss 546.60193645236	
    Loss 546.61233027625	
    Loss 545.3043882417	
    Loss 545.64017654933	
    Loss 545.46557776399	
    Loss 543.74111965159	
    Loss 542.83172991601	
    Loss 542.33517426545	
    Loss 541.47049020513	
    Loss 540.89489152425	
    Loss 540.28114601703	
    Loss 539.72914294715	
    Loss 539.23414578061	
    Loss 538.64548659795	
    Loss 538.49467560138	
    Loss 538.2843672266	
    Loss 538.19751890766	
    Loss 537.64149450671	
    Loss 537.52166723554	
    Loss 537.22149985554	
    Loss 535.25921798359	
    Loss 535.16079212986	
    Loss 534.35582250165	
    Loss 534.59389266056	
    Loss 534.93240130835	
    Loss 535.184283332	
    Loss 535.77134626351	
    Loss 535.33726441153	
    Loss 534.78900426125	
    Loss 534.59432166457	
    Loss 533.52171309248	
    Loss 534.01397617337	
    Loss 534.27200871849	
    Loss 533.94343796606	
    Loss 533.75148276842	
    Loss 533.32523215986	
    Loss 532.72509668559	
    Loss 531.77682925554	
    Loss 531.0029176012	
    Loss 531.75060731278	
    Loss 531.14454752337	
    Loss 530.34273780482	
    Loss 530.02651563671	
    Loss 529.87856346919	
    Loss 529.64339663268	
    Loss 529.90370294999	
    Loss 529.66987818589	
    Loss 529.26520649059	
    Loss 528.60588492302	
    Loss 528.39376959312	
    Loss 528.95891943074	
    Loss 528.65279087594	
    Loss 529.14245292278	
    Loss 529.13822521407	
Epoch 4	
 131766
      0
      0
      0
      0
      0
      0
      0
     42
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006221170187	
Grad norm	6.5414356872305	
    Loss 528.65387775542	
    Loss 528.01850915399	
    Loss 528.59015632828	
    Loss 528.63822623704	
    Loss 528.28421942261	
    Loss 528.00576122509	
    Loss 528.13889074008	
    Loss 527.61475817551	
    Loss 527.89400801862	
    Loss 527.91530998779	
    Loss 528.39957713947	
    Loss 527.96232839681	
    Loss 527.52617196279	
    Loss 527.43447614211	
    Loss 526.92280709231	
    Loss 526.99339690163	
    Loss 527.36233539271	
    Loss 527.84069861972	
    Loss 528.05907586991	
    Loss 527.71113409392	
    Loss 527.78546441507	
    Loss 528.3918324688	
    Loss 528.4272947272	
    Loss 528.51163994583	
    Loss 527.8621920262	
    Loss 527.4632286351	
    Loss 527.34109107097	
    Loss 526.81406033601	
    Loss 526.86796197137	
    Loss 526.95069156074	
    Loss 527.57243911509	
    Loss 527.38923389497	
    Loss 526.94778733912	
    Loss 526.67790217551	
    Loss 526.71414400785	
    Loss 526.50387057399	
    Loss 525.93355450029	
    Loss 525.53970383145	
    Loss 525.90361046754	
    Loss 525.41220541903	
    Loss 525.48462235928	
    Loss 525.55646894074	
    Loss 524.76000693789	
    Loss 524.77160487279	
    Loss 525.06846312691	
    Loss 524.76851748812	
    Loss 524.90877606894	
    Loss 525.002308976	
    Loss 525.14963108591	
    Loss 524.73716796495	
    Loss 524.1218387986	
    Loss 524.49936490889	
    Loss 523.08676503161	
    Loss 523.64623972608	
    Loss 524.08549635843	
    Loss 524.79342180649	
    Loss 524.90615481935	
    Loss 524.71529225212	
    Loss 525.32955591697	
    Loss 526.22640935523	
    Loss 526.2439748404	
    Loss 526.3366421732	
    Loss 526.42931420601	
    Loss 525.83730738959	
    Loss 525.97769974397	
    Loss 526.09634891185	
    Loss 526.05201522795	
    Loss 526.405773792	
    Loss 526.27628750148	
    Loss 525.87256047694	
    Loss 525.85181146216	
    Loss 525.83217009111	
    Loss 525.28472544658	
    Loss 524.9691072908	
    Loss 525.0993097659	
    Loss 525.70256297328	
    Loss 525.50847474467	
    Loss 525.67938844785	
    Loss 525.54304515748	
    Loss 526.15171715312	
    Loss 525.69505373708	
    Loss 525.9843684051	
    Loss 525.8821750439	
    Loss 525.81003625493	
    Loss 526.32668650772	
    Loss 525.61926785972	
    Loss 524.74615510442	
    Loss 524.88676736372	
    Loss 524.29034068188	
    Loss 524.23057610798	
    Loss 525.01485955296	
    Loss 524.44802765306	
    Loss 525.50010198266	
    Loss 526.01684791286	
    Loss 524.96101459841	
    Loss 524.70006997141	
    Loss 524.82479255009	
    Loss 524.57132564039	
    Loss 524.58245849819	
    Loss 524.52884703479	
    Loss 524.52128805566	
    Loss 524.553799523	
    Loss 524.47132225726	
    Loss 524.8085259074	
    Loss 525.07234397785	
    Loss 525.44534486911	
    Loss 525.32689237482	
    Loss 525.63556528234	
    Loss 525.74261545318	
    Loss 524.18208364723	
    Loss 524.46892444471	
    Loss 524.03587228136	
    Loss 524.63211307415	
    Loss 525.31546110979	
    Loss 525.89706924989	
    Loss 526.80433593365	
    Loss 526.68375547169	
    Loss 526.4304817819	
    Loss 526.52619881782	
    Loss 525.73212461968	
    Loss 526.4949849967	
    Loss 527.01122551103	
    Loss 526.93690102934	
    Loss 526.98738038586	
    Loss 526.78994730996	
    Loss 526.41870870489	
    Loss 525.68830335124	
    Loss 525.12960197531	
    Loss 526.08103328059	
    Loss 525.67179763154	
    Loss 525.05972110618	
    Loss 524.92580791041	
    Loss 524.95152135001	
    Loss 524.8860699898	
    Loss 525.31416025719	
    Loss 525.23738385739	
    Loss 524.98695512211	
    Loss 524.4767185171	
    Loss 524.40710818499	
    Loss 525.11082302345	
    Loss 524.93961983395	
    Loss 525.55942230796	
    Loss 525.67833252819	
Epoch 5	
 131766
      0
      0
      0
      0
      0
      0
      0
     42
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006221170187	
Grad norm	6.0317747512592	
    Loss 525.26702340511	
    Loss 524.7484199254	
    Loss 525.43216141113	
    Loss 525.5904768457	
    Loss 525.34068961868	
    Loss 525.16582613029	
    Loss 525.39750860347	
    Loss 524.97074504557	
    Loss 525.33980554169	
    Loss 525.44668210899	
    Loss 526.01772755213	
    Loss 525.66454680914	
    Loss 525.30940512383	
    Loss 525.2935331687	
    Loss 524.85623230708	
    Loss 524.99868896287	
    Loss 525.43527772029	
    Loss 525.98133996962	
    Loss 526.26269772671	
    Loss 525.97646354023	
    Loss 526.11107450065	
    Loss 526.77696628645	
    Loss 526.86990794217	
    Loss 527.00832661606	
    Loss 526.41107023711	
    Loss 526.06150924881	
    Loss 525.98859442052	
    Loss 525.50854330336	
    Loss 525.60802994283	
    Loss 525.73366168658	
    Loss 526.39879459307	
    Loss 526.25485595435	
    Loss 525.85224493433	
    Loss 525.62048394811	
    Loss 525.69427189573	
    Loss 525.51841777787	
    Loss 524.98288065526	
    Loss 524.62121462894	
    Loss 525.01693421861	
    Loss 524.55746125819	
    Loss 524.66013342741	
    Loss 524.76182560064	
    Loss 523.9935859664	
    Loss 524.03077218248	
    Loss 524.35259970682	
    Loss 524.07821839355	
    Loss 524.24263552161	
    Loss 524.35904944458	
    Loss 524.5286867919	
    Loss 524.1369681829	
    Loss 523.54172626356	
    Loss 523.93998792654	
    Loss 522.5473873893	
    Loss 523.1251092117	
    Loss 523.58296186997	
    Loss 524.30880492688	
    Loss 524.43884658546	
    Loss 524.26484980703	
    Loss 524.89502009592	
    Loss 525.80740649039	
    Loss 525.83951195085	
