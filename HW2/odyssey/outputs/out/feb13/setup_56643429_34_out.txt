[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	10	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1733
 1893
 3211
 1864
 2282
 9014
 1142
 3056
 5107
  946
 2864
 1331
 4860
 4700
 2535
  834
 1673
 1709
 3475
 3981
 2586
 3163
 4776
 1273
 1242
 1458
  941
 2117
 3101
 2523
 4700
 1549
  915
 7527
  794
 6373
 1095
 1959
 5440
 2814
 5080
 1920
 1691
 5521
 3040
[torch.DoubleTensor of size 45]

Validation accuracy:	0.019884984219471	
Grad norm	0	
    Loss 11382068.093841	
    Loss 5635264.1845965	
    Loss 2790513.2355664	
    Loss 1382208.0282607	
    Loss 685027.09207835	
    Loss 339847.01192277	
    Loss 168982.72880699	
    Loss 84355.980778702	
    Loss 42503.988782085	
    Loss 21749.951731583	
    Loss 11501.057377732	
    Loss 6398.7191470117	
    Loss 3844.3225497351	
    Loss 2600.2027186254	
    Loss 2005.43435963	
    Loss 1682.3364839691	
    Loss 1546.9483574627	
    Loss 1499.3779055792	
    Loss 1447.3386327072	
    Loss 1412.7775999177	
    Loss 1420.6646403923	
    Loss 1417.6655409235	
    Loss 1417.0942086307	
    Loss 1422.5100753813	
    Loss 1398.0233373691	
    Loss 1409.7786581046	
    Loss 1389.8458265384	
    Loss 1402.1208698658	
    Loss 1407.4691498133	
    Loss 1393.1074270687	
    Loss 1418.4699861972	
    Loss 1396.1626153287	
    Loss 1420.7765405058	
    Loss 1420.6247756742	
    Loss 1409.3378311696	
    Loss 1419.7035689322	
    Loss 1403.8525867673	
    Loss 1402.3494019293	
    Loss 1409.2662710592	
    Loss 1403.0722508937	
    Loss 1428.4760632097	
    Loss 1414.0617604867	
    Loss 1401.372345834	
    Loss 1394.5956265734	
    Loss 1432.6639482786	
    Loss 1406.97352071	
    Loss 1417.8867492991	
    Loss 1409.9999063419	
    Loss 1386.4687575764	
    Loss 1383.2757067113	
    Loss 1377.029646812	
    Loss 1418.4445774238	
    Loss 1383.7736126197	
    Loss 1419.6495459834	
    Loss 1428.8350320381	
    Loss 1419.5899284747	
    Loss 1406.3967222253	
    Loss 1419.7571700293	
    Loss 1424.172827417	
    Loss 1419.1423861981	
    Loss 1391.9291536355	
    Loss 1401.1279438458	
    Loss 1415.189368613	
    Loss 1398.209817278	
    Loss 1408.9393161855	
    Loss 1408.9801106078	
    Loss 1412.5197689938	
    Loss 1411.0873720799	
    Loss 1396.2017683397	
    Loss 1426.9105559929	
    Loss 1404.8665941987	
    Loss 1392.0082063846	
    Loss 1387.5833924793	
    Loss 1394.1129239939	
    Loss 1400.5634418914	
    Loss 1392.928920031	
    Loss 1408.4442035502	
    Loss 1385.9828472442	
    Loss 1404.6315707022	
    Loss 1427.8252974498	
    Loss 1403.0442480897	
    Loss 1421.3320113985	
    Loss 1370.5336469583	
    Loss 1399.6344147568	
    Loss 1418.6867818029	
    Loss 1392.0190896836	
    Loss 1355.7049608668	
    Loss 1391.3481386854	
    Loss 1409.8019034046	
    Loss 1424.4689130811	
    Loss 1413.7054393182	
    Loss 1398.6012138719	
    Loss 1402.7755327411	
    Loss 1393.0522481422	
    Loss 1371.8764941324	
    Loss 1374.5829493044	
    Loss 1384.9002258908	
    Loss 1357.9931273288	
    Loss 1383.3893574867	
    Loss 1402.5395955026	
    Loss 1411.080108941	
    Loss 1403.857902384	
    Loss 1398.744811503	
    Loss 1412.1091906065	
    Loss 1410.9319912355	
    Loss 1432.7220955442	
    Loss 1398.6840274994	
    Loss 1376.9801816604	
    Loss 1411.0645545797	
    Loss 1368.2899293143	
    Loss 1388.6987966423	
    Loss 1396.2585196417	
    Loss 1385.073291338	
    Loss 1393.8186107165	
    Loss 1398.7871103201	
    Loss 1430.2566843521	
    Loss 1395.4380380425	
    Loss 1418.5085932197	
    Loss 1415.8672655687	
    Loss 1403.5821292772	
    Loss 1403.0466909665	
    Loss 1411.5043597045	
    Loss 1392.774190734	
    Loss 1400.6637575144	
    Loss 1394.4534715799	
    Loss 1394.2197300026	
    Loss 1400.3258855094	
    Loss 1429.2968135393	
    Loss 1424.4512893163	
    Loss 1412.3679021587	
    Loss 1395.8339658148	
    Loss 1380.467510322	
    Loss 1366.2612113836	
    Loss 1363.1053285224	
    Loss 1384.393391141	
    Loss 1406.3110287531	
    Loss 1414.1404013084	
    Loss 1419.2623818457	
    Loss 1427.4683007584	
    Loss 1421.0680509998	
    Loss 1389.091167648	
    Loss 1405.3156415065	
    Loss 1408.0475623507	
Epoch 2	
  8318
  1453
    24
     0
 43041
     0
     0
 15429
 59723
  3814
     6
[torch.DoubleTensor of size 11]

Validation accuracy:	0.08760469774217	
Grad norm	6.9833999913836	
    Loss 1362.0170980566	
    Loss 1396.7671317476	
    Loss 1427.5615824875	
    Loss 1401.7165896878	
    Loss 1400.2782491633	
    Loss 1399.9066802258	
    Loss 1406.1235171799	
    Loss 1386.8858849908	
    Loss 1425.6448245201	
    Loss 1412.1797771602	
    Loss 1430.0417917291	
    Loss 1411.776819184	
    Loss 1373.7201668882	
    Loss 1375.0276713139	
    Loss 1397.3737587414	
    Loss 1381.714780566	
    Loss 1398.3398747619	
    Loss 1425.8911506535	
    Loss 1410.9980208517	
    Loss 1394.753907241	
    Loss 1411.6018640879	
    Loss 1413.2154675887	
    Loss 1414.8825093995	
    Loss 1421.4306595579	
    Loss 1397.4644384252	
    Loss 1409.4995769995	
    Loss 1389.7047087778	
    Loss 1402.0507340626	
    Loss 1407.4313255485	
    Loss 1393.0869671723	
    Loss 1418.4599543406	
    Loss 1396.1586398118	
    Loss 1420.7749175746	
    Loss 1420.6237996495	
    Loss 1409.3377769613	
    Loss 1419.7033057093	
    Loss 1403.8526300597	
    Loss 1402.3493954834	
    Loss 1409.2661613816	
    Loss 1403.0722297405	
    Loss 1428.4760781542	
    Loss 1414.0617548473	
    Loss 1401.3723553659	
    Loss 1394.5956347718	
    Loss 1432.6639510057	
    Loss 1406.9735226472	
    Loss 1417.8867526172	
    Loss 1409.9999068083	
    Loss 1386.4687543047	
    Loss 1383.275703719	
    Loss 1377.0296525993	
    Loss 1418.444581916	
    Loss 1383.7735922082	
    Loss 1419.6495468449	
    Loss 1428.8350346706	
    Loss 1419.5899289	
    Loss 1406.3967226941	
    Loss 1419.7571694297	
    Loss 1424.1728282452	
    Loss 1419.1423862454	
    Loss 1391.9291535653	
    Loss 1401.1279437505	
    Loss 1415.1893685522	
    Loss 1398.2098171775	
    Loss 1408.939316078	
    Loss 1408.9801105446	
    Loss 1412.5197689699	
    Loss 1411.0873720668	
    Loss 1396.2017683277	
    Loss 1426.9105559862	
    Loss 1404.8665941966	
    Loss 1392.0082063801	
    Loss 1387.5833924773	
    Loss 1394.1129239931	
    Loss 1400.563441921	
    Loss 1392.9289200334	
    Loss 1408.4442035413	
    Loss 1385.9828472421	
    Loss 1404.6315706926	
    Loss 1427.8252974492	
    Loss 1403.0442480875	
    Loss 1421.332011398	
    Loss 1370.5336469579	
    Loss 1399.634414756	
    Loss 1418.6867818028	
    Loss 1392.0190896835	
    Loss 1355.7049608667	
    Loss 1391.3481386853	
    Loss 1409.8019034048	
    Loss 1424.4689130812	
    Loss 1413.7054393182	
    Loss 1398.601213872	
    Loss 1402.7755327411	
    Loss 1393.0522481422	
    Loss 1371.8764941324	
    Loss 1374.5829493044	
    Loss 1384.9002258908	
    Loss 1357.9931273288	
    Loss 1383.3893574867	
    Loss 1402.5395955026	
    Loss 1411.080108941	
    Loss 1403.857902384	
    Loss 1398.744811503	
    Loss 1412.1091906065	
    Loss 1410.9319912355	
    Loss 1432.7220955442	
    Loss 1398.6840274994	
    Loss 1376.9801816604	
    Loss 1411.0645545797	
    Loss 1368.2899293143	
    Loss 1388.6987966423	
    Loss 1396.2585196417	
    Loss 1385.073291338	
    Loss 1393.8186107165	
    Loss 1398.7871103201	
    Loss 1430.2566843521	
    Loss 1395.4380380425	
    Loss 1418.5085932197	
    Loss 1415.8672655687	
    Loss 1403.5821292772	
    Loss 1403.0466909665	
    Loss 1411.5043597045	
    Loss 1392.774190734	
    Loss 1400.6637575144	
    Loss 1394.4534715799	
    Loss 1394.2197300026	
    Loss 1400.3258855094	
    Loss 1429.2968135393	
    Loss 1424.4512893163	
    Loss 1412.3679021587	
    Loss 1395.8339658148	
    Loss 1380.467510322	
    Loss 1366.2612113836	
    Loss 1363.1053285224	
    Loss 1384.393391141	
    Loss 1406.3110287531	
    Loss 1414.1404013084	
    Loss 1419.2623818457	
    Loss 1427.4683007584	
    Loss 1421.0680509998	
    Loss 1389.091167648	
    Loss 1405.3156415065	
    Loss 1408.0475623507	
Epoch 3	
  8318
  1453
    24
     0
 43041
     0
     0
 15429
 59723
  3814
     6
[torch.DoubleTensor of size 11]

Validation accuracy:	0.08760469774217	
Grad norm	6.9833999913836	
    Loss 1362.0170980566	
    Loss 1396.7671317476	
    Loss 1427.5615824875	
    Loss 1401.7165896878	
    Loss 1400.2782491633	
    Loss 1399.9066802258	
    Loss 1406.1235171799	
    Loss 1386.8858849908	
    Loss 1425.6448245201	
    Loss 1412.1797771602	
    Loss 1430.0417917291	
    Loss 1411.776819184	
    Loss 1373.7201668882	
    Loss 1375.0276713139	
    Loss 1397.3737587414	
    Loss 1381.714780566	
    Loss 1398.3398747619	
    Loss 1425.8911506535	
    Loss 1410.9980208517	
    Loss 1394.753907241	
    Loss 1411.6018640879	
    Loss 1413.2154675887	
    Loss 1414.8825093995	
    Loss 1421.4306595579	
    Loss 1397.4644384252	
    Loss 1409.4995769995	
    Loss 1389.7047087778	
    Loss 1402.0507340626	
    Loss 1407.4313255485	
    Loss 1393.0869671723	
    Loss 1418.4599543406	
    Loss 1396.1586398118	
    Loss 1420.7749175746	
    Loss 1420.6237996495	
    Loss 1409.3377769613	
    Loss 1419.7033057093	
    Loss 1403.8526300597	
    Loss 1402.3493954834	
    Loss 1409.2661613816	
    Loss 1403.0722297405	
    Loss 1428.4760781542	
    Loss 1414.0617548473	
    Loss 1401.3723553659	
    Loss 1394.5956347718	
    Loss 1432.6639510057	
    Loss 1406.9735226472	
    Loss 1417.8867526172	
    Loss 1409.9999068083	
    Loss 1386.4687543047	
    Loss 1383.275703719	
    Loss 1377.0296525993	
    Loss 1418.444581916	
    Loss 1383.7735922082	
    Loss 1419.6495468449	
    Loss 1428.8350346706	
    Loss 1419.5899289	
    Loss 1406.3967226941	
    Loss 1419.7571694297	
    Loss 1424.1728282452	
    Loss 1419.1423862454	
    Loss 1391.9291535653	
    Loss 1401.1279437505	
    Loss 1415.1893685522	
    Loss 1398.2098171775	
    Loss 1408.939316078	
    Loss 1408.9801105446	
    Loss 1412.5197689699	
    Loss 1411.0873720668	
    Loss 1396.2017683277	
    Loss 1426.9105559862	
    Loss 1404.8665941966	
    Loss 1392.0082063801	
    Loss 1387.5833924773	
    Loss 1394.1129239931	
    Loss 1400.563441921	
    Loss 1392.9289200334	
    Loss 1408.4442035413	
    Loss 1385.9828472421	
    Loss 1404.6315706926	
    Loss 1427.8252974492	
    Loss 1403.0442480875	
    Loss 1421.332011398	
    Loss 1370.5336469579	
    Loss 1399.634414756	
    Loss 1418.6867818028	
    Loss 1392.0190896835	
    Loss 1355.7049608667	
    Loss 1391.3481386853	
    Loss 1409.8019034048	
    Loss 1424.4689130812	
    Loss 1413.7054393182	
    Loss 1398.601213872	
    Loss 1402.7755327411	
    Loss 1393.0522481422	
    Loss 1371.8764941324	
    Loss 1374.5829493044	
    Loss 1384.9002258908	
    Loss 1357.9931273288	
    Loss 1383.3893574867	
    Loss 1402.5395955026	
    Loss 1411.080108941	
    Loss 1403.857902384	
    Loss 1398.744811503	
    Loss 1412.1091906065	
    Loss 1410.9319912355	
    Loss 1432.7220955442	
    Loss 1398.6840274994	
    Loss 1376.9801816604	
    Loss 1411.0645545797	
    Loss 1368.2899293143	
    Loss 1388.6987966423	
    Loss 1396.2585196417	
    Loss 1385.073291338	
    Loss 1393.8186107165	
    Loss 1398.7871103201	
    Loss 1430.2566843521	
    Loss 1395.4380380425	
    Loss 1418.5085932197	
    Loss 1415.8672655687	
    Loss 1403.5821292772	
    Loss 1403.0466909665	
    Loss 1411.5043597045	
    Loss 1392.774190734	
    Loss 1400.6637575144	
    Loss 1394.4534715799	
    Loss 1394.2197300026	
    Loss 1400.3258855094	
    Loss 1429.2968135393	
    Loss 1424.4512893163	
    Loss 1412.3679021587	
    Loss 1395.8339658148	
    Loss 1380.467510322	
    Loss 1366.2612113836	
    Loss 1363.1053285224	
    Loss 1384.393391141	
    Loss 1406.3110287531	
    Loss 1414.1404013084	
    Loss 1419.2623818457	
    Loss 1427.4683007584	
    Loss 1421.0680509998	
    Loss 1389.091167648	
    Loss 1405.3156415065	
    Loss 1408.0475623507	
Epoch 4	
  8318
  1453
    24
     0
 43041
     0
     0
 15429
 59723
  3814
     6
[torch.DoubleTensor of size 11]

Validation accuracy:	0.08760469774217	
Grad norm	6.9833999913836	
    Loss 1362.0170980566	
    Loss 1396.7671317476	
    Loss 1427.5615824875	
    Loss 1401.7165896878	
    Loss 1400.2782491633	
    Loss 1399.9066802258	
    Loss 1406.1235171799	
    Loss 1386.8858849908	
    Loss 1425.6448245201	
    Loss 1412.1797771602	
    Loss 1430.0417917291	
    Loss 1411.776819184	
    Loss 1373.7201668882	
    Loss 1375.0276713139	
    Loss 1397.3737587414	
    Loss 1381.714780566	
    Loss 1398.3398747619	
    Loss 1425.8911506535	
    Loss 1410.9980208517	
    Loss 1394.753907241	
    Loss 1411.6018640879	
    Loss 1413.2154675887	
    Loss 1414.8825093995	
    Loss 1421.4306595579	
    Loss 1397.4644384252	
    Loss 1409.4995769995	
    Loss 1389.7047087778	
    Loss 1402.0507340626	
    Loss 1407.4313255485	
    Loss 1393.0869671723	
    Loss 1418.4599543406	
    Loss 1396.1586398118	
    Loss 1420.7749175746	
    Loss 1420.6237996495	
    Loss 1409.3377769613	
    Loss 1419.7033057093	
    Loss 1403.8526300597	
    Loss 1402.3493954834	
    Loss 1409.2661613816	
    Loss 1403.0722297405	
    Loss 1428.4760781542	
    Loss 1414.0617548473	
    Loss 1401.3723553659	
    Loss 1394.5956347718	
    Loss 1432.6639510057	
    Loss 1406.9735226472	
    Loss 1417.8867526172	
    Loss 1409.9999068083	
    Loss 1386.4687543047	
    Loss 1383.275703719	
    Loss 1377.0296525993	
    Loss 1418.444581916	
    Loss 1383.7735922082	
    Loss 1419.6495468449	
    Loss 1428.8350346706	
    Loss 1419.5899289	
    Loss 1406.3967226941	
    Loss 1419.7571694297	
    Loss 1424.1728282452	
    Loss 1419.1423862454	
    Loss 1391.9291535653	
    Loss 1401.1279437505	
    Loss 1415.1893685522	
    Loss 1398.2098171775	
    Loss 1408.939316078	
    Loss 1408.9801105446	
    Loss 1412.5197689699	
    Loss 1411.0873720668	
    Loss 1396.2017683277	
    Loss 1426.9105559862	
    Loss 1404.8665941966	
    Loss 1392.0082063801	
    Loss 1387.5833924773	
    Loss 1394.1129239931	
    Loss 1400.563441921	
    Loss 1392.9289200334	
    Loss 1408.4442035413	
    Loss 1385.9828472421	
    Loss 1404.6315706926	
    Loss 1427.8252974492	
    Loss 1403.0442480875	
    Loss 1421.332011398	
    Loss 1370.5336469579	
    Loss 1399.634414756	
    Loss 1418.6867818028	
    Loss 1392.0190896835	
    Loss 1355.7049608667	
    Loss 1391.3481386853	
    Loss 1409.8019034048	
    Loss 1424.4689130812	
    Loss 1413.7054393182	
    Loss 1398.601213872	
    Loss 1402.7755327411	
    Loss 1393.0522481422	
    Loss 1371.8764941324	
    Loss 1374.5829493044	
    Loss 1384.9002258908	
    Loss 1357.9931273288	
    Loss 1383.3893574867	
    Loss 1402.5395955026	
    Loss 1411.080108941	
    Loss 1403.857902384	
    Loss 1398.744811503	
    Loss 1412.1091906065	
    Loss 1410.9319912355	
    Loss 1432.7220955442	
    Loss 1398.6840274994	
    Loss 1376.9801816604	
    Loss 1411.0645545797	
    Loss 1368.2899293143	
    Loss 1388.6987966423	
    Loss 1396.2585196417	
    Loss 1385.073291338	
    Loss 1393.8186107165	
    Loss 1398.7871103201	
    Loss 1430.2566843521	
    Loss 1395.4380380425	
    Loss 1418.5085932197	
    Loss 1415.8672655687	
    Loss 1403.5821292772	
    Loss 1403.0466909665	
    Loss 1411.5043597045	
    Loss 1392.774190734	
    Loss 1400.6637575144	
    Loss 1394.4534715799	
    Loss 1394.2197300026	
    Loss 1400.3258855094	
    Loss 1429.2968135393	
    Loss 1424.4512893163	
    Loss 1412.3679021587	
    Loss 1395.8339658148	
    Loss 1380.467510322	
    Loss 1366.2612113836	
    Loss 1363.1053285224	
    Loss 1384.393391141	
    Loss 1406.3110287531	
    Loss 1414.1404013084	
    Loss 1419.2623818457	
    Loss 1427.4683007584	
    Loss 1421.0680509998	
    Loss 1389.091167648	
    Loss 1405.3156415065	
    Loss 1408.0475623507	
Epoch 5	
  8318
  1453
    24
     0
 43041
     0
     0
 15429
 59723
  3814
     6
[torch.DoubleTensor of size 11]

Validation accuracy:	0.08760469774217	
Grad norm	6.9833999913836	
    Loss 1362.0170980566	
    Loss 1396.7671317476	
    Loss 1427.5615824875	
    Loss 1401.7165896878	
    Loss 1400.2782491633	
    Loss 1399.9066802258	
    Loss 1406.1235171799	
    Loss 1386.8858849908	
    Loss 1425.6448245201	
    Loss 1412.1797771602	
    Loss 1430.0417917291	
    Loss 1411.776819184	
    Loss 1373.7201668882	
    Loss 1375.0276713139	
    Loss 1397.3737587414	
    Loss 1381.714780566	
    Loss 1398.3398747619	
    Loss 1425.8911506535	
    Loss 1410.9980208517	
    Loss 1394.753907241	
    Loss 1411.6018640879	
    Loss 1413.2154675887	
    Loss 1414.8825093995	
    Loss 1421.4306595579	
    Loss 1397.4644384252	
    Loss 1409.4995769995	
    Loss 1389.7047087778	
    Loss 1402.0507340626	
    Loss 1407.4313255485	
    Loss 1393.0869671723	
    Loss 1418.4599543406	
    Loss 1396.1586398118	
    Loss 1420.7749175746	
    Loss 1420.6237996495	
    Loss 1409.3377769613	
    Loss 1419.7033057093	
    Loss 1403.8526300597	
    Loss 1402.3493954834	
    Loss 1409.2661613816	
    Loss 1403.0722297405	
    Loss 1428.4760781542	
    Loss 1414.0617548473	
    Loss 1401.3723553659	
    Loss 1394.5956347718	
    Loss 1432.6639510057	
    Loss 1406.9735226472	
    Loss 1417.8867526172	
    Loss 1409.9999068083	
    Loss 1386.4687543047	
    Loss 1383.275703719	
    Loss 1377.0296525993	
    Loss 1418.444581916	
    Loss 1383.7735922082	
    Loss 1419.6495468449	
    Loss 1428.8350346706	
    Loss 1419.5899289	
    Loss 1406.3967226941	
    Loss 1419.7571694297	
    Loss 1424.1728282452	
