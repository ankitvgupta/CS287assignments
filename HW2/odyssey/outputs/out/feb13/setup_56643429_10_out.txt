[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.5	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
  1822
  5193
  3481
  1859
  1320
  4201
  3129
  1794
  3399
 10195
  1686
  1329
  4058
  4253
  2132
  5479
  3112
  3192
  2887
  4041
   970
  2040
  1422
  3410
  1181
  4205
  2202
  1202
  1640
  2541
  1218
  1715
  5503
  2001
  1932
  1018
  3059
  2135
  1740
  9357
  1614
  1974
  7142
  1091
   934
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02198652585579	
Grad norm	0	
    Loss 2279671.2806734	
    Loss 2263646.8873725	
    Loss 2247744.6457617	
    Loss 2231956.5827892	
    Loss 2216279.6967167	
    Loss 2200717.4322779	
    Loss 2185263.3134574	
    Loss 2169920.2239681	
    Loss 2154685.2241616	
    Loss 2139559.2010412	
    Loss 2124539.2510953	
    Loss 2109627.4812411	
    Loss 2094817.8444009	
    Loss 2080116.8030835	
    Loss 2065519.0586154	
    Loss 2051021.1994582	
    Loss 2036626.5819038	
    Loss 2022332.8786894	
    Loss 2008140.4992836	
    Loss 1994049.2221673	
    Loss 1980054.1011591	
    Loss 1966159.3984782	
    Loss 1952360.2441764	
    Loss 1938659.311347	
    Loss 1925057.3385686	
    Loss 1911549.6130822	
    Loss 1898140.1887612	
    Loss 1884823.1903077	
    Loss 1871598.9001118	
    Loss 1858469.2350908	
    Loss 1845429.9177178	
    Loss 1832481.530084	
    Loss 1819623.0197439	
    Loss 1806855.6732385	
    Loss 1794179.9649081	
    Loss 1781590.9033276	
    Loss 1769093.3410668	
    Loss 1756683.1159455	
    Loss 1744358.0453079	
    Loss 1732122.0451095	
    Loss 1719971.6189762	
    Loss 1707903.8148341	
    Loss 1695924.2221112	
    Loss 1684028.0705913	
    Loss 1672212.9590367	
    Loss 1660482.5026329	
    Loss 1648834.9490706	
    Loss 1637269.208214	
    Loss 1625785.9119276	
    Loss 1614383.529851	
    Loss 1603060.3583087	
    Loss 1591814.1699191	
    Loss 1580649.4429024	
    Loss 1569563.2136961	
    Loss 1558555.5124531	
    Loss 1547622.6808902	
    Loss 1536767.7696886	
    Loss 1525988.7327786	
    Loss 1515286.6280233	
    Loss 1504659.5660298	
    Loss 1494106.7036493	
    Loss 1483629.8214607	
    Loss 1473224.9579398	
    Loss 1462894.1086446	
    Loss 1452634.1622404	
    Loss 1442447.9178549	
    Loss 1432331.4079191	
    Loss 1422285.9741959	
    Loss 1412310.4026066	
    Loss 1402404.7199419	
    Loss 1392571.1692279	
    Loss 1382806.7067861	
    Loss 1373110.9694827	
    Loss 1363482.6558188	
    Loss 1353922.6757193	
    Loss 1344429.0818249	
    Loss 1335002.6142881	
    Loss 1325641.9450568	
    Loss 1316346.7722475	
    Loss 1307115.4730248	
    Loss 1297949.3953139	
    Loss 1288847.5815504	
    Loss 1279812.7620139	
    Loss 1270839.1626877	
    Loss 1261928.5103505	
    Loss 1253081.4144441	
    Loss 1244296.9363122	
    Loss 1235572.738927	
    Loss 1226910.1225778	
    Loss 1218306.508003	
    Loss 1209763.3611711	
    Loss 1201281.9410966	
    Loss 1192860.1898424	
    Loss 1184498.6343171	
    Loss 1176194.5523623	
    Loss 1167947.0765565	
    Loss 1159756.8092669	
    Loss 1151627.8594764	
    Loss 1143552.9109126	
    Loss 1135533.2860522	
    Loss 1127569.7405905	
    Loss 1119662.8548154	
    Loss 1111812.5217743	
    Loss 1104016.3255336	
    Loss 1096274.5830082	
    Loss 1088589.7823791	
    Loss 1080958.0411666	
    Loss 1073379.544475	
    Loss 1065854.2586384	
    Loss 1058384.6440842	
    Loss 1050964.117247	
    Loss 1043596.3932342	
    Loss 1036280.8456429	
    Loss 1029014.7083506	
    Loss 1021800.5562532	
    Loss 1014635.5783861	
    Loss 1007523.5754837	
    Loss 1000461.1773323	
    Loss 993448.52919667	
    Loss 986484.18835981	
    Loss 979567.65766702	
    Loss 972700.48090859	
    Loss 965882.56631084	
    Loss 959112.29128535	
    Loss 952389.46831468	
    Loss 945713.77925791	
    Loss 939084.40339697	
    Loss 932500.31647465	
    Loss 925963.5573141	
    Loss 919472.99016482	
    Loss 913028.61639509	
    Loss 906629.9545781	
    Loss 900276.30905411	
    Loss 893965.55668529	
    Loss 887699.89461158	
    Loss 881476.74346918	
    Loss 875298.83800058	
    Loss 869161.65697384	
    Loss 863068.50279004	
    Loss 857018.56022927	
    Loss 851011.87669227	
    Loss 845046.68084489	
    Loss 839123.55441644	
Epoch 2	
 80656
  4696
   995
   316
 10542
    30
    42
  7621
 21232
  4058
  1135
    32
    29
    31
    19
   298
    44
    32
[torch.DoubleTensor of size 18]

Validation accuracy:	0.097300619082302	
Grad norm	159.72033942836	
    Loss 835590.56867243	
    Loss 829734.05989136	
    Loss 823918.05150153	
    Loss 818142.370445	
    Loss 812407.72291712	
    Loss 806714.34920774	
    Loss 801060.54467054	
    Loss 795445.16937855	
    Loss 789869.61958278	
    Loss 784334.00043531	
    Loss 778837.38915845	
    Loss 773379.43747121	
    Loss 767958.92637893	
    Loss 762578.01320111	
    Loss 757234.43821774	
    Loss 751927.01047843	
    Loss 746656.60320965	
    Loss 741423.46885884	
    Loss 736227.30118358	
    Loss 731068.13317464	
    Loss 725943.85767413	
    Loss 720855.91150362	
    Loss 715802.819574	
    Loss 710785.86313557	
    Loss 705804.96551198	
    Loss 700858.51534809	
    Loss 695947.92210808	
    Loss 691071.29917946	
    Loss 686228.68800571	
    Loss 681420.43934276	
    Loss 676645.00167904	
    Loss 671902.33506106	
    Loss 667192.95019291	
    Loss 662516.79260805	
    Loss 657874.16643314	
    Loss 653262.64188474	
    Loss 648685.29634183	
    Loss 644139.47727666	
    Loss 639625.03790442	
    Loss 635143.05586495	
    Loss 630692.11158626	
    Loss 626271.46492463	
    Loss 621883.46582441	
    Loss 617526.02326099	
    Loss 613197.89634877	
    Loss 608900.74865187	
    Loss 604634.04055793	
    Loss 600397.37380294	
    Loss 596190.76079165	
    Loss 592013.80729474	
    Loss 587865.95104522	
    Loss 583745.88498174	
    Loss 579655.89196013	
    Loss 575594.63060694	
    Loss 571561.9002229	
    Loss 567556.38877421	
    Loss 563579.65721787	
    Loss 559630.79970923	
    Loss 555710.05198777	
    Loss 551816.67914995	
    Loss 547950.18277311	
    Loss 544111.96950986	
    Loss 540299.35427728	
    Loss 536514.20801166	
    Loss 532754.80456065	
    Loss 529022.44927761	
    Loss 525315.49001925	
    Loss 521634.7888143	
    Loss 517979.25301806	
    Loss 514349.22548905	
    Loss 510746.18235825	
    Loss 507168.36020661	
    Loss 503615.99934408	
    Loss 500087.87090162	
    Loss 496584.89841986	
    Loss 493106.0963533	
    Loss 489651.68852673	
    Loss 486221.73992105	
    Loss 482815.62682178	
    Loss 479432.78276324	
    Loss 476073.5924157	
    Loss 472738.13562405	
    Loss 469427.46584605	
    Loss 466139.00333666	
    Loss 462873.72784222	
    Loss 459631.72499594	
    Loss 456412.62785336	
    Loss 453215.43368325	
    Loss 450040.98293657	
    Loss 446887.9483976	
    Loss 443756.86998254	
    Loss 440648.52558428	
    Loss 437562.42868397	
    Loss 434498.2544936	
    Loss 431454.72920161	
    Loss 428432.11376317	
    Loss 425430.37719456	
    Loss 422451.22403867	
    Loss 419491.57781984	
    Loss 416552.08460442	
    Loss 413633.17264356	
    Loss 410734.90206851	
    Loss 407857.55384341	
    Loss 405000.0590758	
    Loss 402162.37726408	
    Loss 399345.97427303	
    Loss 396548.91205689	
    Loss 393771.07801547	
    Loss 391012.88606741	
    Loss 388275.19340691	
    Loss 385555.46212938	
    Loss 382854.85766395	
    Loss 380173.61473891	
    Loss 377510.18868049	
    Loss 374866.24820739	
    Loss 372240.12664764	
    Loss 369633.42318649	
    Loss 367044.92556955	
    Loss 364474.50598063	
    Loss 361921.64650708	
    Loss 359386.39999269	
    Loss 356869.43911316	
    Loss 354370.43540618	
    Loss 351888.90540194	
    Loss 349424.71876669	
    Loss 346977.63813937	
    Loss 344547.56381942	
    Loss 342134.07653272	
    Loss 339738.16305122	
    Loss 337358.90556219	
    Loss 334996.5219989	
    Loss 332651.09934287	
    Loss 330322.04628216	
    Loss 328008.64030189	
    Loss 325711.85559687	
    Loss 323430.57641172	
    Loss 321166.11763098	
    Loss 318915.98507571	
    Loss 316682.44709466	
    Loss 314464.69887046	
    Loss 312262.85658477	
    Loss 310076.28162445	
    Loss 307905.08682743	
Epoch 3	
 106836
   1103
     77
      5
   4964
      0
      1
   2599
  15173
    964
     80
      0
      0
      0
      0
      6
[torch.DoubleTensor of size 16]

Validation accuracy:	0.098855911629036	
Grad norm	98.713702785294	
    Loss 306609.90752737	
    Loss 304463.07572223	
    Loss 302331.00863348	
    Loss 300213.74831857	
    Loss 298111.44850554	
    Loss 296024.30472766	
    Loss 293951.88555926	
    Loss 291893.18928973	
    Loss 289849.30259542	
    Loss 287820.01165104	
    Loss 285805.22266534	
    Loss 283804.3576913	
    Loss 281817.11798142	
    Loss 279844.37950931	
    Loss 277885.36645025	
    Loss 275939.6263409	
    Loss 274007.34914867	
    Loss 272088.78736259	
    Loss 270183.84423306	
    Loss 268292.36293319	
    Loss 266413.85368685	
    Loss 264548.58957817	
    Loss 262695.95826382	
    Loss 260856.64632589	
    Loss 259030.45828456	
    Loss 257216.83735104	
    Loss 255416.44016231	
    Loss 253628.64515632	
    Loss 251853.37583207	
    Loss 250090.55237626	
    Loss 248339.87809555	
    Loss 246600.92018806	
    Loss 244874.18614386	
    Loss 243159.64642672	
    Loss 241457.57326263	
    Loss 239766.61913684	
    Loss 238088.54504905	
    Loss 236421.7162302	
    Loss 234766.57233959	
    Loss 233123.27787857	
    Loss 231491.30180643	
    Loss 229870.40444756	
    Loss 228261.55960415	
    Loss 226663.91960736	
    Loss 225077.10175957	
    Loss 223501.50810913	
    Loss 221937.1685974	
    Loss 220383.82252437	
    Loss 218841.46287534	
    Loss 217309.90786735	
    Loss 215789.06341746	
    Loss 214278.42247422	
    Loss 212778.64980212	
    Loss 211289.72635367	
    Loss 209811.19406946	
    Loss 208342.62280774	
    Loss 206884.56466284	
    Loss 205436.73713788	
    Loss 203999.28747267	
    Loss 202571.92216285	
    Loss 201154.11852768	
    Loss 199746.92595267	
    Loss 198348.894994	
    Loss 196960.96018377	
    Loss 195582.52577059	
    Loss 194213.99050896	
    Loss 192854.69569587	
    Loss 191505.13153349	
    Loss 190164.64566411	
    Loss 188833.51786471	
    Loss 187512.31636915	
    Loss 186200.42721117	
    Loss 184897.90868361	
    Loss 183604.18636038	
    Loss 182319.70258771	
    Loss 181044.10780243	
    Loss 179777.31236891	
    Loss 178519.60958563	
    Loss 177270.65614166	
    Loss 176030.2736849	
    Loss 174798.33461743	
    Loss 173575.31461969	
    Loss 172361.37680954	
    Loss 171155.51749475	
    Loss 169958.27728032	
    Loss 168769.36210499	
    Loss 167588.85628578	
    Loss 166416.4317095	
    Loss 165252.42679648	
    Loss 164096.25669131	
    Loss 162948.11224722	
    Loss 161808.21303902	
    Loss 160676.68522019	
    Loss 159553.11256002	
    Loss 158436.86238019	
    Loss 157328.40133071	
    Loss 156227.61592803	
    Loss 155135.09077907	
    Loss 154049.65803821	
    Loss 152971.55632696	
    Loss 151901.03821106	
    Loss 150838.03285925	
    Loss 149782.83341233	
    Loss 148734.84784832	
    Loss 147694.18994227	
    Loss 146661.45977106	
    Loss 145635.74550473	
    Loss 144616.99751819	
    Loss 143605.53706261	
    Loss 142601.44440222	
    Loss 141604.10607509	
    Loss 140613.61146059	
    Loss 139630.41643968	
    Loss 138653.63364247	
    Loss 137684.11754981	
    Loss 136721.15676279	
    Loss 135765.19894586	
    Loss 134816.01389235	
    Loss 133873.30524738	
    Loss 132936.97305883	
    Loss 132007.14689181	
    Loss 131084.15909322	
    Loss 130167.69685224	
    Loss 129257.64956176	
    Loss 128354.0023364	
    Loss 127456.52995331	
    Loss 126565.26455425	
    Loss 125680.07452088	
    Loss 124801.58736734	
    Loss 123928.98620963	
    Loss 123062.48540972	
    Loss 122202.37427121	
    Loss 121348.12529292	
    Loss 120499.61878346	
    Loss 119657.31840153	
    Loss 118820.62459338	
    Loss 117990.24254046	
    Loss 117164.7666699	
    Loss 116345.65906158	
    Loss 115532.30147453	
    Loss 114724.76972539	
    Loss 113922.93493508	
    Loss 113126.73376991	
Epoch 4	
 122834
     98
      0
      0
   1117
      0
      0
    350
   7335
     74
[torch.DoubleTensor of size 10]

Validation accuracy:	0.098764870114105	
Grad norm	61.82955575087	
    Loss 112651.66897367	
    Loss 111864.3424324	
    Loss 111082.4404706	
    Loss 110306.01315271	
    Loss 109534.97909558	
    Loss 108769.48325246	
    Loss 108009.48350812	
    Loss 107254.4336439	
    Loss 106504.8831175	
    Loss 105760.6389517	
    Loss 105021.78131973	
    Loss 104287.94108806	
    Loss 103558.96558956	
    Loss 102835.36088201	
    Loss 102116.82958625	
    Loss 101403.18706607	
    Loss 100694.48924839	
    Loss 99990.81073839	
    Loss 99292.173997026	
    Loss 98598.354293389	
    Loss 97909.478026843	
    Loss 97225.447520031	
    Loss 96545.914578618	
    Loss 95871.305973247	
    Loss 95201.390755609	
    Loss 94536.068391687	
    Loss 93875.659779094	
    Loss 93219.961578879	
    Loss 92568.914331362	
    Loss 91922.353330709	
    Loss 91280.37686074	
    Loss 90642.47140623	
    Loss 90009.014140987	
    Loss 89380.078695018	
    Loss 88755.844999248	
    Loss 88135.518253064	
    Loss 87520.091526474	
    Loss 86908.596053619	
    Loss 86301.550686451	
    Loss 85698.78901865	
    Loss 85100.168066925	
    Loss 84505.588851169	
    Loss 83915.398183123	
    Loss 83329.381918504	
    Loss 82747.416251352	
    Loss 82169.462020759	
    Loss 81595.693275461	
    Loss 81025.937475317	
    Loss 80460.220729754	
    Loss 79898.346974565	
    Loss 79340.443314819	
    Loss 78786.35271548	
    Loss 78236.048151844	
    Loss 77690.030996207	
    Loss 77147.776776781	
    Loss 76609.219629561	
    Loss 76074.424642466	
    Loss 75543.376979174	
    Loss 75016.220288399	
    Loss 74492.828264902	
    Loss 73972.717003429	
    Loss 73456.658931355	
    Loss 72943.863822643	
    Loss 72434.703336471	
    Loss 71929.142147534	
    Loss 71427.178788087	
    Loss 70928.576605363	
    Loss 70433.609037374	
    Loss 69941.879759733	
    Loss 69453.57584652	
    Loss 68968.904132065	
    Loss 68487.678760996	
    Loss 68009.895240751	
    Loss 67535.315256972	
    Loss 67064.136284609	
    Loss 66596.281691592	
    Loss 66131.529745415	
    Loss 65670.167288495	
    Loss 65212.050820227	
    Loss 64757.142334179	
    Loss 64305.13581982	
    Loss 63856.579632757	
    Loss 63411.311977899	
    Loss 62968.975573901	
    Loss 62529.914356713	
    Loss 62093.67370266	
    Loss 61660.539122765	
    Loss 61230.456862101	
    Loss 60803.468241746	
    Loss 60379.39227536	
    Loss 59958.303386987	
    Loss 59540.100852552	
    Loss 59125.149012015	
    Loss 58713.054794975	
    Loss 58303.446909977	
    Loss 57896.780477107	
    Loss 57492.976805823	
    Loss 57092.149637246	
    Loss 56693.926320351	
    Loss 56298.350539136	
    Loss 55905.582320234	
    Loss 55515.540322566	
    Loss 55128.447261855	
    Loss 54743.949903911	
    Loss 54362.237983333	
    Loss 53983.460393004	
    Loss 53607.191525571	
    Loss 53233.475558434	
    Loss 52862.457306698	
    Loss 52493.935824561	
    Loss 52128.101622377	
    Loss 51764.647030429	
    Loss 51404.05019801	
    Loss 51045.737186729	
    Loss 50690.127401849	
    Loss 50336.996560904	
    Loss 49986.279305527	
    Loss 49638.14040197	
    Loss 49292.258097336	
    Loss 48948.673791185	
    Loss 48607.536571228	
    Loss 48268.997227265	
    Loss 47932.798367208	
    Loss 47598.953117163	
    Loss 47267.489030893	
    Loss 46938.2400869	
    Loss 46611.199514664	
    Loss 46286.395613526	
    Loss 45964.287358534	
    Loss 45644.153698496	
    Loss 45326.180885677	
    Loss 45010.680553535	
    Loss 44697.219463961	
    Loss 44385.886430269	
    Loss 44076.928426197	
    Loss 43769.952442976	
    Loss 43465.378681619	
    Loss 43162.375209185	
    Loss 42861.913035868	
    Loss 42563.546241799	
    Loss 42267.282972389	
    Loss 41973.209563044	
    Loss 41681.201318368	
Epoch 5	
 129332
      1
      0
      0
     69
      0
      0
     15
   2390
      1
[torch.DoubleTensor of size 10]

Validation accuracy:	0.098863498421947	
Grad norm	39.521394325919	
    Loss 41506.875556254	
    Loss 41218.050137546	
    Loss 40931.263571224	
    Loss 40646.510396713	
    Loss 40363.647413066	
    Loss 40082.784758128	
    Loss 39804.014016521	
    Loss 39527.035817985	
    Loss 39252.109213899	
    Loss 38979.091422138	
    Loss 38708.082089578	
    Loss 38438.851197316	
    Loss 38171.300156143	
    Loss 37905.773651801	
    Loss 37642.134665355	
    Loss 37380.309762874	
    Loss 37120.345779847	
    Loss 36862.20275089	
    Loss 36605.940650117	
    Loss 36351.319580701	
    Loss 36098.687177211	
    Loss 35847.839827674	
    Loss 35598.517806413	
    Loss 35351.031432283	
    Loss 35105.148119494	
    Loss 34860.952024016	
    Loss 34618.5998483	
    Loss 34378.048539793	
    Loss 34139.25100286	
    Loss 33902.053545537	
    Loss 33666.657688663	
    Loss 33432.563788874	
    Loss 33200.059557417	
    Loss 32969.267761987	
    Loss 32740.294694162	
    Loss 32512.635830113	
    Loss 32286.876996692	
    Loss 32062.422698564	
    Loss 31839.758877839	
    Loss 31618.584824128	
    Loss 31398.942284588	
    Loss 31180.776282528	
    Loss 30964.127100047	
    Loss 30749.110771494	
    Loss 30535.655773415	
    Loss 30323.587794083	
    Loss 30113.086894894	
    Loss 29904.047269072	
    Loss 29696.51789068	
    Loss 29490.262711119	
    Loss 29285.508107774	
    Loss 29082.227873766	
    Loss 28880.13939924	
    Loss 28679.905812231	
    Loss 28481.011160845	
    Loss 28283.539166235	
    Loss 28087.332788887	
    Loss 27892.498058379	
    Loss 27699.172515032	
    Loss 27507.291179186	
    Loss 27316.431344135	
    Loss 27127.179174299	
    Loss 26939.07417901	
    Loss 26752.191385353	
    Loss 26566.767618784	
    Loss 26382.628198528	
    Loss 26199.71080709	
    Loss 26018.180904393	
    Loss 25837.767791999	
    Loss 25658.587669645	
    Loss 25480.740480646	
    Loss 25304.158517587	
    Loss 25128.835673701	
    Loss 24954.68747613	
    Loss 24781.803463276	
    Loss 24610.214486595	
    Loss 24439.646096761	
    Loss 24270.348810881	
    Loss 24102.282053211	
    Loss 23935.468010741	
    Loss 23769.545686486	
    Loss 23605.037383663	
    Loss 23441.682285729	
    Loss 23279.382400114	
    Loss 23118.398429482	
    Loss 22958.212548872	
    Loss 22799.195994198	
    Loss 22641.397892183	
    Loss 22484.696575568	
    Loss 22329.125575781	
    Loss 22174.720443504	
    Loss 22021.22112676	
    Loss 21869.085714735	
    Loss 21717.951218536	
    Loss 21567.550416068	
    Loss 21418.300055791	
    Loss 21270.15084835	
    Loss 21123.027311978	
    Loss 20976.889069465	
    Loss 20831.691237015	
    Loss 20687.540565848	
    Loss 20544.372672195	
    Loss 20402.341084643	
    Loss 20261.230277841	
    Loss 20121.246710955	
    Loss 19982.317585673	
    Loss 19844.254325934	
    Loss 19707.165087724	
    Loss 19571.051704968	
    Loss 19435.629610195	
    Loss 19301.419013451	
    Loss 19167.972428603	
    Loss 19035.743874794	
    Loss 18904.306756884	
    Loss 18773.867090954	
    Loss 18644.427608527	
    Loss 18515.701674804	
    Loss 18388.009494461	
    Loss 18261.049631209	
    Loss 18134.900970757	
    Loss 18009.724862724	
    Loss 17885.569976164	
    Loss 17762.213826359	
    Loss 17639.71982561	
    Loss 17518.121496577	
    Loss 17397.312617766	
    Loss 17277.224853774	
    Loss 17157.977923813	
    Loss 17039.939945041	
    Loss 16922.460518702	
    Loss 16805.706931907	
    Loss 16689.963931392	
    Loss 16574.878633532	
    Loss 16460.60322487	
    Loss 16347.284204452	
    Loss 16234.624970586	
    Loss 16122.899064794	
    Loss 16011.582784185	
    Loss 15901.364914663	
    Loss 15791.920383266	
    Loss 15683.202683911	
    Loss 15575.383635276	
    Loss 15468.321725415	
Epoch 6	
 131022
      0
      0
      0
      0
      0
      0
      0
    786
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098954539936878	
Grad norm	26.036218585489	
    Loss 15404.319859678	
    Loss 15298.342092612	
    Loss 15193.184695069	
    Loss 15088.779660481	
    Loss 14984.9856132	
    Loss 14881.88820123	
    Loss 14779.632567813	
    Loss 14678.027331001	
    Loss 14577.19904404	
    Loss 14477.038044916	
    Loss 14377.63476176	
    Loss 14278.826278526	
    Loss 14180.554600493	
    Loss 14083.073783075	
    Loss 13986.300802666	
    Loss 13890.215754972	
    Loss 13794.876701357	
    Loss 13700.186118977	
    Loss 13606.207019347	
    Loss 13512.699658757	
    Loss 13420.090128621	
    Loss 13328.152327369	
    Loss 13236.655565065	
    Loss 13145.85666769	
    Loss 13055.530052304	
    Loss 12965.835476523	
    Loss 12876.840922218	
    Loss 12788.576387113	
    Loss 12700.994995228	
    Loss 12613.970217233	
    Loss 12527.719532131	
    Loss 12441.775083522	
    Loss 12356.370865884	
    Loss 12271.648767151	
    Loss 12187.665088656	
    Loss 12104.075190126	
    Loss 12021.245625142	
    Loss 11938.785835333	
    Loss 11857.136441269	
    Loss 11775.939163031	
    Loss 11695.328873296	
    Loss 11615.265268752	
    Loss 11535.629729694	
    Loss 11456.71527098	
    Loss 11378.444575127	
    Loss 11300.609597193	
    Loss 11223.36653197	
    Loss 11146.655070084	
    Loss 11070.534732276	
    Loss 10994.740645449	
    Loss 10919.543190132	
    Loss 10844.958555626	
    Loss 10770.621211343	
    Loss 10697.227648889	
    Loss 10624.28793416	
    Loss 10551.946099584	
    Loss 10479.943199407	
    Loss 10408.444883065	
    Loss 10337.57777843	
    Loss 10267.298724565	
    Loss 10197.239266168	
    Loss 10127.868479837	
    Loss 10058.890948753	
    Loss 9990.2289677134	
    Loss 9922.2523305555	
    Loss 9854.7123750635	
    Loss 9787.617370769	
    Loss 9721.080750048	
    Loss 9654.8924882231	
    Loss 9589.1215049071	
    Loss 9523.8503368979	
    Loss 9459.0261174734	
    Loss 9394.6560973396	
    Loss 9330.7271530423	
    Loss 9267.2805789136	
    Loss 9204.3903797433	
    Loss 9141.7596532888	
    Loss 9079.609811983	
    Loss 9017.9502742966	
    Loss 8956.8327153951	
    Loss 8895.8821963933	
    Loss 8835.5790189158	
    Loss 8775.6413744241	
    Loss 8716.0766313506	
    Loss 8657.1103117279	
    Loss 8598.1996861787	
    Loss 8539.7524638664	
    Loss 8481.8559314299	
    Loss 8424.2983803435	
    Loss 8367.2282931158	
    Loss 8310.6689733035	
    Loss 8254.2817189884	
    Loss 8198.5631816755	
    Loss 8143.1716318527	
    Loss 8087.881771535	
    Loss 8033.0799001092	
    Loss 7978.7325122938	
    Loss 7924.6886319461	
    Loss 7871.0502377177	
    Loss 7817.7332519955	
    Loss 7764.8119563794	
    Loss 7712.2396996621	
    Loss 7660.1198398112	
    Loss 7608.3194625823	
    Loss 7557.0399811461	
    Loss 7506.0978500959	
    Loss 7455.4264217023	
    Loss 7405.1713492229	
    Loss 7355.2370714032	
    Loss 7305.3237392776	
    Loss 7256.0884354102	
    Loss 7207.0368138442	
    Loss 7158.5948423141	
    Loss 7110.4084542406	
    Loss 7062.5778860444	
    Loss 7015.2108084096	
    Loss 6967.9254931031	
    Loss 6921.1028671179	
    Loss 6874.4669582079	
    Loss 6828.1018836611	
    Loss 6782.1779473729	
    Loss 6736.6851679539	
    Loss 6691.4191940268	
    Loss 6646.4679081809	
    Loss 6601.8592973848	
    Loss 6557.531726882	
    Loss 6513.3689032712	
    Loss 6469.5423007506	
    Loss 6426.3692676158	
    Loss 6383.2441296681	
    Loss 6340.3248081709	
    Loss 6297.8655013087	
    Loss 6255.5691170972	
    Loss 6213.6033564671	
    Loss 6172.0642052719	
    Loss 6130.7051714291	
    Loss 6089.7195921317	
    Loss 6048.7531142432	
    Loss 6008.3343979971	
    Loss 5968.2155028964	
    Loss 5928.3122085991	
    Loss 5888.8296236273	
    Loss 5849.6247665813	
Epoch 7	
 131345
      0
      0
      0
      0
      0
      0
      0
    463
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099189730517116	
Grad norm	17.895428077139	
    Loss 5826.1050818396	
    Loss 5787.2065906495	
    Loss 5748.6960164084	
    Loss 5710.4554487738	
    Loss 5672.360167765	
    Loss 5634.4795166689	
    Loss 5596.9830999286	
    Loss 5559.7240427218	
    Loss 5522.7700786889	
    Loss 5486.030390398	
    Loss 5449.584873722	
    Loss 5413.2999214011	
    Loss 5377.1492546575	
    Loss 5341.3318861816	
    Loss 5305.7818933529	
    Loss 5270.5084588593	
    Loss 5235.5797570794	
    Loss 5200.8713554896	
    Loss 5166.4390148692	
    Loss 5132.0466061785	
    Loss 5098.1481753527	
    Loss 5064.5180647619	
    Loss 5030.9341826006	
    Loss 4997.6284711017	
    Loss 4964.3816999195	
    Loss 4931.3863630694	
    Loss 4898.6593806406	
    Loss 4866.2691169279	
    Loss 4834.1638867485	
    Loss 4802.2404363524	
    Loss 4770.7097804651	
    Loss 4739.1322720583	
    Loss 4707.7096471214	
    Loss 4676.5910373968	
    Loss 4645.8006051931	
    Loss 4615.0847165828	
    Loss 4584.6885191469	
    Loss 4554.3353142515	
    Loss 4524.4296978573	
    Loss 4494.5879741668	
    Loss 4464.994070196	
    Loss 4435.6129592261	
    Loss 4406.2392792646	
    Loss 4377.2638560792	
    Loss 4348.5928521814	
    Loss 4320.0148332356	
    Loss 4291.6631214804	
    Loss 4263.5058758292	
    Loss 4235.6066120717	
    Loss 4207.6834700954	
    Loss 4180.0264796813	
    Loss 4152.6624021258	
    Loss 4125.2044677935	
    Loss 4098.344596846	
    Loss 4071.6165640469	
    Loss 4045.188419273	
    Loss 4018.752890577	
    Loss 3992.5074116925	
    Loss 3966.5678175778	
    Loss 3940.8982371629	
    Loss 3915.1700722764	
    Loss 3889.7814030698	
    Loss 3864.5208612081	
    Loss 3839.2314680807	
    Loss 3814.3494536954	
    Loss 3789.5947239461	
    Loss 3765.0036707145	
    Loss 3740.6641801178	
    Loss 3716.3966654492	
    Loss 3692.2400341759	
    Loss 3668.284335883	
    Loss 3644.464756446	
    Loss 3620.8016472368	
    Loss 3597.3163003366	
    Loss 3574.0275387332	
    Loss 3551.027661726	
    Loss 3528.009014384	
    Loss 3505.1739803156	
    Loss 3482.5567704905	
    Loss 3460.2244671886	
    Loss 3437.8006592502	
    Loss 3415.7309240415	
    Loss 3393.7355522267	
    Loss 3371.866130567	
    Loss 3350.3328921463	
    Loss 3328.5830407449	
    Loss 3307.0399963361	
    Loss 3285.8057580432	
    Loss 3264.6174973337	
    Loss 3243.686666034	
    Loss 3223.0334529321	
    Loss 3202.278850767	
    Loss 3181.936744716	
    Loss 3161.6785443931	
    Loss 3141.2962257326	
    Loss 3121.1561184263	
    Loss 3101.2326059957	
    Loss 3081.3439601136	
    Loss 3061.6556288998	
    Loss 3042.0663654644	
    Loss 3022.6304055856	
    Loss 3003.3138940677	
    Loss 2984.1876784333	
    Loss 2965.168601322	
    Loss 2946.4484824155	
    Loss 2927.7864621597	
    Loss 2909.1817186929	
    Loss 2890.800999192	
    Loss 2872.4867348999	
    Loss 2853.9435931789	
    Loss 2835.8870952236	
    Loss 2817.8086051463	
    Loss 2800.1120265249	
    Loss 2782.481391532	
    Loss 2764.9623657076	
    Loss 2747.7121584602	
    Loss 2730.3078661052	
    Loss 2713.1538651335	
    Loss 2695.9940958523	
    Loss 2678.9099874266	
    Loss 2662.0779908299	
    Loss 2645.4538035397	
    Loss 2628.8425173611	
    Loss 2612.3450790997	
    Loss 2595.9823958602	
    Loss 2579.7224042744	
    Loss 2563.4173191649	
    Loss 2547.2675974063	
    Loss 2531.5621593407	
    Loss 2515.7213733677	
    Loss 2499.8995932123	
    Loss 2484.3278038911	
    Loss 2468.7442257652	
    Loss 2453.3180809637	
    Loss 2438.1183262187	
    Loss 2422.9262002186	
    Loss 2407.8896825095	
    Loss 2392.7491886992	
    Loss 2377.9424709706	
    Loss 2363.2689096988	
    Loss 2348.619554028	
    Loss 2334.2127227198	
    Loss 2319.9081485635	
Epoch 8	
 131412
      0
      0
      0
      0
      0
      0
      0
    396
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099356639961156	
Grad norm	12.994873299369	
    Loss 2311.2457892471	
    Loss 2296.9570626172	
    Loss 2282.9066620179	
    Loss 2268.9417347311	
    Loss 2254.9532511393	
    Loss 2241.0003096384	
    Loss 2227.264718312	
    Loss 2213.6179055316	
    Loss 2200.1007897643	
    Loss 2186.6331078521	
    Loss 2173.2874444731	
    Loss 2159.9421816411	
    Loss 2146.5939935493	
    Loss 2133.4081617284	
    Loss 2120.3230354905	
    Loss 2107.3649097008	
    Loss 2094.6084442881	
    Loss 2081.9167574549	
    Loss 2069.3373502427	
    Loss 2056.6377856096	
    Loss 2044.2821576769	
    Loss 2032.0496579728	
    Loss 2019.7199999242	
    Loss 2007.5134862105	
    Loss 1995.2149408947	
    Loss 1983.0321511462	
    Loss 1970.9517721594	
    Loss 1959.0640971139	
    Loss 1947.3119916508	
    Loss 1935.6087136832	
    Loss 1924.1555923808	
    Loss 1912.5335086382	
    Loss 1900.9246312541	
    Loss 1889.4810177244	
    Loss 1878.2066768989	
    Loss 1866.9008141798	
    Loss 1855.7397639552	
    Loss 1844.5112463714	
    Loss 1833.5949568217	
    Loss 1822.5957653567	
    Loss 1811.7243655539	
    Loss 1800.947814269	
    Loss 1790.0115439441	
    Loss 1779.3618210041	
    Loss 1768.8904406427	
    Loss 1758.3897979511	
    Loss 1747.9771465573	
    Loss 1737.6376092216	
    Loss 1727.4359228652	
    Loss 1717.0807894905	
    Loss 1706.871184795	
    Loss 1696.8343658026	
    Loss 1686.5819598194	
    Loss 1676.7940669913	
    Loss 1667.0216392892	
    Loss 1657.442198591	
    Loss 1647.7244761562	
    Loss 1638.083906204	
    Loss 1628.6277340288	
    Loss 1619.3221639513	
    Loss 1609.8655062665	
    Loss 1600.6135732642	
    Loss 1591.3986718212	
    Loss 1582.0219275147	
    Loss 1572.9537786335	
    Loss 1563.9007935036	
    Loss 1554.9103285908	
    Loss 1546.0571931841	
    Loss 1537.177950158	
    Loss 1528.2929220217	
    Loss 1519.5031742826	
    Loss 1510.7293227462	
