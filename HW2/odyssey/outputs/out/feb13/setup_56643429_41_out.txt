[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	10	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3290
 4262
 1533
 2008
 1802
 6487
 1519
 4004
 5727
 2895
 1674
 2312
 2766
 4387
 4121
 4991
 4419
 3115
 2796
 4433
 5577
 2071
 1385
 5944
  728
 4305
 3346
 1376
 3780
 3741
 3587
 1569
  902
 1684
  727
 1061
 4636
 3169
 1265
 1549
 3295
 2638
  869
 1387
 2676
[torch.DoubleTensor of size 45]

Validation accuracy:	0.023648033503277	
Grad norm	0	
    Loss 11387412.975039	
    Loss 2785607.1277236	
    Loss 681814.81169017	
    Loss 167237.06233297	
    Loss 41369.801096439	
    Loss 10579.354563333	
    Loss 3029.8978797308	
    Loss 1187.7935021828	
    Loss 742.76003652782	
    Loss 635.6951098679	
    Loss 612.46678926945	
    Loss 609.60286696807	
    Loss 602.54854346938	
    Loss 593.79135432504	
    Loss 601.80890916217	
    Loss 598.67335764436	
    Loss 603.65852598995	
    Loss 603.3212423371	
    Loss 599.98499155437	
    Loss 601.44319413107	
    Loss 603.22687514206	
    Loss 595.85244176821	
    Loss 608.15885506645	
    Loss 600.94499244747	
    Loss 604.31065927418	
    Loss 595.61379184862	
    Loss 584.04105900703	
    Loss 604.76471183496	
    Loss 599.94106044842	
    Loss 600.12539768362	
    Loss 591.65458070757	
    Loss 595.4541556215	
    Loss 606.08700582603	
    Loss 599.61975332138	
    Loss 595.1769184344	
    Loss 597.00749758847	
    Loss 594.196542848	
    Loss 595.87117977698	
    Loss 598.62571456159	
    Loss 603.46284934274	
    Loss 595.32282980892	
    Loss 589.75570257479	
    Loss 605.66978043587	
    Loss 590.40582155468	
    Loss 597.07751381034	
    Loss 605.40021405674	
    Loss 599.95164708705	
    Loss 583.35468299228	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 2	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 3	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 4	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 5	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 6	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797622	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 7	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797622	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 8	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 9	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 10	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 11	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 12	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 13	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
    Loss 608.11820930157	
    Loss 597.17030130852	
    Loss 602.1833366143	
    Loss 602.90367593576	
    Loss 598.20972553362	
    Loss 595.50217301337	
    Loss 600.85326340475	
    Loss 606.13458063128	
    Loss 601.20599797623	
    Loss 585.76148921985	
    Loss 593.84225662155	
    Loss 597.84312486438	
    Loss 602.29594021698	
    Loss 590.98994965442	
    Loss 600.09466171211	
Epoch 14	
 87388
  5325
     0
 12769
  5473
     0
     0
  1525
 16147
  3181
[torch.DoubleTensor of size 10]

Validation accuracy:	0.082020818159748	
Grad norm	7.1265742754062	
    Loss 582.31773198253	
    Loss 601.06532283981	
    Loss 605.33051499092	
    Loss 598.07004313779	
    Loss 606.70219639652	
    Loss 606.60518347531	
    Loss 590.26122599564	
    Loss 591.37996092482	
    Loss 596.90709096195	
    Loss 600.06484435902	
    Loss 603.71354459023	
    Loss 607.45509203839	
    Loss 602.02647074345	
    Loss 593.66804121369	
    Loss 601.77985922012	
    Loss 598.66744602744	
    Loss 603.65740678777	
    Loss 603.3212184075	
    Loss 599.98483343879	
    Loss 601.44314218727	
    Loss 603.22687771554	
    Loss 595.85243014288	
    Loss 608.15885724856	
    Loss 600.94499539167	
    Loss 604.31065915431	
    Loss 595.6137912726	
    Loss 584.04105840993	
    Loss 604.76471148819	
    Loss 599.94106028556	
    Loss 600.12539760608	
    Loss 591.65458068643	
    Loss 595.45415561706	
    Loss 606.08700582605	
    Loss 599.61975332283	
    Loss 595.17691843634	
    Loss 597.00749758871	
    Loss 594.19654284835	
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	
