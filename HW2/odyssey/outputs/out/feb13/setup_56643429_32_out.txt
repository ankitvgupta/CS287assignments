[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	1	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3437
 5118
 5552
 7356
 3316
 2919
 2426
 4176
 1302
 3178
 1405
 1429
 4632
 3205
 3507
  839
 2150
 3506
 3501
 1236
 3718
 4247
 1337
  665
  851
 4165
 8066
  626
 3078
 3315
  956
 1847
 3674
 1432
 2226
 1584
 1563
 3049
 2873
 1884
 3023
 4442
 1848
 3493
 3656
[torch.DoubleTensor of size 45]

Validation accuracy:	0.023170065549891	
Grad norm	0	
    Loss 11379289.869173	
    Loss 10607719.49961	
    Loss 9888532.3399852	
    Loss 9218139.4511391	
    Loss 8593191.6212041	
    Loss 8010641.878566	
    Loss 7467583.1757908	
    Loss 6961347.6047723	
    Loss 6489436.1149352	
    Loss 6049523.6335857	
    Loss 5639439.3673991	
    Loss 5257158.0047972	
    Loss 4900801.1912421	
    Loss 4568615.5102648	
    Loss 4258936.0756072	
    Loss 3970245.8813988	
    Loss 3701131.9436701	
    Loss 3450266.536523	
    Loss 3216409.0855852	
    Loss 2998406.4748762	
    Loss 2795171.4772446	
    Loss 2605719.111663	
    Loss 2429110.5913131	
    Loss 2264478.1770789	
    Loss 2111011.3438881	
    Loss 1967949.7298953	
    Loss 1834590.8491855	
    Loss 1710267.5658119	
    Loss 1594370.4120451	
    Loss 1486331.6773104	
    Loss 1385619.5395689	
    Loss 1291728.9706296	
    Loss 1204200.2526895	
    Loss 1122608.3183663	
    Loss 1046548.5577047	
    Loss 975644.70250228	
    Loss 909543.2324788	
    Loss 847928.97960931	
    Loss 790490.07807063	
    Loss 736941.43524998	
    Loss 687027.95851768	
    Loss 640492.12772138	
    Loss 597111.80994579	
    Loss 556677.39219363	
    Loss 518977.73323984	
    Loss 483835.68292414	
    Loss 451077.50860985	
    Loss 420539.07074524	
    Loss 392069.95624953	
    Loss 365533.97781164	
    Loss 340793.14892466	
    Loss 317730.70909584	
    Loss 296229.83003035	
    Loss 276189.56751162	
    Loss 257508.28216178	
    Loss 240091.90750158	
    Loss 223855.71750742	
    Loss 208721.55274652	
    Loss 194612.82118581	
    Loss 181460.81733351	
    Loss 169200.83952675	
    Loss 157771.27723789	
    Loss 147115.30695364	
    Loss 137181.72971258	
    Loss 127920.7899289	
    Loss 119288.40532993	
    Loss 111240.48245133	
    Loss 103740.52381785	
    Loss 96746.247189617	
    Loss 90226.301363775	
    Loss 84149.297825361	
    Loss 78484.08732569	
    Loss 73202.513738988	
    Loss 68278.07837748	
    Loss 63688.947223813	
    Loss 59410.897939638	
    Loss 55422.83480108	
    Loss 51705.875498492	
    Loss 48239.247322461	
    Loss 45010.106608631	
    Loss 41996.307281011	
    Loss 39189.114381927	
    Loss 36570.459568664	
    Loss 34130.089968956	
    Loss 31856.9755189	
    Loss 29734.677847113	
    Loss 27756.414278592	
    Loss 25914.16765609	
    Loss 24194.55164605	
    Loss 22592.840432859	
    Loss 21101.347267261	
    Loss 19708.324059944	
    Loss 18412.885331941	
    Loss 17204.254745635	
    Loss 16074.892712738	
    Loss 15022.626934392	
    Loss 14042.563532916	
    Loss 13128.131360536	
    Loss 12276.534266686	
    Loss 11482.663328293	
    Loss 10743.009472104	
    Loss 10053.431101497	
    Loss 9409.524417795	
    Loss 8810.8753949686	
    Loss 8252.6199912234	
    Loss 7732.5042621908	
    Loss 7246.2856304241	
    Loss 6793.9655366675	
    Loss 6372.4097917078	
    Loss 5975.6630704151	
    Loss 5609.979855527	
    Loss 5267.3390983248	
    Loss 4949.0352826588	
    Loss 4652.8556085393	
    Loss 4376.7623264575	
    Loss 4120.8361756308	
    Loss 3879.5096059164	
    Loss 3654.6884716735	
    Loss 3445.6643324143	
    Loss 3249.4017289115	
    Loss 3068.8512238043	
    Loss 2900.0322301025	
    Loss 2741.8418722123	
    Loss 2594.6978344354	
    Loss 2456.6997194579	
    Loss 2327.4637822491	
    Loss 2207.1897676549	
    Loss 2095.1538738501	
    Loss 1993.721531676	
    Loss 1895.8943248763	
    Loss 1804.6770380101	
    Loss 1720.5773194336	
    Loss 1641.9976528612	
    Loss 1568.5790114119	
    Loss 1501.6230500275	
    Loss 1437.5937038323	
    Loss 1378.2437695098	
    Loss 1322.3993272394	
    Loss 1271.7093731861	
    Loss 1224.9329072652	
    Loss 1179.8925775989	
    Loss 1139.7515209831	
    Loss 1100.4734932023	
Epoch 2	
 130152
      0
      0
      0
      1
      0
      0
      0
   1655
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098119992716679	
Grad norm	14.15271298538	
    Loss 1076.8726086097	
    Loss 1041.6931454228	
    Loss 1011.2229986851	
    Loss 980.70753947306	
    Loss 952.87578829964	
    Loss 926.23384750894	
    Loss 902.69678623598	
    Loss 878.40569250395	
    Loss 858.52898528494	
    Loss 838.89570566283	
    Loss 822.1993039891	
    Loss 804.54173579552	
    Loss 787.64398914993	
    Loss 772.21922534222	
    Loss 757.70906361783	
    Loss 744.7687739501	
    Loss 733.99223349823	
    Loss 723.56237509208	
    Loss 713.4246309696	
    Loss 703.20954551632	
    Loss 693.9194895163	
    Loss 686.63877016088	
    Loss 678.384121492	
    Loss 671.13181156008	
    Loss 662.54391781065	
    Loss 656.34627759919	
    Loss 649.95859412703	
    Loss 643.81578703657	
    Loss 639.00330824252	
    Loss 634.14940823985	
    Loss 630.75534070031	
    Loss 625.60343355316	
    Loss 620.99328864147	
    Loss 617.00907877801	
    Loss 613.95580006433	
    Loss 610.90883560735	
    Loss 606.70162695225	
    Loss 603.85930331788	
    Loss 602.02752327121	
    Loss 598.68459784692	
    Loss 597.11692709307	
    Loss 594.55722912635	
    Loss 590.84093394566	
    Loss 589.9069127803	
    Loss 589.02444075219	
    Loss 587.22720550999	
    Loss 586.0615069226	
    Loss 584.85370652651	
    Loss 583.46138621666	
    Loss 581.22455478874	
    Loss 578.88870434508	
    Loss 578.94297177675	
    Loss 575.21299482095	
    Loss 576.29346335003	
    Loss 576.33190207026	
    Loss 576.98343567286	
    Loss 576.03868504502	
    Loss 575.22316895436	
    Loss 576.22106001673	
    Loss 577.34922946453	
    Loss 576.3836779399	
    Loss 576.1992021363	
    Loss 575.93561581132	
    Loss 574.16354772077	
    Loss 573.87334408002	
    Loss 573.2223699451	
    Loss 572.33447012001	
    Loss 572.78130890765	
    Loss 571.24582461662	
    Loss 570.37514244949	
    Loss 570.25465390749	
    Loss 569.72133186315	
    Loss 568.81115239811	
    Loss 568.39311233548	
    Loss 568.905693022	
    Loss 569.38074066836	
    Loss 568.90246888888	
    Loss 569.06915071593	
    Loss 568.4375406558	
    Loss 570.03210702559	
    Loss 568.63362477679	
    Loss 569.0921892839	
    Loss 568.1883928962	
    Loss 568.07781793046	
    Loss 569.87568033011	
    Loss 567.89728827817	
    Loss 566.39303117943	
    Loss 566.79139781747	
    Loss 565.59092768648	
    Loss 565.4617080867	
    Loss 567.15293741801	
    Loss 566.02762989318	
    Loss 568.1421650248	
    Loss 568.91045809562	
    Loss 566.39576074593	
    Loss 565.77800972528	
    Loss 565.64758724852	
    Loss 564.82630130088	
    Loss 564.8925142484	
    Loss 564.79226344782	
    Loss 565.05212989371	
    Loss 565.43664135427	
    Loss 564.92060978258	
    Loss 566.02042254127	
    Loss 566.71525748536	
    Loss 567.68894251235	
    Loss 567.25526432992	
    Loss 567.79108640726	
    Loss 567.95609206253	
    Loss 564.66055056915	
    Loss 565.63773442241	
    Loss 564.91341409384	
    Loss 565.37970667342	
    Loss 566.3669643276	
    Loss 567.41364913645	
    Loss 569.72612992757	
    Loss 569.24792744126	
    Loss 568.96859682304	
    Loss 569.11020830613	
    Loss 567.8099560912	
    Loss 569.03499800953	
    Loss 569.65994697595	
    Loss 569.41826862846	
    Loss 569.4855247516	
    Loss 568.83238782065	
    Loss 567.55907344436	
    Loss 566.48848453814	
    Loss 565.62844717663	
    Loss 567.93335988146	
    Loss 566.81568015894	
    Loss 565.66390541273	
    Loss 565.49695681846	
    Loss 565.28758495387	
    Loss 564.92441957709	
    Loss 566.1848416038	
    Loss 565.54377580398	
    Loss 565.42603342281	
    Loss 564.73708161536	
    Loss 565.47346208125	
    Loss 566.59622580825	
    Loss 566.13988626052	
    Loss 567.66452143941	
    Loss 567.25881475634	
Epoch 3	
 130150
      0
      0
      0
      1
      0
      0
      0
   1657
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098119992716679	
Grad norm	6.1783027945323	
    Loss 565.68001049149	
    Loss 565.15252081079	
    Loss 566.99130545112	
    Loss 566.59742886332	
    Loss 566.81969709439	
    Loss 566.31635626432	
    Loss 567.1813980227	
    Loss 565.60013744617	
    Loss 566.92914560652	
    Loss 567.05048999569	
    Loss 568.76289186581	
    Loss 568.27327278456	
    Loss 567.36815902911	
    Loss 566.85410369266	
    Loss 566.29575316281	
    Loss 566.35674525622	
    Loss 567.66651966187	
    Loss 568.486727989	
    Loss 568.84088956963	
    Loss 568.40929220817	
    Loss 568.27070878262	
    Loss 569.51653452408	
    Loss 569.20504340623	
    Loss 569.35174088567	
    Loss 567.64834783709	
    Loss 567.87718714616	
    Loss 567.47705175127	
    Loss 566.91369338373	
    Loss 567.31365624078	
    Loss 567.30600451412	
    Loss 568.41628301071	
    Loss 567.49416467569	
    Loss 566.82762813129	
    Loss 566.50512983387	
    Loss 566.87236994181	
    Loss 567.0146186084	
    Loss 565.79681018711	
    Loss 565.72293201541	
    Loss 566.47761706981	
    Loss 565.54942183281	
    Loss 566.22228712959	
    Loss 565.76360253131	
    Loss 564.00636527455	
    Loss 564.88702731756	
    Loss 565.71454252934	
    Loss 565.50025988717	
    Loss 565.80530224756	
    Loss 565.96945879798	
    Loss 565.87022717144	
    Loss 564.81484835597	
    Loss 563.60202711706	
    Loss 564.70072485476	
    Loss 561.93778840444	
    Loss 563.92264000385	
    Loss 564.79823335503	
    Loss 566.23430111151	
    Loss 566.02233666825	
    Loss 565.87917763181	
    Loss 567.51798181866	
    Loss 569.2406880394	
    Loss 568.81735868644	
    Loss 569.14502015649	
    Loss 569.36323306676	
    Loss 568.03372368207	
    Loss 568.16537919781	
    Loss 567.90379334996	
    Loss 567.37628228321	
    Loss 568.15396786891	
    Loss 566.93099267879	
    Loss 566.35350323744	
    Loss 566.50666962464	
    Loss 566.22613615667	
    Loss 565.55283482834	
    Loss 565.36200780937	
    Loss 566.08188518805	
    Loss 566.75021920368	
    Loss 566.44760128662	
    Loss 566.77565291732	
    Loss 566.29891931191	
    Loss 568.03570610049	
    Loss 566.77578970975	
    Loss 567.35737837635	
    Loss 566.57340973398	
    Loss 566.57290869746	
    Loss 568.47366409454	
    Loss 566.58889858656	
    Loss 565.17350421837	
    Loss 565.65231803982	
    Loss 564.5315094055	
    Loss 564.47443788907	
    Loss 566.23302068798	
    Loss 565.17152019216	
    Loss 567.34501837066	
    Loss 568.1673802344	
    Loss 565.70020870412	
    Loss 565.13251053882	
    Loss 565.04627568974	
    Loss 564.26714110614	
    Loss 564.3719457774	
    Loss 564.30710475353	
    Loss 564.59936428441	
    Loss 565.01523452232	
    Loss 564.52982900589	
    Loss 565.65805690495	
    Loss 566.37817730677	
    Loss 567.37565354261	
    Loss 566.96396048555	
    Loss 567.5209127342	
    Loss 567.70267209379	
    Loss 564.42474418497	
    Loss 565.41765098586	
    Loss 564.70849419955	
    Loss 565.1888925134	
    Loss 566.18974547347	
    Loss 567.24951710921	
    Loss 569.57339579045	
    Loss 569.10668701498	
    Loss 568.83796360297	
    Loss 568.98873880468	
    Loss 567.69673368171	
    Loss 568.92960735892	
    Loss 569.56169412403	
    Loss 569.32664399966	
    Loss 569.40002811273	
    Loss 568.75305747377	
    Loss 567.48505391428	
    Loss 566.41903423614	
    Loss 565.56341844638	
    Loss 567.8730452047	
    Loss 566.75987535759	
    Loss 565.6118037148	
    Loss 565.44823457951	
    Loss 565.24270730658	
    Loss 564.88301167614	
    Loss 566.1473717759	
    Loss 565.5088787499	
    Loss 565.39427610563	
    Loss 564.70780738578	
    Loss 565.44659739656	
    Loss 566.57146347945	
    Loss 566.11667794581	
    Loss 567.64327393396	
    Loss 567.23956839058	
Epoch 4	
 130150
      0
      0
      0
      1
      0
      0
      0
   1657
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098119992716679	
Grad norm	6.1466365164075	
    Loss 565.66176114647	
    Loss 565.13556178566	
    Loss 566.97571302032	
    Loss 566.58297190263	
    Loss 566.80619143501	
    Loss 566.30364296879	
    Loss 567.16960433674	
    Loss 565.58897315462	
    Loss 566.91883179098	
    Loss 567.04083484933	
    Loss 568.75382982565	
    Loss 568.26480221508	
    Loss 567.36012709555	
    Loss 566.84652623786	
    Loss 566.28885969491	
    Loss 566.35047486227	
    Loss 567.66063351434	
    Loss 568.48112836386	
    Loss 568.83555664413	
    Loss 568.40425098175	
    Loss 568.26609026696	
    Loss 569.51232921652	
    Loss 569.20114229043	
    Loss 569.34809981117	
    Loss 567.64486488887	
    Loss 567.87391124734	
    Loss 567.47395945252	
    Loss 566.9107533194	
    Loss 567.31092958193	
    Loss 567.30338133888	
    Loss 568.41369469458	
    Loss 567.49180062864	
    Loss 566.82544582549	
    Loss 566.50304271515	
    Loss 566.87040060358	
    Loss 567.01277693126	
    Loss 565.79517039173	
    Loss 565.72138553195	
    Loss 566.47619138387	
    Loss 565.54812176396	
    Loss 566.22105127085	
    Loss 565.76248158268	
    Loss 564.00537479197	
    Loss 564.88608502478	
    Loss 565.71374232777	
    Loss 565.49952995761	
    Loss 565.80460972435	
    Loss 565.96879325423	
    Loss 565.86968648921	
    Loss 564.81427855012	
    Loss 563.60156770355	
    Loss 564.7003486783	
    Loss 561.9374466413	
    Loss 563.92234998911	
    Loss 564.79796181853	
    Loss 566.23405974557	
    Loss 566.02214265266	
    Loss 565.87896242244	
    Loss 567.51783138135	
    Loss 569.24057768026	
    Loss 568.81720601225	
    Loss 569.14487207849	
    Loss 569.36311484899	
    Loss 568.03359996449	
    Loss 568.16530426763	
    Loss 567.90374199661	
    Loss 567.37622890643	
    Loss 568.15388461389	
    Loss 566.93090743558	
    Loss 566.35343095285	
    Loss 566.50660773066	
    Loss 566.22606974355	
    Loss 565.5527749065	
    Loss 565.36199358865	
    Loss 566.0818813173	
    Loss 566.7502284929	
    Loss 566.44759570063	
    Loss 566.77561688806	
    Loss 566.29888196695	
    Loss 568.0356556141	
    Loss 566.77576622659	
    Loss 567.35733625066	
    Loss 566.57338610173	
    Loss 566.57289110789	
    Loss 568.47365317325	
    Loss 566.58887807986	
    Loss 565.17348724255	
    Loss 565.65228649482	
    Loss 564.53149450704	
    Loss 564.4744265258	
    Loss 566.23301224768	
    Loss 565.17152281268	
    Loss 567.34502679109	
    Loss 568.16738851858	
    Loss 565.70020138902	
    Loss 565.1325207501	
    Loss 565.04628778609	
    Loss 564.26716051387	
    Loss 564.37196746132	
    Loss 564.30712678968	
    Loss 564.59938209364	
    Loss 565.01525438226	
    Loss 564.52986122775	
    Loss 565.65809954226	
    Loss 566.37822189436	
    Loss 567.37570039144	
    Loss 566.96400759725	
    Loss 567.520966572	
    Loss 567.70271256503	
    Loss 564.42478461014	
    Loss 565.41768775478	
    Loss 564.70853052342	
    Loss 565.18892709098	
    Loss 566.18978303181	
    Loss 567.24955876282	
    Loss 569.57343631634	
    Loss 569.10673204503	
    Loss 568.83801157965	
    Loss 568.98878559989	
    Loss 567.69677738252	
    Loss 568.92964887998	
    Loss 569.56173246807	
    Loss 569.3266788924	
    Loss 569.40006034055	
    Loss 568.75308911014	
    Loss 567.48508247112	
    Loss 566.41905794847	
    Loss 565.56343832721	
    Loss 567.87306528639	
    Loss 566.75989640467	
    Loss 565.6118225717	
    Loss 565.44825163372	
    Loss 565.24272635458	
    Loss 564.88303178361	
    Loss 566.14739716588	
    Loss 565.50890287863	
    Loss 565.39430341293	
    Loss 564.70783461846	
    Loss 565.44662525713	
    Loss 566.57149143155	
    Loss 566.11670354327	
    Loss 567.64330005077	
    Loss 567.23959602876	
Epoch 5	
 130150
      0
      0
      0
      1
      0
      0
      0
   1657
[torch.DoubleTensor of size 9]

Validation accuracy:	0.098119992716679	
Grad norm	6.146607414324	
    Loss 565.66178887067	
    Loss 565.13558797634	
    Loss 566.97573926755	
    Loss 566.58299677301	
    Loss 566.8062145976	
    Loss 566.30366390287	
    Loss 567.16962420129	
    Loss 565.58899071975	
    Loss 566.91884898429	
    Loss 567.04085066692	
    Loss 568.75384429243	
    Loss 568.26481573415	
    Loss 567.36013882075	
    Loss 566.84653664876	
    Loss 566.28887035356	
    Loss 566.35048572264	
    Loss 567.66064332051	
    Loss 568.48113690284	
    Loss 568.83556392913	
    Loss 568.40425742826	
    Loss 568.26609677386	
    Loss 569.51233607098	
    Loss 569.20114873997	
    Loss 569.34810579658	
    Loss 567.64486989114	
    Loss 567.87391571223	
    Loss 567.47396342101	
    Loss 566.91075667845	
    Loss 567.31093283935	
    Loss 567.3033838472	
    Loss 568.41369618304	
    Loss 567.49180237422	
    Loss 566.82544756292	
    Loss 566.50304400301	
    Loss 566.87040161759	
    Loss 567.01277785127	
    Loss 565.79517168013	
    Loss 565.72138663672	
    Loss 566.47619250283	
    Loss 565.54812297993	
    Loss 566.22105228989	
    Loss 565.7624826683	
    Loss 564.00537617252	
    Loss 564.88608622084	
    Loss 565.71374388065	
    Loss 565.49953150795	
    Loss 565.80461109864	
    Loss 565.96879435589	
    Loss 565.86968800143	
    Loss 564.81427955579	
    Loss 563.60156910413	
    Loss 564.70035030779	
    Loss 561.93744820884	
    Loss 563.92235162569	
    Loss 564.79796335543	
    Loss 566.23406122631	
    Loss 566.02214423894	
    Loss 565.87896371581	
    Loss 567.51783290885	
    Loss 569.24057929212	
    Loss 568.81720716798	
