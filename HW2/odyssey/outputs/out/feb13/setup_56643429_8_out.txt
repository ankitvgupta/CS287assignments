[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.1	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 8331
 1639
 5145
 1406
 1504
  981
 1283
 2607
 4071
 2545
 5937
 2357
 1739
 1109
 1663
 2173
 2382
 2944
 8652
 3092
 1279
 1973
 1264
 7379
 3300
 2321
 3443
  992
 2480
 1654
 1469
 2907
 1084
 2003
 3891
 3448
 1571
 4866
 4082
 1194
 5881
 1672
 6938
 1474
 1683
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02408806749211	
Grad norm	0	
    Loss 2279324.101051	
    Loss 2276107.6797763	
    Loss 2272897.7910244	
    Loss 2269694.4902367	
    Loss 2266495.9076372	
    Loss 2263301.8325854	
    Loss 2260113.1711994	
    Loss 2256927.9799251	
    Loss 2253747.2007754	
    Loss 2250571.7637914	
    Loss 2247401.2115977	
    Loss 2244234.895917	
    Loss 2241073.7592731	
    Loss 2237916.4677394	
    Loss 2234764.4315883	
    Loss 2231616.8277186	
    Loss 2228473.4394687	
    Loss 2225334.5321502	
    Loss 2222199.4521606	
    Loss 2219069.7115553	
    Loss 2215943.3693336	
    Loss 2212822.7310304	
    Loss 2209705.6332485	
    Loss 2206593.3147251	
    Loss 2203486.35252	
    Loss 2200383.2778529	
    Loss 2197284.797027	
    Loss 2194190.9516992	
    Loss 2191101.4349076	
    Loss 2188015.6521523	
    Loss 2184934.4004464	
    Loss 2181857.9060264	
    Loss 2178785.500433	
    Loss 2175717.0076602	
    Loss 2172653.4971795	
    Loss 2169593.9714006	
    Loss 2166539.2708329	
    Loss 2163488.6567802	
    Loss 2160441.8841206	
    Loss 2157400.1613476	
    Loss 2154363.0662239	
    Loss 2151329.9702841	
    Loss 2148300.8498363	
    Loss 2145276.8028743	
    Loss 2142256.1969854	
    Loss 2139239.6453481	
    Loss 2136227.7076316	
    Loss 2133220.5618704	
    Loss 2130217.1337729	
    Loss 2127218.7580395	
    Loss 2124224.3469946	
    Loss 2121233.7087878	
    Loss 2118247.6376115	
    Loss 2115265.8848174	
    Loss 2112288.0684501	
    Loss 2109314.0510732	
    Loss 2106343.9764108	
    Loss 2103378.8254162	
    Loss 2100418.0117291	
    Loss 2097460.9898223	
    Loss 2094508.415276	
    Loss 2091559.6954568	
    Loss 2088614.9481676	
    Loss 2085675.4887558	
    Loss 2082739.6250113	
    Loss 2079807.7843958	
    Loss 2076880.2098247	
    Loss 2073956.8035726	
    Loss 2071037.6298869	
    Loss 2068121.882177	
    Loss 2065211.2559558	
    Loss 2062304.4472153	
    Loss 2059401.5015461	
    Loss 2056503.1751525	
    Loss 2053608.4451325	
    Loss 2050718.104707	
    Loss 2047831.4679603	
    Loss 2044949.0416888	
    Loss 2042071.4178125	
    Loss 2039196.9444915	
    Loss 2036326.8795722	
    Loss 2033460.7373495	
    Loss 2030598.8956865	
    Loss 2027740.9813599	
    Loss 2024887.1719287	
    Loss 2022036.9649887	
    Loss 2019191.3083844	
    Loss 2016349.2740539	
    Loss 2013512.1202721	
    Loss 2010678.3074755	
    Loss 2007847.5593249	
    Loss 2005021.7199076	
    Loss 2002199.8898562	
    Loss 1999382.5848685	
    Loss 1996569.0238884	
    Loss 1993759.3065128	
    Loss 1990952.8699037	
    Loss 1988151.6011779	
    Loss 1985353.8029121	
    Loss 1982559.6477031	
    Loss 1979769.2357195	
    Loss 1976982.524667	
    Loss 1974200.6486401	
    Loss 1971422.2855402	
    Loss 1968647.7876987	
    Loss 1965877.8451127	
    Loss 1963110.9906243	
    Loss 1960348.4258142	
    Loss 1957589.4837389	
    Loss 1954835.0354198	
    Loss 1952083.1357251	
    Loss 1949336.436164	
    Loss 1946593.3549186	
    Loss 1943853.4802893	
    Loss 1941117.7058588	
    Loss 1938385.4374949	
    Loss 1935657.668599	
    Loss 1932933.5326165	
    Loss 1930213.9951937	
    Loss 1927497.7822209	
    Loss 1924785.2579698	
    Loss 1922076.3910339	
    Loss 1919372.3831012	
    Loss 1916671.2401109	
    Loss 1913974.3645536	
    Loss 1911280.8923408	
    Loss 1908591.4978618	
    Loss 1905905.5925035	
    Loss 1903223.6733823	
    Loss 1900545.6849843	
    Loss 1897871.32415	
    Loss 1895201.0031848	
    Loss 1892534.6169318	
    Loss 1889872.2486159	
    Loss 1887212.7343455	
    Loss 1884556.665279	
    Loss 1881904.9689247	
    Loss 1879256.8688551	
    Loss 1876612.3233114	
    Loss 1873971.7918989	
    Loss 1871335.2382083	
    Loss 1868702.0289555	
    Loss 1866072.7516529	
Epoch 2	
 52793
  8263
  3210
  1487
 15195
   406
   405
 12035
 22589
  7898
  3271
   351
   362
   384
   476
  1610
   490
   581
     0
     0
     0
     0
     0
     0
     0
     1
     0
     0
     0
     0
     0
     0
     0
     0
     1
[torch.DoubleTensor of size 35]

Validation accuracy:	0.090745630007283	
Grad norm	235.68553545137	
    Loss 1864497.0993901	
    Loss 1861873.6123183	
    Loss 1859253.62353	
    Loss 1856637.262388	
    Loss 1854024.6825288	
    Loss 1851415.7318881	
    Loss 1848811.0238492	
    Loss 1846209.037918	
    Loss 1843610.9177876	
    Loss 1841016.9640473	
    Loss 1838426.4127802	
    Loss 1835839.9277553	
    Loss 1833257.510489	
    Loss 1830678.5220073	
    Loss 1828103.4517463	
    Loss 1825531.5569023	
    Loss 1822963.006291	
    Loss 1820397.9580918	
    Loss 1817836.3878474	
    Loss 1815278.9108295	
    Loss 1812723.9123879	
    Loss 1810173.3871425	
    Loss 1807626.1053725	
    Loss 1805082.2857038	
    Loss 1802543.2469948	
    Loss 1800007.329239	
    Loss 1797475.0391041	
    Loss 1794946.5480053	
    Loss 1792421.3364143	
    Loss 1789899.552845	
    Loss 1787381.0585873	
    Loss 1784866.1982186	
    Loss 1782354.9051482	
    Loss 1779846.7548315	
    Loss 1777342.5529732	
    Loss 1774841.6185673	
    Loss 1772344.7361596	
    Loss 1769851.166711	
    Loss 1767360.6381616	
    Loss 1764874.2686104	
    Loss 1762391.5640473	
    Loss 1759912.0296289	
    Loss 1757435.8649906	
    Loss 1754963.8300372	
    Loss 1752494.4890523	
    Loss 1750028.4443563	
    Loss 1747566.232464	
    Loss 1745107.8789533	
    Loss 1742652.7498021	
    Loss 1740201.516128	
    Loss 1737753.5674596	
    Loss 1735308.6023851	
    Loss 1732867.5802643	
    Loss 1730429.6975178	
    Loss 1727995.2411362	
    Loss 1725563.5946713	
    Loss 1723135.383654	
    Loss 1720711.1177562	
    Loss 1718290.3889533	
    Loss 1715872.7603795	
    Loss 1713458.9168189	
    Loss 1711048.2422576	
    Loss 1708640.5737459	
    Loss 1706237.269357	
    Loss 1703836.8291629	
    Loss 1701439.7531531	
    Loss 1699046.0399395	
    Loss 1696655.8320675	
    Loss 1694269.0119491	
    Loss 1691884.8565005	
    Loss 1689505.2815901	
    Loss 1687128.4991688	
    Loss 1684754.9738412	
    Loss 1682385.335051	
    Loss 1680018.5577775	
    Loss 1677655.2681116	
    Loss 1675295.0621128	
    Loss 1672938.4633219	
    Loss 1670585.3911761	
    Loss 1668234.9742232	
    Loss 1665888.272511	
    Loss 1663544.5649037	
    Loss 1661204.6941601	
    Loss 1658867.8583944	
    Loss 1656534.3635089	
    Loss 1654203.9359379	
    Loss 1651877.1460674	
    Loss 1649553.2482868	
    Loss 1647233.333757	
    Loss 1644916.0393449	
    Loss 1642601.1660841	
    Loss 1640290.5428983	
    Loss 1637983.277563	
    Loss 1635679.5301775	
    Loss 1633378.9572708	
    Loss 1631081.4723245	
    Loss 1628786.507897	
    Loss 1626496.0581471	
    Loss 1624208.1849372	
    Loss 1621923.3376208	
    Loss 1619641.4808086	
    Loss 1617362.6345087	
    Loss 1615087.749544	
    Loss 1612815.7635042	
    Loss 1610546.9356442	
    Loss 1608281.8168911	
    Loss 1606019.2187912	
    Loss 1603760.1480882	
    Loss 1601503.8976871	
    Loss 1599251.5785955	
    Loss 1597001.1049373	
    Loss 1594754.9374613	
    Loss 1592511.8342868	
    Loss 1590271.2522339	
    Loss 1588034.012838	
    Loss 1585799.5761302	
    Loss 1583568.9250729	
    Loss 1581341.1218491	
    Loss 1579117.286119	
    Loss 1576895.9846002	
    Loss 1574677.7951274	
    Loss 1572462.5386141	
    Loss 1570251.3270537	
    Loss 1568042.4256593	
    Loss 1565836.984804	
    Loss 1563634.3500976	
    Loss 1561434.9186305	
    Loss 1559238.4050987	
    Loss 1557045.0768513	
    Loss 1554855.0168257	
    Loss 1552668.0258217	
    Loss 1550484.2910156	
    Loss 1548303.7869603	
    Loss 1546126.4808678	
    Loss 1543951.4673457	
    Loss 1541779.2884813	
    Loss 1539610.6639558	
    Loss 1537445.0225983	
    Loss 1535282.1724741	
    Loss 1533122.6962478	
    Loss 1530966.6102471	
    Loss 1528813.0657836	
    Loss 1526662.9080009	
Epoch 3	
 60658
  7291
  2407
  1031
 14517
   218
   263
 10570
 22866
  6873
  2516
   209
   211
   245
   224
  1120
   253
   336
[torch.DoubleTensor of size 18]

Validation accuracy:	0.092429898033503	
Grad norm	213.63876279765	
    Loss 1525374.3094075	
    Loss 1523228.7530505	
    Loss 1521086.0419237	
    Loss 1518946.2078393	
    Loss 1516809.5394368	
    Loss 1514675.8635644	
    Loss 1512545.6987104	
    Loss 1510417.6310749	
    Loss 1508292.8123176	
    Loss 1506171.4017616	
    Loss 1504052.7524172	
    Loss 1501937.5452779	
    Loss 1499825.5209531	
    Loss 1497716.4113226	
    Loss 1495610.4374345	
    Loss 1493507.057047	
    Loss 1491406.2989504	
    Loss 1489308.4734099	
    Loss 1487213.526347	
    Loss 1485121.9191711	
    Loss 1483032.2458298	
    Loss 1480946.1773232	
    Loss 1478862.9302962	
    Loss 1476782.3925668	
    Loss 1474705.8464856	
    Loss 1472631.8446709	
    Loss 1470560.7734791	
    Loss 1468492.8096099	
    Loss 1466427.5893979	
    Loss 1464365.1385518	
    Loss 1462305.3429848	
    Loss 1460248.5036898	
    Loss 1458194.5672028	
    Loss 1456143.2157138	
    Loss 1454095.0796561	
    Loss 1452049.6173018	
    Loss 1450007.460064	
    Loss 1447968.0471658	
    Loss 1445931.0957025	
    Loss 1443897.5811083	
    Loss 1441866.9750067	
    Loss 1439839.0078238	
    Loss 1437813.8331997	
    Loss 1435792.0422703	
    Loss 1433772.390508	
    Loss 1431755.4137076	
    Loss 1429741.6007734	
    Loss 1427731.0015741	
    Loss 1425723.0411824	
    Loss 1423718.226408	
    Loss 1421716.1071339	
    Loss 1419716.3761697	
    Loss 1417719.9766032	
    Loss 1415726.0035816	
    Loss 1413734.9223705	
    Loss 1411746.0041428	
    Loss 1409759.9619113	
    Loss 1407777.1754241	
    Loss 1405797.2496607	
    Loss 1403819.8212563	
    Loss 1401845.574567	
    Loss 1399873.965512	
    Loss 1397904.6892272	
    Loss 1395939.0183659	
    Loss 1393975.6361099	
    Loss 1392015.0482047	
    Loss 1390057.1646324	
    Loss 1388102.2247847	
    Loss 1386150.0138644	
    Loss 1384199.9156021	
    Loss 1382253.7365614	
    Loss 1380309.6962316	
    Loss 1378368.3874085	
    Loss 1376430.294528	
    Loss 1374494.5050982	
    Loss 1372561.5522129	
    Loss 1370631.0942531	
    Loss 1368703.7040118	
    Loss 1366779.028483	
    Loss 1364856.5799034	
    Loss 1362937.1465842	
    Loss 1361020.1697774	
    Loss 1359106.3964549	
    Loss 1357195.0461738	
    Loss 1355286.4441501	
    Loss 1353380.3828786	
    Loss 1351477.2594947	
    Loss 1349576.4682254	
    Loss 1347678.9717821	
    Loss 1345783.5559697	
    Loss 1343890.0423083	
    Loss 1342000.1530593	
    Loss 1340113.0420674	
    Loss 1338228.7328516	
    Loss 1336347.0457289	
    Loss 1334467.8939351	
    Loss 1332590.7102955	
    Loss 1330717.3546967	
    Loss 1328845.9987202	
    Loss 1326977.1421621	
    Loss 1325110.7115306	
    Loss 1323246.7115896	
    Loss 1321385.9629719	
    Loss 1319527.599662	
    Loss 1317671.8204084	
    Loss 1315819.0777802	
    Loss 1313968.4001989	
    Loss 1312120.603878	
    Loss 1310275.0586722	
    Loss 1308432.8747744	
    Loss 1306592.0423029	
    Loss 1304754.8084631	
    Loss 1302920.0892649	
    Loss 1301087.3935328	
    Loss 1299257.4195778	
    Loss 1297429.7519739	
    Loss 1295605.2209811	
    Loss 1293782.9439776	
    Loss 1291964.0162777	
    Loss 1290147.0448007	
    Loss 1288332.6891986	
    Loss 1286520.7133904	
    Loss 1284712.0911756	
    Loss 1282905.3342398	
    Loss 1281101.3933123	
    Loss 1279299.7355655	
    Loss 1277500.6573037	
    Loss 1275704.0191151	
    Loss 1273909.9330215	
    Loss 1272118.5271216	
    Loss 1270329.6993902	
    Loss 1268543.5161676	
    Loss 1266759.9868548	
    Loss 1264979.0149421	
    Loss 1263199.8980383	
    Loss 1261423.1060712	
    Loss 1259649.2299068	
    Loss 1257877.798637	
    Loss 1256108.6017957	
    Loss 1254342.2401118	
    Loss 1252578.7009993	
    Loss 1250817.1413623	
    Loss 1249058.4357204	
Epoch 4	
 67629
  6360
  1878
   690
 13670
   116
   148
  9585
 22222
  5985
  1960
   122
   125
   144
    99
   753
   130
   192
[torch.DoubleTensor of size 18]

Validation accuracy:	0.094265901917941	
Grad norm	193.71174599724	
    Loss 1248004.3881221	
    Loss 1246249.378358	
    Loss 1244496.6633009	
    Loss 1242746.303208	
    Loss 1240998.5490773	
    Loss 1239253.2488719	
    Loss 1237510.8639363	
    Loss 1235770.1059219	
    Loss 1234032.0578335	
    Loss 1232296.8065079	
    Loss 1230563.817882	
    Loss 1228833.6889831	
    Loss 1227106.095579	
    Loss 1225380.9354581	
    Loss 1223658.3030599	
    Loss 1221937.8238546	
    Loss 1220219.3862921	
    Loss 1218503.407743	
    Loss 1216789.7981057	
    Loss 1215078.911516	
    Loss 1213369.5617662	
    Loss 1211663.1279099	
    Loss 1209959.0842943	
    Loss 1208257.2043093	
    Loss 1206558.643103	
    Loss 1204862.1389805	
    Loss 1203168.0171258	
    Loss 1201476.4421404	
    Loss 1199787.1654503	
    Loss 1198100.10294	
    Loss 1196415.2196346	
    Loss 1194732.7489709	
    Loss 1193052.6331413	
    Loss 1191374.6300344	
    Loss 1189699.2667091	
    Loss 1188026.0868471	
    Loss 1186355.6097834	
    Loss 1184687.385259	
    Loss 1183021.1629843	
    Loss 1181357.7716114	
    Loss 1179696.7133474	
    Loss 1178037.8379826	
    Loss 1176381.2700167	
    Loss 1174727.4774996	
    Loss 1173075.4053325	
    Loss 1171425.4938264	
    Loss 1169778.1888915	
    Loss 1168133.564079	
    Loss 1166491.0742643	
    Loss 1164851.1571723	
    Loss 1163213.4380357	
    Loss 1161577.6391182	
    Loss 1159944.638127	
    Loss 1158313.5481068	
    Loss 1156684.8673149	
    Loss 1155057.8795918	
    Loss 1153433.2623926	
    Loss 1151811.359096	
    Loss 1150191.7747796	
    Loss 1148574.1897652	
    Loss 1146959.2557048	
    Loss 1145346.5147229	
    Loss 1143735.6094765	
    Loss 1142127.6746447	
    Loss 1140521.581242	
    Loss 1138917.8000941	
    Loss 1137316.2109231	
    Loss 1135717.0649773	
    Loss 1134120.1345896	
    Loss 1132524.8864531	
    Loss 1130932.9485769	
    Loss 1129342.6705022	
    Loss 1127754.679545	
    Loss 1126169.3420212	
    Loss 1124585.8576809	
    Loss 1123004.6920647	
    Loss 1121425.5428368	
    Loss 1119848.9701185	
    Loss 1118274.538476	
    Loss 1116701.9565328	
    Loss 1115131.8076348	
    Loss 1113563.7059409	
    Loss 1111998.2278148	
    Loss 1110434.7122375	
    Loss 1108873.4573215	
    Loss 1107314.2889974	
    Loss 1105757.5049738	
    Loss 1104202.6095621	
    Loss 1102650.4456903	
    Loss 1101099.9487417	
    Loss 1099550.9349719	
    Loss 1098004.992555	
    Loss 1096461.3385085	
    Loss 1094919.9403554	
    Loss 1093380.6860757	
    Loss 1091843.5249658	
    Loss 1090307.9044962	
    Loss 1088775.4998078	
    Loss 1087244.681343	
    Loss 1085715.9172999	
    Loss 1084189.1304032	
    Loss 1082664.3004144	
    Loss 1081142.1478166	
    Loss 1079621.9524819	
    Loss 1078103.8706771	
    Loss 1076588.2733869	
    Loss 1075074.3648937	
    Loss 1073562.8180888	
    Loss 1072053.0766491	
    Loss 1070546.1770188	
    Loss 1069040.2779605	
    Loss 1067537.3851955	
    Loss 1066036.5415229	
    Loss 1064537.3297046	
    Loss 1063040.3467692	
    Loss 1061545.2616821	
    Loss 1060052.7581156	
    Loss 1058562.0544975	
    Loss 1057074.1519664	
    Loss 1055587.7693953	
    Loss 1054103.5809693	
    Loss 1052621.3312119	
    Loss 1051141.8554464	
    Loss 1049663.8771366	
    Loss 1048188.2009354	
    Loss 1046714.3720377	
    Loss 1045242.6479428	
    Loss 1043772.9556238	
    Loss 1042305.3134129	
    Loss 1040839.8592942	
    Loss 1039376.5577757	
    Loss 1037915.416263	
    Loss 1036456.4465518	
    Loss 1034999.5347585	
    Loss 1033544.1301744	
    Loss 1032090.6254892	
    Loss 1030639.5245892	
    Loss 1029190.4153189	
    Loss 1027743.1054993	
    Loss 1026298.1739377	
    Loss 1024855.5791741	
    Loss 1023414.5355983	
    Loss 1021975.8773231	
Epoch 5	
 73912
  5330
  1320
   444
 12706
    47
    68
  8572
 21958
  5064
  1486
    63
    56
    73
    52
   499
    63
    95
[torch.DoubleTensor of size 18]

Validation accuracy:	0.095616351056082	
Grad norm	175.69740097073	
    Loss 1021113.6060178	
    Loss 1019677.9305989	
    Loss 1018244.1033296	
    Loss 1016812.2224943	
    Loss 1015382.4745738	
    Loss 1013954.7303942	
    Loss 1012529.4118364	
    Loss 1011105.3578915	
    Loss 1009683.5566808	
    Loss 1008264.0536938	
    Loss 1006846.4055114	
    Loss 1005431.1209496	
    Loss 1004017.8748206	
    Loss 1002606.6414317	
    Loss 1001197.4573045	
    Loss 999790.06520255	
    Loss 998384.25992474	
    Loss 996980.51184338	
    Loss 995578.71121423	
    Loss 994179.13059444	
    Loss 992780.7800568	
    Loss 991384.793491	
    Loss 989990.80892142	
    Loss 988598.56726958	
    Loss 987209.07955536	
    Loss 985821.24741998	
    Loss 984435.35985842	
    Loss 983051.56192783	
    Loss 981669.68628395	
    Loss 980289.57202333	
    Loss 978911.26161637	
    Loss 977534.91896657	
    Loss 976160.48633059	
    Loss 974787.77604552	
    Loss 973417.23890675	
    Loss 972048.48530191	
    Loss 970681.94674351	
    Loss 969317.24476964	
    Loss 967954.17765497	
    Loss 966593.43818099	
    Loss 965234.5825383	
    Loss 963877.52679762	
    Loss 962522.36760263	
    Loss 961169.49298795	
    Loss 959818.00744323	
    Loss 958468.26468053	
    Loss 957120.66599911	
    Loss 955775.29988523	
    Loss 954431.65811888	
    Loss 953090.12365573	
    Loss 951750.38127891	
    Loss 950412.18599023	
    Loss 949076.33031773	
    Loss 947741.9946545	
    Loss 946409.65735371	
    Loss 945078.65439688	
    Loss 943749.58934072	
    Loss 942422.79559367	
    Loss 941097.88634158	
    Loss 939774.57323472	
    Loss 938453.45679493	
    Loss 937134.16468484	
    Loss 935816.32178047	
    Loss 934500.92890445	
    Loss 933187.02540652	
    Loss 931875.02627721	
    Loss 930564.8129886	
    Loss 929256.61684757	
    Loss 927950.22378929	
    Loss 926645.17200231	
    Loss 925342.90086323	
    Loss 924041.92369985	
    Loss 922742.85981691	
    Loss 921445.98290062	
    Loss 920150.59375454	
    Loss 918857.10828752	
    Loss 917565.25080512	
    Loss 916275.54395453	
    Loss 914987.54832974	
    Loss 913701.07862116	
    Loss 912416.56978566	
    Loss 911133.77437771	
    Loss 909853.11132361	
    Loss 908574.049383	
    Loss 907296.84547014	
    Loss 906021.34810981	
    Loss 904747.78881731	
    Loss 903475.76295876	
    Loss 902206.00510642	
    Loss 900937.59098935	
    Loss 899670.3252495	
    Loss 898405.65060628	
    Loss 897142.85900333	
    Loss 895881.89396921	
    Loss 894622.67311896	
    Loss 893365.183811	
    Loss 892108.89968457	
    Loss 890855.30737308	
    Loss 889602.98665904	
    Loss 888352.3517876	
    Loss 887103.33190615	
    Loss 885855.88400624	
    Loss 884610.64551731	
    Loss 883367.00785193	
    Loss 882125.10057883	
    Loss 880885.22510119	
    Loss 879646.72689367	
    Loss 878410.1714888	
    Loss 877175.06541139	
    Loss 875942.34515581	
    Loss 874710.36987418	
    Loss 873480.90443167	
    Loss 872253.10508085	
    Loss 871026.62609033	
    Loss 869801.97612708	
    Loss 868578.8841669	
    Loss 867357.9102305	
    Loss 866138.3822779	
    Loss 864921.18442936	
    Loss 863705.17284669	
    Loss 862490.99839467	
    Loss 861278.40553437	
    Loss 860068.10749929	
    Loss 858859.0034388	
    Loss 857651.7890986	
    Loss 856446.06493987	
    Loss 855242.06987181	
    Loss 854039.76017916	
    Loss 852839.10314157	
    Loss 851640.22558344	
    Loss 850443.14034325	
    Loss 849247.82531753	
    Loss 848054.28004669	
    Loss 846862.40094896	
    Loss 845671.74902777	
    Loss 844482.64606067	
    Loss 843295.52877574	
    Loss 842110.02829878	
    Loss 840925.98123793	
    Loss 839743.92539821	
    Loss 838563.80105989	
    Loss 837384.89494045	
    Loss 836207.97518962	
Epoch 6	
 79717
  4381
   916
   275
 11677
    28
    30
  7568
 21457
  4167
  1092
    35
    19
    39
    22
   315
    27
    43
[torch.DoubleTensor of size 18]

Validation accuracy:	0.095532896334062	
Grad norm	159.40758062279	
    Loss 835502.5467224	
    Loss 834328.03984947	
    Loss 833155.02084344	
    Loss 831983.61787973	
    Loss 830813.95503198	
    Loss 829645.92462548	
    Loss 828479.91208313	
    Loss 827314.88534838	
    Loss 826151.72669132	
    Loss 824990.4580192	
    Loss 823830.70796521	
    Loss 822672.90964893	
    Loss 821516.75585464	
    Loss 820362.25971748	
    Loss 819209.43112513	
    Loss 818058.09457353	
    Loss 816907.98951926	
    Loss 815759.60187776	
    Loss 814612.81520147	
    Loss 813467.83742481	
    Loss 812323.84757038	
    Loss 811181.77761892	
    Loss 810041.37612139	
    Loss 808902.38361765	
    Loss 807765.67230066	
    Loss 806630.289002	
    Loss 805496.49877385	
    Loss 804364.42476667	
    Loss 803233.95534828	
    Loss 802104.88278718	
    Loss 800977.31027036	
    Loss 799851.34157623	
    Loss 798726.92087498	
    Loss 797603.90456165	
    Loss 796482.67880939	
    Loss 795362.91091005	
    Loss 794244.96043016	
    Loss 793128.50139557	
    Loss 792013.37954441	
    Loss 790900.17116908	
    Loss 789788.48987886	
    Loss 788678.29104252	
    Loss 787569.64266596	
    Loss 786462.87993347	
    Loss 785357.24187818	
    Loss 784253.00717486	
    Loss 783150.53472764	
    Loss 782049.91978137	
    Loss 780950.69764483	
    Loss 779853.2004924	
    Loss 778757.1659507	
    Loss 777662.38034074	
    Loss 776569.54338337	
    Loss 775477.9233806	
    Loss 774387.95832363	
    Loss 773299.04843172	
    Loss 772211.71365575	
    Loss 771126.28391771	
    Loss 770042.38786735	
    Loss 768959.76494391	
    Loss 767878.95508355	
    Loss 766799.664703	
    Loss 765721.51808294	
    Loss 764645.3945228	
    Loss 763570.48176051	
    Loss 762497.13088683	
    Loss 761425.2397806	
    Loss 760355.01063112	
    Loss 759286.24927253	
    Loss 758218.55728287	
    Loss 757153.19417022	
    Loss 756088.84033882	
    Loss 755026.08599027	
    Loss 753965.13460016	
    Loss 752905.37390491	
    Loss 751847.182761	
    Loss 750790.30381391	
    Loss 749735.21400587	
    Loss 748681.5028967	
    Loss 747629.04520071	
    Loss 746578.16720811	
    Loss 745528.72525653	
    Loss 744481.00942605	
    Loss 743434.60564979	
    Loss 742389.72714935	
    Loss 741346.2404578	
    Loss 740304.33049074	
    Loss 739263.66730891	
    Loss 738224.89065483	
    Loss 737187.20125941	
    Loss 736150.39495247	
    Loss 735115.76740819	
    Loss 734082.68938384	
    Loss 733051.09533744	
    Loss 732020.91551895	
    Loss 730992.17055286	
    Loss 729964.36528286	
    Loss 728938.81059717	
    Loss 727914.28067591	
    Loss 726891.13343411	
    Loss 725869.30758484	
    Loss 724848.74145262	
    Loss 723830.00161681	
    Loss 722812.56691183	
    Loss 721796.55187269	
    Loss 720782.19740888	
    Loss 719768.96282505	
    Loss 718757.3311556	
    Loss 717746.86285745	
    Loss 716738.38711782	
    Loss 715730.46746215	
    Loss 714724.64298541	
    Loss 713720.17188532	
    Loss 712716.77245736	
    Loss 711714.87354107	
    Loss 710714.25129915	
    Loss 709715.36403086	
    Loss 708717.64389428	
    Loss 707721.85455012	
    Loss 706726.99380763	
    Loss 705733.66932304	
    Loss 704741.63527038	
    Loss 703751.5025417	
    Loss 702762.31447245	
    Loss 701774.68339065	
    Loss 700788.2521727	
    Loss 699803.2458946	
    Loss 698819.63428671	
    Loss 697837.35961928	
    Loss 696856.52660935	
    Loss 695877.18427021	
    Loss 694899.29587538	
    Loss 693922.84336451	
    Loss 692947.74773055	
    Loss 691973.65246817	
    Loss 691000.81769465	
    Loss 690029.62474747	
    Loss 689059.74135441	
    Loss 688091.03390667	
    Loss 687123.99113699	
    Loss 686158.5440493	
    Loss 685194.0545242	
    Loss 684231.2168434	
Epoch 7	
 85339
  3532
   631
   157
 10524
    12
    11
  6513
 20769
  3315
   760
    16
     5
    14
     8
   165
    15
    22
[torch.DoubleTensor of size 18]

Validation accuracy:	0.097839281378975	
Grad norm	144.67625032033	
    Loss 683654.07805438	
    Loss 682693.19384669	
    Loss 681733.50710664	
    Loss 680775.16589421	
    Loss 679818.24116822	
    Loss 678862.64376272	
    Loss 677908.72425617	
    Loss 676955.57513082	
    Loss 676003.97251111	
    Loss 675053.92485605	
    Loss 674105.11893129	
    Loss 673157.9249842	
    Loss 672212.05888946	
    Loss 671267.55535817	
    Loss 670324.41226078	
    Loss 669382.51169241	
    Loss 668441.56463357	
    Loss 667502.04923626	
    Loss 666563.85056632	
    Loss 665627.12378578	
    Loss 664691.19270338	
    Loss 663756.82337551	
    Loss 662823.84085243	
    Loss 661891.99859425	
    Loss 660962.04341975	
    Loss 660033.15001659	
    Loss 659105.56605657	
    Loss 658179.39268182	
    Loss 657254.55693625	
    Loss 656330.82590611	
    Loss 655408.34599487	
    Loss 654487.17270898	
    Loss 653567.25246822	
    Loss 652648.47840696	
    Loss 651731.18026404	
    Loss 650815.07442001	
    Loss 649900.46095858	
    Loss 648987.05503185	
    Loss 648074.74400744	
    Loss 647164.0064118	
    Loss 646254.51016951	
    Loss 645346.23493114	
    Loss 644439.22037213	
    Loss 643533.76552775	
    Loss 642629.22353274	
    Loss 641725.80900468	
    Loss 640823.84264089	
    Loss 639923.42018047	
    Loss 639024.1231307	
    Loss 638126.23691114	
    Loss 637229.54514266	
    Loss 636333.86419904	
    Loss 635439.80047622	
    Loss 634546.71579367	
    Loss 633655.0008159	
    Loss 632764.12222841	
    Loss 631874.51736841	
    Loss 630986.51288076	
    Loss 630099.75778747	
    Loss 629214.01791279	
    Loss 628329.76972416	
    Loss 627446.78911215	
    Loss 626564.70998795	
    Loss 625684.30196373	
    Loss 624804.88349322	
    Loss 623926.74149523	
    Loss 623049.7956996	
    Loss 622174.21775593	
    Loss 621299.83453247	
    Loss 620426.30282447	
    Loss 619554.71825222	
    Loss 618683.9193205	
    Loss 617814.45720932	
    Loss 616946.48304982	
    Loss 616079.45781065	
    Loss 615213.73216397	
    Loss 614349.06193839	
    Loss 613485.8769609	
    Loss 612623.8089171	
    Loss 611762.7675959	
    Loss 610902.99828864	
    Loss 610044.43348984	
    Loss 609187.26175286	
    Loss 608331.16874205	
    Loss 607476.32647748	
    Loss 606622.61643415	
    Loss 605770.18991167	
    Loss 604918.77803207	
    Loss 604068.93845652	
    Loss 603219.98053064	
    Loss 602371.69690403	
    Loss 601525.24068872	
    Loss 600680.05997328	
    Loss 599836.08769052	
    Loss 598993.25863267	
    Loss 598151.62088733	
    Loss 597310.71208003	
    Loss 596471.68466223	
    Loss 595633.48545826	
    Loss 594796.42033611	
    Loss 593960.4379662	
    Loss 593125.4623276	
    Loss 592291.99856922	
    Loss 591459.59588514	
    Loss 590628.36076494	
    Loss 589798.48172371	
    Loss 588969.510659	
    Loss 588141.86537271	
    Loss 587315.15313614	
    Loss 586490.09810897	
    Loss 585665.45830696	
    Loss 584842.56821221	
    Loss 584020.77677709	
    Loss 583199.8579321	
    Loss 582380.16868759	
    Loss 581561.52419684	
    Loss 580744.2994215	
    Loss 579928.02045327	
    Loss 579113.33650799	
    Loss 578299.38070397	
    Loss 577486.70859215	
    Loss 576675.08868392	
    Loss 575865.0476379	
    Loss 575055.74756428	
    Loss 574247.73467547	
    Loss 573440.68647259	
    Loss 572634.8140417	
    Loss 571830.09441238	
    Loss 571026.459674	
    Loss 570223.98839512	
    Loss 569422.75649794	
    Loss 568622.72087488	
    Loss 567823.8451968	
    Loss 567026.0802947	
    Loss 566229.13163196	
    Loss 565433.20695989	
    Loss 564638.64075971	
    Loss 563845.13320961	
    Loss 563052.57801355	
    Loss 562261.41314229	
    Loss 561471.56639938	
    Loss 560682.47245361	
    Loss 559894.75180843	
Epoch 8	
 91031
  2721
   366
    84
  9120
     5
     1
  5502
 19845
  2555
   466
     1
     2
     5
     3
    90
     5
     6
[torch.DoubleTensor of size 18]

Validation accuracy:	0.099470441854819	
Grad norm	131.35495008214	
    Loss 559422.55605202	
    Loss 558636.41688475	
    Loss 557851.24121306	
    Loss 557067.18825084	
    Loss 556284.28645199	
    Loss 555502.46139219	
    Loss 554722.03146526	
    Loss 553942.20444902	
    Loss 553163.65640176	
    Loss 552386.388014	
    Loss 551610.13380972	
    Loss 550835.2115816	
    Loss 550061.36103731	
    Loss 549288.62958288	
    Loss 548517.01044332	
    Loss 547746.42622109	
    Loss 546976.57781432	
    Loss 546207.9216459	
    Loss 545440.34928317	
    Loss 544673.97206451	
    Loss 543908.2377862	
    Loss 543143.77570926	
    Loss 542380.46445821	
    Loss 541618.07590799	
    Loss 540857.24700942	
    Loss 540097.26393783	
    Loss 539338.36028472	
    Loss 538580.61678921	
    Loss 537823.98778453	
    Loss 537068.22956963	
    Loss 536313.51756327	
    Loss 535559.86947448	
    Loss 534807.23434593	
    Loss 534055.53546386	
    Loss 533305.05352106	
    Loss 532555.54701455	
    Loss 531807.26689379	
    Loss 531059.96163573	
    Loss 530313.55403841	
    Loss 529568.44061297	
    Loss 528824.33852584	
    Loss 528081.24273704	
    Loss 527339.16622886	
    Loss 526598.38249322	
    Loss 525858.3412619	
    Loss 525119.20336331	
    Loss 524381.25573444	
    Loss 523644.59050505	
    Loss 522908.83483042	
    Loss 522174.23203833	
    Loss 521440.6049117	
    Loss 520707.79832974	
    Loss 519976.32964185	
    Loss 519245.6516443	
    Loss 518516.10720678	
    Loss 517787.22709919	
    Loss 517059.37168297	
    Loss 516332.86265225	
    Loss 515607.3722026	
    Loss 514882.69106253	
    Loss 514159.23393761	
    Loss 513436.8360554	
    Loss 512715.14740466	
    Loss 511994.83870189	
    Loss 511275.34327699	
    Loss 510556.88777141	
    Loss 509839.41464948	
    Loss 509123.06640404	
    Loss 508407.69054539	
    Loss 507692.99113314	
    Loss 506979.91816352	
    Loss 506267.45416479	
    Loss 505556.10775115	
    Loss 504845.99111319	
    Loss 504136.62668041	
    Loss 503428.34299523	
    Loss 502720.90599352	
    Loss 502014.7002011	
    Loss 501309.40313144	
    Loss 500604.94550602	
    Loss 499901.51123998	
    Loss 499199.08818606	
    Loss 498497.78624622	
    Loss 497797.37353605	
    Loss 497097.98570108	
    Loss 496399.51643098	
    Loss 495702.09232321	
    Loss 495005.49469092	
    Loss 494310.21007708	
    Loss 493615.64130751	
    Loss 492921.58337918	
    Loss 492229.05505461	
    Loss 491537.57722705	
    Loss 490847.08510003	
    Loss 490157.51431545	
    Loss 489468.93460075	
    Loss 488780.91666087	
    Loss 488094.47189696	
    Loss 487408.69770815	
    Loss 486723.85404026	
    Loss 486039.89872302	
    Loss 485356.74551347	
    Loss 484674.84563298	
    Loss 483993.80630689	
    Loss 483313.72953385	
    Loss 482634.75903799	
    Loss 481956.52270489	
    Loss 481279.38544431	
    Loss 480602.99533148	
    Loss 479927.97879012	
    Loss 479253.27181713	
    Loss 478580.02707093	
    Loss 477907.67371797	
    Loss 477236.0332344	
    Loss 476565.39896809	
    Loss 475895.61876567	
    Loss 475226.99921399	
    Loss 474559.14875329	
    Loss 473892.61261162	
    Loss 473226.64779229	
    Loss 472561.75529995	
    Loss 471897.71977439	
    Loss 471234.99887889	
    Loss 470572.8535129	
    Loss 469911.77549672	
    Loss 469251.47204956	
    Loss 468592.13895231	
    Loss 467933.75861538	
    Loss 467276.26098418	
    Loss 466619.69850577	
    Loss 465964.16657846	
    Loss 465309.62055804	
    Loss 464656.00741904	
    Loss 464003.30769174	
    Loss 463351.27466511	
    Loss 462700.07207533	
    Loss 462049.99438289	
    Loss 461400.77079047	
    Loss 460752.31880238	
    Loss 460105.02743968	
    Loss 459458.82504	
    Loss 458813.21426274	
    Loss 458168.74561406	
Epoch 9	
 96545
  2069
   212
    44
  7745
     0
     0
  4512
 18479
  1889
   266
     1
     0
     3
     0
    40
     1
     2
[torch.DoubleTensor of size 18]

Validation accuracy:	0.09998634377276	
Grad norm	119.3070453334	
    Loss 457782.39969626	
    Loss 457139.21056937	
    Loss 456496.79645188	
    Loss 455855.32092848	
    Loss 455214.77972034	
    Loss 454575.10955663	
    Loss 453936.59989026	
    Loss 453298.56223926	
    Loss 452661.58172994	
    Loss 452025.65443829	
    Loss 451390.55475124	
    Loss 450756.55614969	
    Loss 450123.42139431	
    Loss 449491.2055542	
    Loss 448859.90114822	
    Loss 448229.45881585	
    Loss 447599.5818147	
    Loss 446970.69691994	
    Loss 446342.70509976	
    Loss 445715.68100682	
    Loss 445089.17841837	
    Loss 444463.71339272	
    Loss 443839.20233017	
    Loss 443215.4374388	
    Loss 442592.96081021	
    Loss 441971.15519391	
    Loss 441350.24148464	
    Loss 440730.28277771	
    Loss 440111.2527516	
    Loss 439492.90617941	
    Loss 438875.43685235	
    Loss 438258.83300524	
    Loss 437643.04706509	
    Loss 437028.02629258	
    Loss 436414.00921733	
    Loss 435800.79027788	
    Loss 435188.57993583	
    Loss 434577.15410753	
    Loss 433966.46557341	
    Loss 433356.84163167	
    Loss 432748.04321609	
    Loss 432140.0750134	
    Loss 431532.92586534	
    Loss 430926.85055287	
    Loss 430321.3803705	
    Loss 429716.63161689	
    Loss 429112.86151718	
    Loss 428510.15627604	
    Loss 427908.18617616	
    Loss 427307.1572503	
    Loss 426706.92525495	
    Loss 426107.36145579	
    Loss 425508.90135476	
    Loss 424911.08191412	
    Loss 424314.2009835	
    Loss 423717.84868948	
    Loss 423122.31609359	
    Loss 422527.91791523	
    Loss 421934.35071823	
    Loss 421341.42823952	
    Loss 420749.50788659	
    Loss 420158.47426099	
    Loss 419567.99731305	
    Loss 418978.65893138	
    Loss 418389.99303681	
    Loss 417802.17176127	
    Loss 417215.15899311	
    Loss 416629.07058828	
    Loss 416043.77358753	
    Loss 415459.01214179	
    Loss 414875.60878054	
    Loss 414292.67434707	
    Loss 413710.67495123	
    Loss 413129.69354412	
    Loss 412549.30455343	
    Loss 411969.81834587	
    Loss 411391.00918967	
    Loss 410813.21968733	
    Loss 410236.1721557	
    Loss 409659.8100357	
    Loss 409084.27008489	
    Loss 408509.58006543	
    Loss 407935.789685	
    Loss 407362.73394333	
    Loss 406790.51773707	
    Loss 406219.04445645	
    Loss 405648.42223269	
    Loss 405078.47401238	
    Loss 404509.62438695	
    Loss 403941.35647977	
    Loss 403373.47200525	
    Loss 402806.86565917	
    Loss 402241.12515109	
    Loss 401676.18967505	
    Loss 401111.99432306	
    Loss 400548.62501385	
    Loss 399985.68528322	
    Loss 399424.0615368	
    Loss 398862.98152646	
    Loss 398302.665443	
    Loss 397743.07903211	
    Loss 397184.12952217	
    Loss 396626.22039498	
    Loss 396069.00764465	
    Loss 395512.59032878	
    Loss 394957.07431604	
    Loss 394402.15040144	
    Loss 393848.13967986	
    Loss 393294.72582483	
    Loss 392742.44706378	
    Loss 392190.398396	
    Loss 391639.57278165	
    Loss 391089.47001548	
    Loss 390539.95173807	
    Loss 389991.25573238	
    Loss 389443.25726964	
    Loss 388896.20665883	
    Loss 388349.78337214	
    Loss 387804.4407018	
    Loss 387259.5462463	
    Loss 386715.54762523	
    Loss 386172.24586878	
    Loss 385630.04200959	
    Loss 385088.28004501	
    Loss 384547.40550142	
    Loss 384007.15211693	
    Loss 383467.69910538	
    Loss 382929.03408253	
    Loss 382391.08892486	
    Loss 381853.89211967	
    Loss 381317.55252868	
    Loss 380782.0268781	
    Loss 380247.24795405	
    Loss 379713.22305581	
    Loss 379179.74348808	
    Loss 378646.93612963	
    Loss 378115.06114994	
    Loss 377583.8736227	
    Loss 377053.31131178	
    Loss 376523.71782566	
    Loss 375995.0241526	
    Loss 375466.79511442	
    Loss 374939.51662638	
Epoch 10	
 101793
   1525
    121
     22
   6497
      0
      0
   3580
  16758
   1348
    143
      0
      0
      1
      0
     18
      1
      1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.10038844379704	
Grad norm	108.41055858525	
    Loss 374623.40406612	
    Loss 374097.15971206	
    Loss 373571.53851524	
    Loss 373046.70349403	
    Loss 372522.6258671	
    Loss 371999.25058731	
    Loss 371476.84142182	
    Loss 370954.80156193	
    Loss 370433.63533587	
    Loss 369913.33596518	
    Loss 369393.71149934	
    Loss 368874.9975365	
    Loss 368356.97855793	
    Loss 367839.71423181	
    Loss 367323.1984874	
    Loss 366807.400827	
    Loss 366292.03509726	
    Loss 365777.49444768	
    Loss 365263.69079691	
    Loss 364750.66795459	
    Loss 364238.07004932	
    Loss 363726.31919887	
    Loss 363215.35850967	
    Loss 362705.00069786	
    Loss 362195.7065676	
    Loss 361686.94219587	
    Loss 361178.91614917	
    Loss 360671.67719297	
    Loss 360165.21254198	
    Loss 359659.28130762	
    Loss 359154.08788487	
    Loss 358649.59746372	
    Loss 358145.76683722	
    Loss 357642.56163647	
    Loss 357140.18449184	
    Loss 356638.46041079	
    Loss 356137.56658303	
    Loss 355637.30168651	
    Loss 355137.64392434	
    Loss 354638.86175762	
    Loss 354140.75460707	
    Loss 353643.33335849	
    Loss 353146.5656955	
    Loss 352650.69209059	
    Loss 352155.31264389	
    Loss 351660.50718931	
    Loss 351166.50672629	
    Loss 350673.39049089	
    Loss 350180.86838977	
    Loss 349689.11372598	
    Loss 349198.00990688	
    Loss 348707.45201123	
    Loss 348217.80229961	
    Loss 347728.67317788	
    Loss 347240.32174214	
    Loss 346752.39167142	
    Loss 346265.11309014	
    Loss 345778.79191404	
    Loss 345293.14910692	
    Loss 344808.01927291	
    Loss 344323.70832181	
    Loss 343840.14107831	
    Loss 343357.00946466	
    Loss 342874.81601152	
    Loss 342393.18243246	
    Loss 341912.23266964	
    Loss 341431.94975535	
    Loss 340952.42586649	
    Loss 340473.54603874	
    Loss 339995.08833043	
    Loss 339517.76475654	
    Loss 339040.79889509	
    Loss 338564.61635791	
    Loss 338089.27774728	
    Loss 337614.40208914	
    Loss 337140.28407883	
    Loss 336666.7053318	
    Loss 336193.97053391	
    Loss 335721.84350645	
    Loss 335250.27558859	
    Loss 334779.3668929	
    Loss 334309.17362578	
    Loss 333839.69997913	
    Loss 333370.8344349	
    Loss 332902.65667619	
    Loss 332435.07757989	
    Loss 331968.19124955	
    Loss 331501.85547172	
    Loss 331036.44055337	
    Loss 330571.49869781	
    Loss 330106.84138803	
    Loss 329643.25040839	
    Loss 329180.37381391	
    Loss 328718.15533451	
    Loss 328256.52927091	
    Loss 327795.59292369	
    Loss 327334.98203194	
    Loss 326875.47241169	
    Loss 326416.40435018	
    Loss 325957.96388314	
    Loss 325500.1233359	
    Loss 325042.78655529	
    Loss 324586.31456846	
    Loss 324130.40487464	
    Loss 323675.15412583	
    Loss 323220.63641711	
    Loss 322766.59509658	
    Loss 322313.3142788	
    Loss 321860.50871942	
    Loss 321408.63860835	
    Loss 320956.9388265	
    Loss 320506.26302209	
    Loss 320056.17309461	
    Loss 319606.56429491	
    Loss 319157.62654871	
    Loss 318709.25801724	
    Loss 318261.66253052	
    Loss 317814.58021649	
    Loss 317368.38451172	
    Loss 316922.54021793	
    Loss 316477.44460495	
    Loss 316032.91458015	
    Loss 315589.30438818	
    Loss 315146.02871205	
    Loss 314703.49259976	
    Loss 314261.45409823	
    Loss 313820.07498765	
    Loss 313379.34853783	
    Loss 312939.2103633	
    Loss 312499.6681045	
    Loss 312060.839269	
    Loss 311622.68366323	
    Loss 311185.12263892	
    Loss 310748.18619286	
    Loss 310311.69665397	
    Loss 309875.75006831	
    Loss 309440.57719933	
    Loss 309005.95604651	
    Loss 308571.84145382	
    Loss 308138.53584634	
    Loss 307705.97391959	
    Loss 307273.77654379	
    Loss 306842.37104542	
Epoch 11	
 106756
   1080
     55
      6
   5260
      0
      0
   2723
  14933
    914
     74
      0
      0
      1
      0
      5
      1
[torch.DoubleTensor of size 17]

Validation accuracy:	0.10096504005827	
Grad norm	98.555301131164	
    Loss 306583.71800721	
    Loss 306153.14714172	
    Loss 305723.07709014	
    Loss 305293.66687061	
    Loss 304864.86982165	
    Loss 304436.63693785	
    Loss 304009.20928713	
    Loss 303582.07014748	
    Loss 303155.65307184	
    Loss 302729.94941094	
    Loss 302304.79585626	
    Loss 301880.3953394	
    Loss 301456.55253237	
    Loss 301033.32973934	
    Loss 300610.72335838	
    Loss 300188.71502546	
    Loss 299767.03441626	
    Loss 299346.03964534	
    Loss 298925.65424529	
    Loss 298505.89602159	
    Loss 298086.48592039	
    Loss 297667.76795983	
    Loss 297249.70408803	
    Loss 296832.12682872	
    Loss 296415.42758323	
    Loss 295999.14418083	
    Loss 295583.47279041	
    Loss 295168.45123689	
    Loss 294754.07587101	
    Loss 294340.11379116	
    Loss 293926.77439274	
    Loss 293514.00476932	
    Loss 293101.76706822	
    Loss 292690.04041606	
    Loss 292278.99710229	
    Loss 291868.48807189	
    Loss 291458.66326559	
    Loss 291049.34028884	
    Loss 290640.518996	
    Loss 290232.41748513	
    Loss 289824.868799	
    Loss 289417.88780646	
    Loss 289011.42407907	
    Loss 288605.70660159	
    Loss 288200.39357276	
    Loss 287795.53521541	
    Loss 287391.33925343	
    Loss 286987.8777332	
    Loss 286584.89633059	
    Loss 286182.53978603	
    Loss 285780.71469154	
    Loss 285379.33713907	
    Loss 284978.70511132	
    Loss 284578.49722168	
    Loss 284178.93459435	
    Loss 283779.70835209	
    Loss 283380.99595633	
    Loss 282983.09307533	
    Loss 282585.74431065	
    Loss 282188.80294652	
    Loss 281792.52959152	
    Loss 281396.88126775	
    Loss 281001.57251874	
    Loss 280607.03537323	
    Loss 280212.96797221	
    Loss 279819.45225499	
    Loss 279426.48778688	
    Loss 279034.14610115	
    Loss 278642.32848349	
    Loss 278250.841667	
    Loss 277860.30246083	
    Loss 277470.03259378	
    Loss 277080.42013682	
    Loss 276691.50820584	
    Loss 276302.95465147	
    Loss 275915.04008149	
    Loss 275527.55292712	
    Loss 275140.76406802	
    Loss 274754.4745764	
    Loss 274368.64086189	
    Loss 273983.33430507	
    Loss 273598.63103165	
    Loss 273214.50128476	
    Loss 272830.87581834	
    Loss 272447.8140939	
    Loss 272065.23262528	
    Loss 271683.21469328	
    Loss 271301.64745634	
    Loss 270920.85340788	
    Loss 270540.444368	
    Loss 270160.24349642	
    Loss 269780.93087532	
    Loss 269402.20851222	
    Loss 269024.02457763	
    Loss 268646.31285587	
    Loss 268269.17800226	
    Loss 267892.28698534	
    Loss 267516.31782559	
    Loss 267140.70790562	
    Loss 266765.61399507	
    Loss 266391.01372931	
    Loss 266016.81014323	
    Loss 265643.32644709	
    Loss 265270.29573563	
    Loss 264897.81234648	
    Loss 264525.92377784	
    Loss 264154.41768441	
    Loss 263783.5464534	
    Loss 263413.05202433	
    Loss 263043.32642123	
    Loss 262673.72636195	
    Loss 262304.98443009	
    Loss 261936.71714401	
    Loss 261568.84763936	
    Loss 261201.5250839	
    Loss 260834.66660469	
    Loss 260468.4375367	
    Loss 260102.62945964	
    Loss 259737.547259	
    Loss 259372.74034851	
    Loss 259008.55953807	
    Loss 258644.8367393	
    Loss 258281.88717478	
    Loss 257919.18627901	
    Loss 257557.10302368	
    Loss 257195.41809982	
    Loss 256834.27542048	
    Loss 256473.674498	
    Loss 256113.55521634	
    Loss 255753.90775626	
    Loss 255394.85446132	
    Loss 255036.3591618	
    Loss 254678.33442026	
    Loss 254320.82866476	
    Loss 253963.69006668	
    Loss 253606.98888294	
    Loss 253250.93065869	
    Loss 252895.31358869	
    Loss 252540.10689319	
    Loss 252185.5761648	
    Loss 251831.66019983	
    Loss 251478.02992917	
    Loss 251125.06013014	
Epoch 12	
 111207
    724
     18
      1
   4116
      0
      0
   1954
  13147
    602
     38
      0
      0
      0
      0
      1
[torch.DoubleTensor of size 16]

Validation accuracy:	0.10153404952658	
Grad norm	89.641802754341	
    Loss 250913.41739808	
    Loss 250561.12003804	
    Loss 250209.22493142	
    Loss 249857.88481194	
    Loss 249507.04033009	
    Loss 249156.64698339	
    Loss 248806.92578725	
    Loss 248457.4294638	
    Loss 248108.53015652	
    Loss 247760.21788367	
    Loss 247412.35381749	
    Loss 247065.112575	
    Loss 246718.31743817	
    Loss 246372.03203883	
    Loss 246026.25567786	
    Loss 245680.9772191	
    Loss 245335.9452885	
    Loss 244991.4832153	
    Loss 244647.52622198	
    Loss 244304.07013575	
    Loss 243960.90105848	
    Loss 243618.29800495	
    Loss 243276.23611231	
    Loss 242934.56657811	
    Loss 242593.62164314	
    Loss 242253.00086857	
    Loss 241912.88785445	
    Loss 241573.31264065	
    Loss 241234.27731551	
    Loss 240895.55916576	
    Loss 240557.36857428	
    Loss 240219.63838308	
    Loss 239882.33692507	
    Loss 239545.45274367	
    Loss 239209.13263001	
    Loss 238873.24947987	
    Loss 238537.93102105	
    Loss 238203.01058766	
    Loss 237868.50655847	
    Loss 237534.59367147	
    Loss 237201.13421089	
    Loss 236868.14566429	
    Loss 236535.56237475	
    Loss 236203.60365234	
    Loss 235871.97684851	
    Loss 235540.70831262	
    Loss 235209.98499032	
    Loss 234879.87200485	
    Loss 234550.14697112	
    Loss 234220.92976775	
    Loss 233892.14646738	
    Loss 233563.73136368	
    Loss 233235.92682331	
    Loss 232908.46870876	
    Loss 232581.54693696	
    Loss 232254.89411174	
    Loss 231928.64278429	
    Loss 231603.07735748	
    Loss 231277.96480762	
    Loss 230953.17499777	
    Loss 230628.92926678	
    Loss 230305.21001756	
    Loss 229981.75411936	
    Loss 229658.93127757	
    Loss 229336.5060525	
    Loss 229014.52410845	
    Loss 228692.99893606	
    Loss 228371.98433723	
    Loss 228051.39605779	
    Loss 227731.06509702	
    Loss 227411.52661581	
    Loss 227092.18728254	
    Loss 226773.40095713	
    Loss 226455.19670978	
    Loss 226137.26664528	
    Loss 225819.87833286	
    Loss 225502.8267161	
    Loss 225186.3528099	
    Loss 224870.2904493	
    Loss 224554.59947253	
    Loss 224239.32857398	
    Loss 223924.56745318	
    Loss 223610.26145564	
    Loss 223296.37440226	
    Loss 222982.94982253	
    Loss 222669.90837805	
    Loss 222357.32487273	
    Loss 222045.11137272	
    Loss 221733.54808505	
    Loss 221422.29846974	
    Loss 221111.19838457	
    Loss 220800.83696131	
    Loss 220490.96415085	
    Loss 220181.53213254	
    Loss 219872.47463513	
    Loss 219563.90043357	
    Loss 219255.50645047	
    Loss 218947.88422211	
    Loss 218640.5549498	
    Loss 218333.65033465	
    Loss 218027.15218851	
    Loss 217720.96475225	
    Loss 217415.37754702	
    Loss 217110.15434366	
    Loss 216805.38709387	
    Loss 216501.10102705	
    Loss 216197.12148722	
    Loss 215893.67325429	
    Loss 215590.52210461	
    Loss 215288.00102447	
    Loss 214985.57215721	
    Loss 214683.86308488	
    Loss 214382.53841146	
    Loss 214081.54421103	
    Loss 213780.99527471	
    Loss 213480.82430715	
    Loss 213181.16469806	
    Loss 212881.85146793	
    Loss 212583.13112447	
    Loss 212284.62630095	
    Loss 211986.64551382	
    Loss 211689.03466385	
    Loss 211392.07607397	
    Loss 211095.29786147	
    Loss 210799.03645597	
    Loss 210503.09380325	
    Loss 210207.59589485	
    Loss 209912.54901445	
    Loss 209617.8971613	
    Loss 209323.61633737	
    Loss 209029.83075784	
    Loss 208736.50872613	
    Loss 208443.55638586	
    Loss 208151.03661417	
    Loss 207858.81942631	
    Loss 207566.95351257	
    Loss 207275.62278294	
    Loss 206984.64313556	
    Loss 206693.99592961	
    Loss 206403.91409791	
    Loss 206114.34053464	
    Loss 205824.99058994	
    Loss 205536.19227041	
Epoch 13	
 115194
    469
      8
      0
   3152
      0
      0
   1309
  11291
    370
     15
[torch.DoubleTensor of size 11]

Validation accuracy:	0.10181476086429	
Grad norm	81.579974308191	
    Loss 205363.01183671	
    Loss 205074.75412877	
    Loss 204786.81929628	
    Loss 204499.35234921	
    Loss 204212.28526142	
    Loss 203925.57698344	
    Loss 203639.43079778	
    Loss 203353.45933938	
    Loss 203067.9818267	
    Loss 202782.9872128	
    Loss 202498.35765249	
    Loss 202214.24315767	
    Loss 201930.48405739	
    Loss 201647.14441259	
    Loss 201364.22649557	
    Loss 201081.72285626	
    Loss 200799.40270072	
    Loss 200517.55586006	
    Loss 200236.12894078	
    Loss 199955.09914671	
    Loss 199674.30774372	
    Loss 199393.97960053	
    Loss 199114.09894692	
    Loss 198834.53432299	
    Loss 198555.56755973	
    Loss 198276.85133105	
    Loss 197998.55656382	
    Loss 197720.70826863	
    Loss 197443.31163684	
    Loss 197166.15538403	
    Loss 196889.448039	
    Loss 196613.11121231	
    Loss 196337.12002004	
    Loss 196061.4691229	
    Loss 195786.2839722	
    Loss 195511.45598996	
    Loss 195237.0948984	
    Loss 194963.04707056	
    Loss 194689.34679823	
    Loss 194416.13131516	
    Loss 194143.28826031	
    Loss 193870.83692542	
    Loss 193598.69902485	
    Loss 193327.08538557	
    Loss 193055.74503766	
    Loss 192784.6852203	
    Loss 192514.07422845	
    Loss 192243.97078997	
    Loss 191974.18084742	
    Loss 191704.802654	
    Loss 191435.77871211	
    Loss 191167.05883597	
    Loss 190898.83777609	
    Loss 190630.90037195	
    Loss 190363.40980641	
    Loss 190096.13459682	
    Loss 189829.16939233	
    Loss 189562.78671407	
    Loss 189296.77436929	
    Loss 189031.01685343	
    Loss 188765.70180724	
    Loss 188500.8313579	
    Loss 188236.1637775	
    Loss 187972.01391466	
    Loss 187708.20398522	
    Loss 187444.74851658	
    Loss 187181.67249143	
    Loss 186919.01465969	
    Loss 186656.70350716	
    Loss 186394.59057597	
    Loss 186133.1412355	
    Loss 185871.83537114	
    Loss 185610.99600095	
    Loss 185350.64061402	
    Loss 185090.49185757	
    Loss 184830.80503639	
    Loss 184571.3813942	
    Loss 184312.43571629	
    Loss 184053.8302645	
    Loss 183795.52737128	
    Loss 183537.55771729	
    Loss 183280.01986093	
    Loss 183022.84128352	
    Loss 182766.01141406	
    Loss 182509.56142422	
    Loss 182253.41495754	
    Loss 181997.64008263	
    Loss 181742.1700777	
    Loss 181487.24764398	
    Loss 181232.58113608	
    Loss 180978.01948311	
    Loss 180724.07088433	
    Loss 180470.52773692	
