[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	1	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4905
 1459
 1496
 1611
 6654
 5392
 3611
 2839
 3329
 1125
 1721
 1384
 2235
 4023
  958
 3166
 3601
 1836
 2737
 3562
 4632
 1058
 1408
 4828
 2223
 4381
  876
 2262
 1424
 1684
 2305
 2800
 8452
 1973
 1836
 2153
 2382
 4560
 3904
 1465
 2034
 1914
 2815
 7134
 3661
[torch.DoubleTensor of size 45]

Validation accuracy:	0.019467710609371	
Grad norm	0	
    Loss 2277813.1278336	
    Loss 2214617.7820798	
    Loss 2153189.1267063	
    Loss 2093471.8862954	
    Loss 2035414.3337146	
    Loss 1978970.4547793	
    Loss 1924097.3889309	
    Loss 1870749.7340666	
    Loss 1818879.8259665	
    Loss 1768449.7309717	
    Loss 1719416.261867	
    Loss 1671744.224922	
    Loss 1625397.4471894	
    Loss 1580337.9093948	
    Loss 1536526.8668	
    Loss 1493931.1495254	
    Loss 1452518.2666868	
    Loss 1412251.3549287	
    Loss 1373103.1527396	
    Loss 1335039.324301	
    Loss 1298030.8465353	
    Loss 1262050.8533166	
    Loss 1227066.3038905	
    Loss 1193053.7687023	
    Loss 1159986.0001769	
    Loss 1127835.3707121	
    Loss 1096574.971068	
    Loss 1066179.8055763	
    Loss 1036628.1751173	
    Loss 1007895.8221418	
    Loss 979962.12358855	
    Loss 952802.66442132	
    Loss 926395.61349963	
    Loss 900719.69198866	
    Loss 875755.95029543	
    Loss 851486.41876521	
    Loss 827890.00919048	
    Loss 804947.42166945	
    Loss 782640.16967922	
    Loss 760951.76840553	
    Loss 739863.58834006	
    Loss 719360.62907017	
    Loss 699426.31782228	
    Loss 680045.98693723	
    Loss 661200.73539484	
    Loss 642877.24530792	
    Loss 625063.68568586	
    Loss 607743.74579072	
    Loss 590902.55508081	
    Loss 574529.89909629	
    Loss 558608.39774944	
    Loss 543129.58197771	
    Loss 528079.662501	
    Loss 513448.45282431	
    Loss 499222.73663911	
    Loss 485389.86678976	
    Loss 471940.94522251	
    Loss 458865.69566138	
    Loss 446151.49535506	
    Loss 433791.3504262	
    Loss 421772.72731191	
    Loss 410087.8755172	
    Loss 398726.77300424	
    Loss 387680.59026335	
    Loss 376939.90354848	
    Loss 366497.66592806	
    Loss 356345.35287905	
    Loss 346473.44544758	
    Loss 336875.24506287	
    Loss 327542.47745116	
    Loss 318469.32852103	
    Loss 309647.45045789	
Epoch 2	
 51790
  6763
    55
 10749
  6818
     0
     3
  3984
 39506
 11877
   135
    37
     0
     0
     5
    58
     8
    10
    10
[torch.DoubleTensor of size 19]

Validation accuracy:	0.094721109492595	
Grad norm	192.26844722151	
    Loss 307049.74417667	
    Loss 298544.29288302	
    Loss 290274.5426465	
    Loss 282234.80371136	
    Loss 274416.55773704	
    Loss 266816.09160894	
    Loss 259426.96900766	
    Loss 252242.86039518	
    Loss 245256.88225667	
    Loss 238464.81894171	
    Loss 231859.93836046	
    Loss 225438.60215778	
    Loss 219195.70392497	
    Loss 213125.82780796	
    Loss 207223.96181247	
    Loss 201485.67436988	
    Loss 195907.19082552	
    Loss 190482.34675749	
    Loss 185208.1907584	
    Loss 180080.02471935	
    Loss 175093.6693232	
    Loss 170246.22347887	
    Loss 165532.85796252	
    Loss 160950.25024326	
    Loss 156494.95866941	
    Loss 152162.99450737	
    Loss 147950.61613933	
    Loss 143855.16769572	
    Loss 139873.18692143	
    Loss 136001.46674266	
    Loss 132237.24277584	
    Loss 128577.37443234	
    Loss 125019.16452257	
    Loss 121558.97438371	
    Loss 118194.96143636	
    Loss 114924.28238832	
    Loss 111744.51698267	
    Loss 108652.52217448	
    Loss 105646.21353889	
    Loss 102723.51802139	
    Loss 99881.439567765	
    Loss 97118.072422698	
    Loss 94431.630439839	
    Loss 91819.630599923	
    Loss 89279.638437277	
    Loss 86810.120405615	
    Loss 84409.254108213	
    Loss 82074.554457292	
    Loss 79804.541337103	
    Loss 77597.718641315	
    Loss 75451.534406714	
    Loss 73365.047699041	
    Loss 71336.455611517	
    Loss 69364.410846646	
    Loss 67447.098039052	
    Loss 65582.166921062	
    Loss 63769.121708467	
    Loss 62006.993784722	
    Loss 60293.117225067	
    Loss 58626.973619994	
    Loss 57006.891665532	
    Loss 55431.711582689	
    Loss 53900.17797063	
    Loss 52411.019040745	
    Loss 50963.21216082	
    Loss 49555.364374184	
    Loss 48186.543882099	
    Loss 46855.620938369	
    Loss 45561.670509106	
    Loss 44303.383392649	
    Loss 43080.295275999	
    Loss 41891.058479387	
Epoch 3	
 77633
   822
     0
  2103
   414
     0
     0
   343
 47989
  2504
[torch.DoubleTensor of size 10]

Validation accuracy:	0.09964493809177	
Grad norm	73.92870600946	
    Loss 41540.808725956	
    Loss 40394.134578307	
    Loss 39279.375944948	
    Loss 38195.483555228	
    Loss 37141.343686153	
    Loss 36116.74199922	
    Loss 35120.597779852	
    Loss 34151.888334892	
    Loss 33210.115299188	
    Loss 32294.626172359	
    Loss 31403.909308645	
    Loss 30538.253010334	
    Loss 29696.529215074	
    Loss 28877.932342374	
    Loss 28082.137698949	
    Loss 27308.453923062	
    Loss 26556.544143985	
    Loss 25825.083460514	
    Loss 25113.939522646	
    Loss 24422.661091736	
    Loss 23750.223633965	
    Loss 23096.724664842	
    Loss 22461.322029967	
    Loss 21843.414354401	
    Loss 21242.705210675	
    Loss 20658.439120724	
    Loss 20090.147054387	
    Loss 19538.10181173	
    Loss 19001.306859017	
    Loss 18479.168827013	
    Loss 17971.533182675	
    Loss 17478.010633901	
    Loss 16998.471494568	
    Loss 16531.835430195	
    Loss 16078.288923694	
    Loss 15637.145073294	
    Loss 15208.331614284	
    Loss 14791.235031138	
    Loss 14385.807755449	
    Loss 13991.786586123	
    Loss 13608.551614643	
    Loss 13235.824078786	
    Loss 12873.727606419	
    Loss 12521.301370262	
    Loss 12178.78911206	
    Loss 11845.924964957	
    Loss 11522.148832606	
    Loss 11207.064021572	
    Loss 10900.924090556	
    Loss 10603.25967976	
    Loss 10313.765991916	
    Loss 10032.368120703	
    Loss 9758.8321961433	
    Loss 9492.9759694689	
    Loss 9234.5439908198	
    Loss 8982.730831588	
    Loss 8738.1143298984	
    Loss 8500.7495763658	
    Loss 8269.6337310728	
    Loss 8044.8780393008	
    Loss 7826.4689403515	
    Loss 7613.9672065412	
    Loss 7407.4449766607	
    Loss 7206.5691845003	
    Loss 7011.3665586925	
    Loss 6821.3934311494	
    Loss 6636.5912899621	
    Loss 6457.0369125936	
    Loss 6282.5412471869	
    Loss 6112.7741417933	
    Loss 5947.9034948613	
    Loss 5787.5843149206	
Epoch 4	
 88092
    36
     0
    74
     0
     0
     0
    22
 43332
   252
[torch.DoubleTensor of size 10]

Validation accuracy:	0.093393420733188	
Grad norm	30.546832505044	
    Loss 5740.2378862765	
    Loss 5585.6516092171	
    Loss 5435.4362769581	
    Loss 5289.1919630248	
    Loss 5147.0501482785	
    Loss 5008.9016585154	
    Loss 4874.5297769907	
    Loss 4743.6564699988	
    Loss 4616.7532211179	
    Loss 4493.5166712682	
    Loss 4373.2230403509	
    Loss 4256.553685039	
    Loss 4142.9886735196	
    Loss 4032.3682333441	
    Loss 3925.0172364606	
    Loss 3820.696412946	
    Loss 3719.3978151689	
    Loss 3620.7165085339	
    Loss 3524.7391827842	
    Loss 3431.7263264372	
    Loss 3340.9611964987	
    Loss 3252.8945359379	
    Loss 3167.3247336254	
    Loss 3083.9536638965	
    Loss 3002.945538544	
    Loss 2923.9743204502	
    Loss 2847.0118259902	
    Loss 2772.7448143331	
    Loss 2700.4837934797	
    Loss 2629.9790002114	
    Loss 2561.463568373	
    Loss 2494.8868960521	
    Loss 2430.4541077609	
    Loss 2367.4956979544	
    Loss 2306.3668423233	
    Loss 2246.7430704041	
    Loss 2188.826587487	
    Loss 2132.4186577021	
    Loss 2077.6991262521	
    Loss 2024.6275119257	
    Loss 1972.9608550759	
    Loss 1922.6114187713	
    Loss 1873.9422734991	
    Loss 1826.1850256515	
    Loss 1780.0268469967	
    Loss 1735.3002468126	
    Loss 1691.6206812378	
    Loss 1648.9111520336	
    Loss 1607.6377357324	
    Loss 1567.4314853985	
    Loss 1528.3624328116	
    Loss 1490.4207455278	
    Loss 1453.5784955553	
    Loss 1417.8186474202	
    Loss 1383.0684113536	
    Loss 1348.8257368384	
    Loss 1315.7628808005	
    Loss 1284.0009858785	
    Loss 1252.877462899	
    Loss 1222.4966083001	
    Loss 1193.1405441031	
    Loss 1164.4130584625	
    Loss 1136.6069449028	
    Loss 1109.4850060085	
    Loss 1083.200430734	
    Loss 1057.5036702675	
    Loss 1032.3907397146	
    Loss 1008.1348141256	
    Loss 984.62722358047	
    Loss 961.68410087445	
    Loss 939.53893607231	
    Loss 917.99236117601	
Epoch 5	
 90861
     5
     0
    19
     0
     0
     0
     4
 40807
   112
[torch.DoubleTensor of size 10]

Validation accuracy:	0.088902039329934	
Grad norm	14.707728898822	
    Loss 911.4777016541	
    Loss 890.71539708604	
    Loss 870.58327969987	
    Loss 850.78155224086	
    Loss 831.68171842839	
    Loss 813.07824245494	
    Loss 794.91167837117	
    Loss 777.01405508156	
    Loss 760.0207168754	
    Loss 743.63710883471	
    Loss 727.27399443163	
    Loss 711.61690758192	
    Loss 696.25650626174	
    Loss 681.12676396381	
    Loss 666.64563609424	
    Loss 652.61311619259	
    Loss 639.02526240898	
    Loss 625.69020528985	
    Loss 612.67071007842	
    Loss 600.36837880057	
    Loss 588.06955394885	
    Loss 576.25588116259	
    Loss 564.85057160447	
    Loss 553.57121522272	
    Loss 542.65762625463	
    Loss 531.83901281559	
    Loss 521.14266812033	
    Loss 511.32107467601	
    Loss 501.71547602494	
    Loss 492.1240269641	
    Loss 482.84144749642	
    Loss 473.85388737023	
    Loss 465.40436516085	
    Loss 456.90413699503	
    Loss 448.69284849379	
    Loss 440.52421281761	
    Loss 432.61910108055	
    Loss 424.86191081509	
    Loss 417.44472287133	
    Loss 410.35238463639	
    Loss 403.40980930854	
    Loss 396.54592207277	
    Loss 390.15209267483	
    Loss 383.48190560894	
    Loss 377.30589049156	
    Loss 371.44881537426	
    Loss 365.5511658874	
    Loss 359.5912138293	
    Loss 354.05686503077	
    Loss 348.58013431908	
    Loss 343.30552721559	
    Loss 338.21370786629	
    Loss 333.30354563956	
    Loss 328.58474624403	
    Loss 323.99553147334	
    Loss 319.111514818	
    Loss 314.59883933345	
    Loss 310.56272610453	
    Loss 306.42655399302	
    Loss 302.26109892351	
    Loss 298.41363696062	
    Loss 294.47644274427	
    Loss 290.78552148006	
    Loss 287.10482383111	
    Loss 283.60142244869	
    Loss 280.07451052159	
    Loss 276.50333315337	
    Loss 273.20225778873	
    Loss 270.0658766296	
    Loss 266.93428309629	
    Loss 264.04355757152	
    Loss 261.21787135519	
Epoch 6	
 91505
     3
     0
    12
     0
     0
     0
     2
 40178
   108
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087521243020151	
Grad norm	9.0091892489729	
    Loss 260.20428570093	
    Loss 257.50076136339	
    Loss 254.91418635157	
    Loss 252.16189163768	
    Loss 249.67318667557	
    Loss 247.19125781294	
    Loss 244.69202527018	
    Loss 242.02527969182	
    Loss 239.85963605218	
    Loss 237.88936175916	
    Loss 235.55283688638	
    Loss 233.51905023771	
    Loss 231.40381781829	
    Loss 229.15586034525	
    Loss 227.20735210458	
    Loss 225.35604480858	
    Loss 223.5851336302	
    Loss 221.76172136424	
    Loss 219.9276515056	
    Loss 218.51802410424	
    Loss 216.80711822245	
    Loss 215.27480303267	
    Loss 213.87481956562	
    Loss 212.31814016782	
    Loss 210.85899585857	
    Loss 209.23305768026	
    Loss 207.47315538122	
    Loss 206.34487086332	
    Loss 205.18973212675	
    Loss 203.81317496737	
    Loss 202.52129004454	
    Loss 201.30254812534	
    Loss 200.40138200277	
    Loss 199.24975086938	
    Loss 198.17103124502	
    Loss 196.9411194083	
    Loss 195.77545153174	
    Loss 194.58030443221	
    Loss 193.54196973042	
    Loss 192.64793610072	
    Loss 191.73765922273	
    Loss 190.73924809238	
    Loss 190.04690713692	
    Loss 188.9156790592	
    Loss 188.13487025841	
    Loss 187.52183659893	
    Loss 186.72037317996	
    Loss 185.72038659721	
    Loss 185.01049631529	
    Loss 184.21852061852	
    Loss 183.50732016634	
    Loss 182.84980797745	
    Loss 182.24810373755	
    Loss 181.71752188245	
    Loss 181.19213768352	
    Loss 180.27158513722	
    Loss 179.61449355662	
    Loss 179.31528432102	
    Loss 178.82277236674	
    Loss 178.19167814565	
    Loss 177.78692548994	
    Loss 177.19357529372	
    Loss 176.75804966541	
    Loss 176.23996338365	
    Loss 175.80758180226	
    Loss 175.27446538919	
    Loss 174.60859392997	
    Loss 174.13587469417	
    Loss 173.74845958564	
    Loss 173.29090652115	
    Loss 172.99741775469	
    Loss 172.69702406118	
Epoch 7	
 91579
     4
     0
    10
     0
     0
     0
     2
 40104
   109
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087293639232824	
Grad norm	7.0198887392706	
    Loss 172.42300630934	
    Loss 172.15728023661	
    Loss 171.93636289186	
    Loss 171.48089830675	
    Loss 171.23721194421	
    Loss 170.92798014978	
    Loss 170.53893435436	
    Loss 169.92317262944	
    Loss 169.75767497689	
    Loss 169.73110120361	
    Loss 169.28870594169	
    Loss 169.09124471044	
    Loss 168.76157546347	
    Loss 168.25108347465	
    Loss 167.99437501843	
    Loss 167.78635986043	
    Loss 167.60388228318	
    Loss 167.33238312401	
    Loss 167.00501271788	
    Loss 167.06604671825	
    Loss 166.78406142842	
    Loss 166.63650531634	
    Loss 166.58611012898	
    Loss 166.33996150277	
    Loss 166.15552339442	
    Loss 165.76901540955	
    Loss 165.21315089605	
    Loss 165.25701178451	
    Loss 165.24080575249	
    Loss 164.9712315281	
    Loss 164.75697311622	
    Loss 164.5860597909	
    Loss 164.70141000659	
    Loss 164.54154610536	
    Loss 164.4224092473	
    Loss 164.12739052532	
    Loss 163.86792698295	
    Loss 163.55737275838	
    Loss 163.37829779956	
    Loss 163.31855961372	
    Loss 163.22129801393	
    Loss 163.01348418726	
    Loss 163.08944976917	
    Loss 162.70385219265	
    Loss 162.65100871209	
    Loss 162.74532500214	
    Loss 162.63085453058	
    Loss 162.30050322995	
    Loss 162.24225727089	
    Loss 162.08186989737	
    Loss 161.98759258434	
    Loss 161.92900822724	
    Loss 161.90851419479	
    Loss 161.94301269856	
    Loss 161.96380127121	
    Loss 161.57870509941	
    Loss 161.44296579979	
    Loss 161.64652520744	
    Loss 161.64642964159	
    Loss 161.49113655557	
    Loss 161.55108249535	
    Loss 161.40846361255	
    Loss 161.41270873733	
    Loss 161.3211580413	
    Loss 161.30210787417	
    Loss 161.17376904764	
    Loss 160.89926942458	
    Loss 160.80842521459	
    Loss 160.79198734619	
    Loss 160.69547408698	
    Loss 160.75202079769	
    Loss 160.79202216288	
Epoch 8	
 91636
     4
     0
    10
     0
     0
     0
     2
 40047
   109
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087506069434329	
Grad norm	6.3447033664026	
    Loss 160.61683760237	
    Loss 160.68034437471	
    Loss 160.77793660848	
    Loss 160.63098166115	
    Loss 160.6914480748	
    Loss 160.67440433701	
    Loss 160.56869457853	
    Loss 160.22815205055	
    Loss 160.33225320673	
    Loss 160.56752824321	
    Loss 160.38120060249	
    Loss 160.43080514742	
    Loss 160.34145304377	
    Loss 160.06510426586	
    Loss 160.03685880888	
    Loss 160.05038378273	
    Loss 160.08006009963	
    Loss 160.01738811341	
    Loss 159.89239495886	
    Loss 160.15209837948	
    Loss 160.06296828463	
    Loss 160.10131507475	
    Loss 160.23282330141	
    Loss 160.16302662392	
    Loss 160.15021181962	
    Loss 159.93058205275	
    Loss 159.53646437323	
    Loss 159.73810081346	
    Loss 159.87504697225	
    Loss 159.75426073691	
    Loss 159.68514516244	
    Loss 159.65546796135	
    Loss 159.90720192043	
    Loss 159.88119254978	
    Loss 159.8904492653	
    Loss 159.72108233614	
    Loss 159.5826973466	
    Loss 159.3911736657	
    Loss 159.3274390913	
    Loss 159.37949771119	
    Loss 159.39153958579	
    Loss 159.29003965573	
    Loss 159.46928001567	
    Loss 159.18364116907	
    Loss 159.2289405847	
    Loss 159.41858672526	
    Loss 159.39655624284	
    Loss 159.15664412269	
    Loss 159.1865133599	
    Loss 159.11111629973	
    Loss 159.10045778352	
    Loss 159.12286536662	
    Loss 159.18070605684	
    Loss 159.29142559007	
    Loss 159.38509378646	
    Loss 159.07244602801	
    Loss 159.00742031206	
    Loss 159.27829550644	
    Loss 159.34490007143	
    Loss 159.25338710281	
    Loss 159.37605227546	
    Loss 159.29406210902	
    Loss 159.35781563931	
    Loss 159.32371813848	
    Loss 159.36005422859	
    Loss 159.28661069713	
    Loss 159.06463319148	
    Loss 159.02537196098	
    Loss 159.05899417883	
    Loss 159.01125090667	
    Loss 159.11483546758	
    Loss 159.20061371198	
Epoch 9	
 91633
     5
     0
    10
     0
     0
     0
     2
 40051
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087589524156349	
Grad norm	6.1243602417844	
    Loss 159.03842181357	
    Loss 159.14642858977	
    Loss 159.28673104965	
    Loss 159.18086991464	
    Loss 159.2827889926	
    Loss 159.30480716954	
    Loss 159.23681985118	
    Loss 158.93285172641	
    Loss 159.07320447374	
    Loss 159.34365786492	
    Loss 159.19204382958	
    Loss 159.27471948947	
    Loss 159.21756698832	
    Loss 158.97269129049	
    Loss 158.97531517891	
    Loss 159.01864370705	
    Loss 159.07616457244	
    Loss 159.04143846647	
    Loss 158.94342251427	
    Loss 159.22999125635	
    Loss 159.16689032557	
    Loss 159.22997279184	
    Loss 159.38594490949	
    Loss 159.33977707528	
    Loss 159.34998053707	
    Loss 159.15273651526	
    Loss 158.7801650295	
    Loss 159.00293179266	
    Loss 159.16033951391	
    Loss 159.05940848051	
    Loss 159.00976735328	
    Loss 158.99910042326	
    Loss 159.26894292869	
    Loss 159.26099818477	
    Loss 159.28717345408	
    Loss 159.13458474499	
    Loss 159.0120918253	
    Loss 158.8365054772	
    Loss 158.78809627872	
    Loss 158.8549522407	
    Loss 158.88158323246	
    Loss 158.79428344121	
    Loss 158.98731143883	
    Loss 158.71492364405	
    Loss 158.77341591203	
    Loss 158.97587811697	
    Loss 158.96622461995	
    Loss 158.73853326473	
    Loss 158.78034639963	
    Loss 158.71632532052	
    Loss 158.71707354289	
    Loss 158.75046470027	
    Loss 158.81883450448	
    Loss 158.93982883934	
    Loss 159.04302871431	
    Loss 158.74021953796	
    Loss 158.68486037289	
    Loss 158.9646340451	
    Loss 159.04032848917	
    Loss 158.95726496732	
    Loss 159.08838943333	
    Loss 159.01450788436	
    Loss 159.08635021245	
    Loss 159.05996430871	
    Loss 159.10363603536	
    Loss 159.03769789713	
    Loss 158.8226906026	
    Loss 158.79040295462	
    Loss 158.83077883533	
    Loss 158.78963011199	
    Loss 158.89948123589	
    Loss 158.99137497498	
Epoch 10	
 91644
     5
     0
    10
     0
     0
     0
     2
 40040
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087581937363438	
Grad norm	6.0558490517737	
    Loss 158.83081360332	
    Loss 158.94484614998	
    Loss 159.09081279945	
    Loss 158.99029808466	
    Loss 159.09795202551	
    Loss 159.12510948757	
    Loss 159.06202947471	
    Loss 158.76279203103	
    Loss 158.90798618758	
    Loss 159.18313314848	
    Loss 159.03625954896	
    Loss 159.12330133854	
    Loss 159.07041243071	
    Loss 158.82973800662	
    Loss 158.83653743051	
    Loss 158.88385112191	
    Loss 158.94485032787	
    Loss 158.91380579273	
    Loss 158.81931434917	
    Loss 159.1095238115	
    Loss 159.04993696538	
    Loss 159.11623115998	
    Loss 159.27547437498	
    Loss 159.23243874438	
    Loss 159.24569799262	
    Loss 159.05142869553	
    Loss 158.68165968126	
    Loss 158.90721590067	
    Loss 159.06730861477	
    Loss 158.96897492967	
    Loss 158.92191867225	
    Loss 158.91380412794	
    Loss 159.18598137412	
    Loss 159.18047438322	
    Loss 159.20878374137	
    Loss 159.05839770276	
    Loss 158.93788601618	
    Loss 158.76440550632	
    Loss 158.71797331347	
    Loss 158.7867211772	
    Loss 158.81525955817	
    Loss 158.72982055154	
    Loss 158.92465572862	
    Loss 158.65397141559	
    Loss 158.71422092599	
    Loss 158.91839477455	
    Loss 158.91037704485	
    Loss 158.68433824075	
    Loss 158.72777964134	
    Loss 158.66525883085	
    Loss 158.66758817798	
    Loss 158.70247948707	
    Loss 158.77225370353	
    Loss 158.89463169389	
    Loss 158.9990072871	
    Loss 158.6975467609	
    Loss 158.64353580354	
    Loss 158.92444602128	
    Loss 159.00139946927	
    Loss 158.91941892539	
    Loss 159.05168173464	
    Loss 158.97886808275	
    Loss 159.05182350734	
    Loss 159.0264631088	
    Loss 159.07107632818	
    Loss 159.00618669801	
    Loss 158.79207443351	
    Loss 158.7607311732	
    Loss 158.8020179555	
    Loss 158.7617630677	
    Loss 158.87242817334	
    Loss 158.96512381133	
Epoch 11	
 91641
     5
     0
    10
     0
     0
     0
     2
 40043
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.087589524156349	
Grad norm	6.0351385142498	
    Loss 158.80473866581	
    Loss 158.91959096283	
    Loss 159.06628617892	
    Loss 158.96642006427	
    Loss 159.07489708771	
    Loss 159.1027012952	
    Loss 159.04021838868	
    Loss 158.74154534649	
    Loss 158.8873738539	
    Loss 159.16313637961	
    Loss 159.01692278049	
    Loss 159.10451947162	
    Loss 159.052177406	
    Loss 158.81205331757	
    Loss 158.81941892161	
    Loss 158.86725642104	
    Loss 158.92862590617	
    Loss 158.89804454292	
    Loss 158.80398859316	
    Loss 159.09469380418	
    Loss 159.03558228915	
    Loss 159.10226486826	
    Loss 159.26193889441	
    Loss 159.21930737731	
    Loss 159.23296113451	
    Loss 159.03907763365	
    Loss 158.66964805836	
    Loss 158.89555737545	
    Loss 159.05598457762	
    Loss 158.95797160094	
    Loss 158.9112479505	
    Loss 158.90347503363	
    Loss 159.17592762344	
    Loss 159.17074972387	
    Loss 159.19929313284	
    Loss 159.04918327455	
    Loss 158.92887964732	
    Loss 158.75566750833	
    Loss 158.7094679686	
    Loss 158.77843329942	
    Loss 158.80720608763	
    Loss 158.7219975629	
    Loss 158.91705771623	
    Loss 158.64657331907	
    Loss 158.7070503636	
    Loss 158.91144878083	
    Loss 158.90363981766	
    Loss 158.67782463564	
    Loss 158.7214910622	
    Loss 158.65915989167	
    Loss 158.66171707034	
    Loss 158.69681694991	
    Loss 158.76677448185	
    Loss 158.88933859726	
    Loss 158.99383271059	
    Loss 158.69256146272	
    Loss 158.63874806471	
    Loss 158.91978948431	
    Loss 158.99692474249	
    Loss 158.91506965924	
    Loss 159.04748462576	
    Loss 158.97480575782	
    Loss 159.04791945068	
    Loss 159.02269210553	
    Loss 159.06741554787	
    Loss 159.00268061291	
    Loss 158.7886717891	
    Loss 158.75745687226	
    Loss 158.79886654421	
    Loss 158.75873354117	
    Loss 158.86949640832	
    Loss 158.96229160324	
Epoch 12	
 91637
     5
     0
    10
     0
     0
     0
     2
 40047
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08759711094926	
Grad norm	6.0288701071144	
    Loss 158.80191457795	
    Loss 158.91687977162	
    Loss 159.0636605241	
    Loss 158.96385537882	
    Loss 159.07246073581	
    Loss 159.10033573588	
    Loss 159.03791029789	
    Loss 158.73928672029	
    Loss 158.88519385301	
    Loss 159.16103361689	
    Loss 159.01491652709	
    Loss 159.10257599835	
    Loss 159.05029783493	
    Loss 158.81024206611	
    Loss 158.81768489678	
    Loss 158.8655878601	
    Loss 158.92697212822	
    Loss 158.89644075252	
    Loss 158.80242972035	
    Loss 159.09320316345	
    Loss 159.03415649401	
    Loss 159.10087547947	
    Loss 159.26060396917	
    Loss 159.2180209198	
    Loss 159.23172158111	
    Loss 159.0378848877	
    Loss 158.66848694938	
    Loss 158.89443524425	
    Loss 159.0548974774	
    Loss 158.95691701668	
    Loss 158.91023222279	
    Loss 158.90250507663	
    Loss 159.17498040613	
    Loss 159.1698470561	
    Loss 159.19840263093	
    Loss 159.04832284814	
    Loss 158.92802583537	
    Loss 158.7548443759	
    Loss 158.70866355244	
    Loss 158.77764462087	
    Loss 158.80644050446	
    Loss 158.72125547813	
    Loss 158.91633934656	
    Loss 158.64587134505	
    Loss 158.70637525584	
    Loss 158.91080169204	
    Loss 158.90301677224	
    Loss 158.67723182172	
    Loss 158.72093035506	
    Loss 158.65862010615	
    Loss 158.66121307423	
    Loss 158.69634322054	
    Loss 158.76632318989	
    Loss 158.88891236367	
    Loss 158.99340791881	
    Loss 158.69216477227	
    Loss 158.63838364806	
    Loss 158.91943513073	
    Loss 158.99659932451	
    Loss 158.91475371291	
    Loss 159.0471886337	
    Loss 158.97452468148	
    Loss 159.04766275495	
    Loss 159.02245151912	
    Loss 159.06718399434	
    Loss 159.0024748379	
    Loss 158.78847344664	
    Loss 158.75727609064	
    Loss 158.79870238995	
    Loss 158.75858626356	
    Loss 158.86935777394	
    Loss 158.96216322225	
Epoch 13	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0270030923832	
    Loss 158.80178168036	
    Loss 158.91676293266	
    Loss 159.0635506804	
    Loss 158.9637441053	
    Loss 159.07237285635	
    Loss 159.10025161467	
    Loss 159.03782572585	
    Loss 158.73919917829	
    Loss 158.88511438003	
    Loss 159.16096267389	
    Loss 159.0148613473	
    Loss 159.10252502346	
    Loss 159.05025205982	
    Loss 158.81020338824	
    Loss 158.81765691201	
    Loss 158.86556677169	
    Loss 158.92694052937	
    Loss 158.89641126379	
    Loss 158.80240151068	
    Loss 159.09318456689	
    Loss 159.03414702287	
    Loss 159.10086523567	
    Loss 159.26059980962	
    Loss 159.21802132518	
    Loss 159.23172605721	
    Loss 159.03789391695	
    Loss 158.66849508742	
    Loss 158.89444543906	
    Loss 159.05490873873	
    Loss 158.95692878536	
    Loss 158.91024700362	
    Loss 158.90252615249	
    Loss 159.17499932773	
    Loss 159.16987207912	
    Loss 159.19842232496	
    Loss 159.04834414308	
    Loss 158.92804026496	
    Loss 158.75486100904	
    Loss 158.70867802695	
    Loss 158.77765632702	
    Loss 158.80645217146	
    Loss 158.72126752224	
    Loss 158.91635225917	
    Loss 158.64588273028	
    Loss 158.70638876399	
    Loss 158.91081816603	
    Loss 158.90303507034	
    Loss 158.67725416795	
    Loss 158.72095761296	
    Loss 158.65864849162	
    Loss 158.66124805763	
    Loss 158.69638302654	
    Loss 158.76636517649	
    Loss 158.88895776015	
    Loss 158.9934482348	
    Loss 158.69220978274	
    Loss 158.63843503605	
    Loss 158.91948523974	
    Loss 158.99665496397	
    Loss 158.91480794054	
    Loss 159.04724533746	
    Loss 158.97458225423	
    Loss 159.04772474916	
    Loss 159.02251505452	
    Loss 159.06724671832	
    Loss 159.00254286206	
    Loss 158.78854003336	
    Loss 158.75734510738	
    Loss 158.79877368443	
    Loss 158.75865998486	
    Loss 158.86943096207	
    Loss 158.96223662861	
Epoch 14	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0265332784643	
    Loss 158.80185246101	
    Loss 158.91683618692	
    Loss 159.06362326247	
    Loss 158.96381298502	
    Loss 159.07244703491	
    Loss 159.10032426373	
    Loss 159.03789539544	
    Loss 158.73926495717	
    Loss 158.88518032671	
    Loss 159.16102915933	
    Loss 159.01493096777	
    Loss 159.1025937054	
    Loss 159.05032025882	
    Loss 158.81027178721	
    Loss 158.81772684931	
    Loss 158.86563692211	
    Loss 158.92700480946	
    Loss 158.89647411583	
    Loss 158.80246288149	
    Loss 159.09324735837	
    Loss 159.03421122635	
    Loss 159.10092731374	
    Loss 159.26066227269	
    Loss 159.21808376161	
    Loss 159.23178823196	
    Loss 159.03795611342	
    Loss 158.66855528823	
    Loss 158.89450473828	
    Loss 159.05496685642	
    Loss 158.95698557266	
    Loss 158.9103033727	
    Loss 158.90258349517	
    Loss 159.17505448872	
    Loss 159.1699281097	
    Loss 159.19847513805	
    Loss 159.04839633135	
    Loss 158.92808872359	
    Loss 158.75490908718	
    Loss 158.70872408939	
    Loss 158.7777002566	
    Loss 158.80649493234	
    Loss 158.72130930889	
    Loss 158.91639334484	
    Loss 158.64592228588	
    Loss 158.70642802559	
    Loss 158.91085753598	
    Loss 158.90307419759	
    Loss 158.67729381337	
    Loss 158.72099811525	
    Loss 158.65868853095	
    Loss 158.66128960945	
    Loss 158.69642548687	
    Loss 158.76640761976	
    Loss 158.88900068883	
    Loss 158.99348856485	
    Loss 158.69225106437	
    Loss 158.63847790751	
    Loss 158.91952702711	
    Loss 158.9966980973	
    Loss 158.91484991241	
    Loss 159.04728756488	
    Loss 158.97462420006	
    Loss 159.04776771371	
    Loss 159.02255801128	
    Loss 159.0672888581	
    Loss 159.00258639321	
    Loss 158.78858246184	
    Loss 158.75738787632	
    Loss 158.79881677834	
    Loss 158.75870345913	
    Loss 158.86947373275	
    Loss 158.96227900233	
Epoch 15	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0264582921835	
    Loss 158.80189375454	
    Loss 158.91687793	
    Loss 159.06366434055	
    Loss 158.96385228835	
    Loss 159.07248780912	
    Loss 159.10036411537	
    Loss 159.03793380503	
    Loss 158.73930157681	
    Loss 158.88521663595	
    Loss 159.16106534357	
    Loss 159.01496794525	
    Loss 159.10263002383	
    Loss 159.05035610191	
    Loss 158.81030738624	
    Loss 158.81776269082	
    Loss 158.86567252678	
    Loss 158.9270380342	
    Loss 158.89650651587	
    Loss 158.80249450858	
    Loss 159.09327921434	
    Loss 159.03424335429	
    Loss 159.10095843553	
    Loss 159.26069329578	
    Loss 159.21811457217	
    Loss 159.23181871414	
    Loss 159.03798639867	
    Loss 158.66858462349	
    Loss 158.89453351872	
    Loss 159.05499499736	
    Loss 158.95701302959	
    Loss 158.91033046857	
    Loss 158.9026107949	
    Loss 159.17508080944	
    Loss 159.16995456911	
    Loss 159.19850026857	
    Loss 159.04842108928	
    Loss 158.92811196532	
    Loss 158.75493203997	
    Loss 158.70874613706	
    Loss 158.77772138331	
    Loss 158.8065154732	
    Loss 158.7213293423	
    Loss 158.91641299638	
    Loss 158.6459412613	
    Loss 158.70644673908	
    Loss 158.91087615909	
    Loss 158.90309261942	
    Loss 158.67731228932	
    Loss 158.72101677216	
    Loss 158.65870690228	
    Loss 158.66130840446	
    Loss 158.6964444946	
    Loss 158.76642651088	
    Loss 158.88901965903	
    Loss 158.99350649014	
    Loss 158.69226922999	
    Loss 158.63849655073	
    Loss 158.91954520831	
    Loss 158.99671668095	
    Loss 158.91486798851	
    Loss 159.04730564751	
    Loss 158.97464210578	
    Loss 159.04778591382	
    Loss 159.02257613705	
    Loss 159.06730662638	
    Loss 159.00260459992	
    Loss 158.78860018243	
    Loss 158.75740564415	
    Loss 158.79883459785	
    Loss 158.75872134889	
    Loss 158.86949129368	
    Loss 158.96229635372	
Epoch 16	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0264510252505	
    Loss 158.80191069835	
    Loss 158.91689497853	
    Loss 159.0636810952	
    Loss 158.96386834015	
    Loss 159.07250432635	
    Loss 159.10038025606	
    Loss 159.03794937984	
    Loss 158.73931645092	
    Loss 158.88523134793	
    Loss 159.1610799749	
    Loss 159.01498281935	
    Loss 159.10264462046	
    Loss 159.05037049223	
    Loss 158.81032164648	
    Loss 158.81777699688	
    Loss 158.86568670256	
    Loss 158.92705132154	
    Loss 158.896519459	
    Loss 158.80250714841	
    Loss 159.093291896	
    Loss 159.03425610778	
    Loss 159.10097079685	
    Loss 159.26070559156	
    Loss 159.21812677137	
    Loss 159.23183076256	
    Loss 159.03799835284	
    Loss 158.66859620258	
    Loss 158.89454486409	
    Loss 159.05500608242	
    Loss 158.95702384031	
    Loss 158.91034111809	
    Loss 158.90262150749	
    Loss 159.17509114204	
    Loss 159.16996492961	
    Loss 159.19851012947	
    Loss 159.04843080089	
    Loss 158.92812110623	
    Loss 158.75494105846	
    Loss 158.70875480145	
    Loss 158.77772969538	
    Loss 158.80652354888	
    Loss 158.72133721162	
    Loss 158.91642071318	
    Loss 158.64594871992	
    Loss 158.70645407782	
    Loss 158.91088344768	
    Loss 158.90309982262	
    Loss 158.67731949214	
    Loss 158.72102402149	
    Loss 158.65871403213	
    Loss 158.6613156692	
    Loss 158.69645181963	
    Loss 158.76643377822	
    Loss 158.88902694323	
    Loss 158.99351338122	
    Loss 158.69227619277	
    Loss 158.63850367287	
    Loss 158.91955215851	
    Loss 158.99672376667	
    Loss 158.91487487885	
    Loss 159.04731252891	
    Loss 158.97464891502	
    Loss 159.04779282096	
    Loss 159.02258300933	
    Loss 159.06731336376	
    Loss 159.00261148931	
    Loss 158.78860688128	
    Loss 158.75741234867	
    Loss 158.79884131257	
    Loss 158.75872807989	
    Loss 158.86949789395	
    Loss 158.96230286869	
Epoch 17	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.026449659784	
    Loss 158.80191706375	
    Loss 158.91690137486	
    Loss 159.06368737929	
    Loss 158.96387436099	
    Loss 159.07251050434	
    Loss 159.10038629445	
    Loss 159.03795520912	
    Loss 158.73932201875	
    Loss 158.8852368505	
    Loss 159.16108544601	
    Loss 159.01498837289	
    Loss 159.10265006969	
    Loss 159.05037586429	
    Loss 158.81032696612	
    Loss 158.81778232771	
    Loss 158.86569197938	
    Loss 158.92705627546	
    Loss 158.8965242807	
    Loss 158.80251185995	
    Loss 159.09329661628	
    Loss 159.03426085245	
    Loss 159.10097539708	
    Loss 159.26071016477	
    Loss 159.21813130951	
    Loss 159.23183524148	
    Loss 159.03800279605	
    Loss 158.66860050565	
    Loss 158.89454907742	
    Loss 159.05501019752	
    Loss 158.95702785266	
    Loss 158.91034506716	
    Loss 158.90262548113	
    Loss 159.1750949746	
    Loss 159.16996876976	
    Loss 159.19851378754	
    Loss 159.04843440476	
    Loss 158.92812450071	
    Loss 158.75494440709	
    Loss 158.70875801745	
    Loss 158.77773278164	
    Loss 158.80652654549	
    Loss 158.72134012981	
    Loss 158.91642357527	
    Loss 158.64595148768	
    Loss 158.70645679741	
    Loss 158.91088614673	
    Loss 158.90310248999	
    Loss 158.67732215601	
    Loss 158.72102669909	
    Loss 158.65871666424	
    Loss 158.66131834709	
    Loss 158.69645451673	
    Loss 158.76643645214	
    Loss 158.88902962206	
    Loss 158.99351591537	
    Loss 158.69227875033	
    Loss 158.63850628619	
    Loss 158.91955471052	
    Loss 158.99672636678	
    Loss 158.91487740668	
    Loss 159.04731505213	
    Loss 158.97465141171	
    Loss 159.04779535237	
    Loss 159.02258552766	
    Loss 159.06731583342	
    Loss 159.00261401383	
    Loss 158.78860933391	
    Loss 158.7574148014	
    Loss 158.79884376793	
    Loss 158.75873053993	
    Loss 158.86950030455	
    Loss 158.96230524691	
Epoch 18	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0264491706897	
    Loss 158.80191938757	
    Loss 158.91690370926	
    Loss 159.06368967273	
    Loss 158.96387655769	
    Loss 159.07251275563	
    Loss 159.1003884956	
    Loss 159.0379573345	
    Loss 158.73932404812	
    Loss 158.88523885542	
    Loss 159.16108744006	
    Loss 159.01499039624	
    Loss 159.10265205519	
    Loss 159.05037782225	
    Loss 158.81032890447	
    Loss 158.81778426949	
    Loss 158.86569390038	
    Loss 158.92705808012	
    Loss 158.89652603596	
    Loss 158.8025135762	
    Loss 159.09329833455	
    Loss 159.03426257999	
    Loss 159.10097707244	
    Loss 159.26071183013	
    Loss 159.21813296298	
    Loss 159.23183687273	
    Loss 159.03800441462	
    Loss 158.66860207288	
    Loss 158.89455061124	
    Loss 159.05501169518	
    Loss 158.9570293127	
    Loss 158.91034650334	
    Loss 158.90262692751	
    Loss 159.17509636942	
    Loss 159.16997016712	
    Loss 159.1985151193	
    Loss 159.04843571744	
    Loss 158.92812573728	
    Loss 158.75494562715	
    Loss 158.70875918862	
    Loss 158.77773390564	
    Loss 158.80652763622	
    Loss 158.72134119141	
    Loss 158.91642461678	
    Loss 158.64595249525	
    Loss 158.70645778642	
    Loss 158.91088712795	
    Loss 158.90310345993	
    Loss 158.67732312397	
    Loss 158.72102767146	
    Loss 158.6587176198	
    Loss 158.66131931864	
    Loss 158.69645549477	
    Loss 158.76643742142	
    Loss 158.88903059306	
    Loss 158.99351683353	
    Loss 158.69227967644	
    Loss 158.63850723217	
    Loss 158.91955563495	
    Loss 158.99672730864	
    Loss 158.91487832215	
    Loss 159.04731596581	
    Loss 158.97465231599	
    Loss 159.04779626927	
    Loss 159.02258643997	
    Loss 159.06731672843	
    Loss 159.00261492894	
    Loss 158.78861022226	
    Loss 158.75741568938	
    Loss 158.79884465675	
    Loss 158.75873143028	
    Loss 158.86950117654	
    Loss 158.96230610689	
Epoch 19	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0264489906131	
    Loss 158.80192022783	
    Loss 158.91690455339	
    Loss 159.06369050215	
    Loss 158.9638773518	
    Loss 159.07251356893	
    Loss 159.10038929106	
    Loss 159.03795810271	
    Loss 158.73932478126	
    Loss 158.88523957962	
    Loss 159.16108816065	
    Loss 159.01499112741	
    Loss 159.10265277281	
    Loss 159.05037853019	
    Loss 158.81032960524	
    Loss 158.81778497148	
    Loss 158.86569459457	
    Loss 158.92705873249	
    Loss 158.89652667004	
    Loss 158.8025141966	
    Loss 159.09329895542	
    Loss 159.03426320452	
    Loss 159.10097767821	
    Loss 159.26071243233	
    Loss 159.21813356126	
    Loss 159.23183746279	
    Loss 159.03800500027	
    Loss 158.66860263985	
    Loss 158.89455116591	
    Loss 159.05501223665	
    Loss 158.95702984051	
    Loss 158.91034702227	
    Loss 158.90262745068	
    Loss 159.17509687384	
    Loss 159.16997067248	
    Loss 159.19851560111	
    Loss 159.04843619263	
    Loss 158.92812618487	
    Loss 158.75494606886	
    Loss 158.70875961243	
    Loss 158.77773431239	
    Loss 158.80652803071	
    Loss 158.72134157517	
    Loss 158.91642499342	
    Loss 158.64595285971	
    Loss 158.70645814386	
    Loss 158.91088748249	
    Loss 158.90310381051	
    Loss 158.67732347367	
    Loss 158.72102802261	
    Loss 158.65871796481	
    Loss 158.66131966931	
    Loss 158.69645584768	
    Loss 158.76643777109	
    Loss 158.88903094336	
    Loss 158.99351716457	
    Loss 158.69228001023	
    Loss 158.63850757311	
    Loss 158.91955596834	
    Loss 158.9967276484	
    Loss 158.91487865232	
    Loss 159.04731629533	
    Loss 158.97465264222	
    Loss 159.04779660014	
    Loss 159.02258676928	
    Loss 159.06731705161	
    Loss 159.00261525951	
    Loss 158.78861054294	
    Loss 158.75741600982	
    Loss 158.79884497746	
    Loss 158.75873175154	
    Loss 158.86950149102	
    Loss 158.96230641698	
Epoch 20	
 91635
     5
     0
    10
     0
     0
     0
     2
 40049
   107
[torch.DoubleTensor of size 10]

Validation accuracy:	0.08760469774217	
Grad norm	6.0264489243171	
    Loss 158.80192053077	
    Loss 158.91690485778	
    Loss 159.06369080128	
    Loss 158.96387763806	
    Loss 159.07251386197	
    Loss 159.10038957781	
    Loss 159.03795837964	
    Loss 158.73932504539	
    Loss 158.88523984052	
    Loss 159.16108842038	
    Loss 159.014991391	
    Loss 159.10265303154	
    Loss 159.05037878556	
    Loss 158.81032985801	
    Loss 158.81778522469	
    Loss 158.86569484489	
    Loss 158.92705896776	
    Loss 158.89652689859	
    Loss 158.80251442035	
    Loss 159.09329917928	
    Loss 159.03426342982	
    Loss 159.10097789678	
    Loss 159.26071264964	
    Loss 159.21813377729	
    Loss 159.23183767579	
    Loss 159.03800521174	
    Loss 158.66860284453	
    Loss 158.89455136609	
    Loss 159.05501243202	
    Loss 158.95703003093	
