[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	50	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1208
 2396
 3893
 1914
 5169
 2307
 2919
  998
 1620
 3333
 2361
 5787
  963
 2087
 1872
 1238
  980
 2820
 5713
 3537
 4658
 2627
  549
 2171
 3589
 3366
 2275
 5381
 4286
 4277
 1213
 1692
 4393
 3340
 2911
 3571
 2809
 2730
 5213
 3659
 3020
 4868
 2579
 1251
 2265
[torch.DoubleTensor of size 45]

Validation accuracy:	0.014748725418791	
Grad norm	0	
    Loss 11381423.218411	
    Loss 12583.873654308	
    Loss 2478.218977293	
    Loss 4234.4698537827	
    Loss 2561.9317390106	
    Loss 3690.7735486723	
    Loss 4905.4729087127	
    Loss 2852.8303731946	
    Loss 2723.673280374	
    Loss 3666.3163001687	
    Loss 5653.5898497926	
    Loss 3800.6150170501	
    Loss 3291.3902760475	
    Loss 4548.0228458213	
    Loss 4617.6578552228	
    Loss 4580.8445085704	
    Loss 4326.215350331	
    Loss 3595.9256726828	
    Loss 4018.6852611015	
    Loss 3091.1081883852	
    Loss 4304.0364125292	
    Loss 5405.0689098161	
    Loss 3025.3093376129	
    Loss 2965.5843958562	
    Loss 3225.0672597359	
    Loss 5817.0933829885	
    Loss 2688.9285161929	
    Loss 3052.0343565091	
    Loss 4443.7917972239	
    Loss 2974.5714286894	
    Loss 2844.80833752	
    Loss 6148.4981480646	
    Loss 4273.0538625956	
    Loss 3700.0702273725	
    Loss 3118.2891367455	
    Loss 4086.9612388724	
    Loss 5943.5096480757	
    Loss 3037.1918459775	
    Loss 2584.9554028596	
    Loss 5814.3870369168	
    Loss 2484.9776117156	
    Loss 3553.9884346595	
    Loss 3844.7157541935	
    Loss 4912.5715705007	
    Loss 4863.6398445197	
    Loss 3815.5777226038	
    Loss 3399.5701620108	
    Loss 3312.0944256435	
    Loss 2803.7612219574	
    Loss 3523.6657109956	
    Loss 3200.679937552	
    Loss 5789.2389270984	
    Loss 4162.4044268528	
    Loss 5237.0381030123	
    Loss 4827.1873639934	
    Loss 3892.4521706852	
    Loss 5027.4454517602	
    Loss 3814.5208958963	
    Loss 5647.0565414001	
    Loss 3449.3186980389	
    Loss 3399.3322637176	
    Loss 4298.3515463093	
    Loss 5317.0646863773	
    Loss 4004.3912375644	
    Loss 2934.4178514925	
    Loss 3229.564735611	
    Loss 2490.7646459344	
    Loss 5563.6377946896	
    Loss 3490.5333727789	
    Loss 2919.1195123647	
    Loss 3587.3781663605	
    Loss 4035.4108479482	
Epoch 2	
  4203
     0
     0
 48387
     0
     0
     0
     0
 79218
[torch.DoubleTensor of size 9]

Validation accuracy:	0.1216618111192	
Grad norm	10.192913394713	
    Loss 3739.8101543231	
    Loss 5308.2988803641	
    Loss 4551.326781247	
    Loss 2768.1278734487	
    Loss 5028.3082430713	
    Loss 2650.1848093641	
    Loss 3046.037721334	
    Loss 2902.4480486186	
    Loss 2953.0041601021	
    Loss 4405.8899429788	
    Loss 4991.2295132562	
    Loss 2634.3027076916	
    Loss 3808.672534215	
    Loss 5538.2017421334	
    Loss 4420.935046526	
    Loss 2885.4608590632	
    Loss 4298.1965010421	
    Loss 3750.3905401663	
    Loss 3734.8200468306	
    Loss 3286.026635982	
    Loss 4837.6006100473	
    Loss 3425.6888694537	
    Loss 3919.9791465761	
    Loss 4170.6469633083	
    Loss 3613.4231701656	
    Loss 3930.7371827711	
    Loss 4726.5980161864	
    Loss 4481.4346782143	
    Loss 4902.0837039775	
    Loss 3623.6976816763	
    Loss 4424.8576816523	
    Loss 4487.9790831978	
    Loss 2992.5112848133	
    Loss 3441.7900486336	
    Loss 4997.6347449476	
    Loss 2860.0376513664	
    Loss 2430.5344566773	
    Loss 3081.2452349699	
    Loss 3533.4849944379	
    Loss 2973.7234573454	
    Loss 4856.2537968086	
    Loss 5693.5814613133	
    Loss 4331.0661185877	
    Loss 3507.3598280065	
    Loss 3140.3402734078	
    Loss 4188.6995840339	
    Loss 5950.6573907707	
    Loss 4769.3890968568	
    Loss 4005.4444205575	
    Loss 3891.2440289802	
    Loss 5658.3617171247	
    Loss 2871.322008241	
    Loss 3851.0752038472	
    Loss 3700.4062424807	
    Loss 3827.3493375334	
    Loss 5033.5922975948	
    Loss 3173.2017433273	
    Loss 3271.2706960701	
    Loss 6221.5251957385	
    Loss 3139.2562847696	
    Loss 3356.7942966081	
    Loss 6089.0443712776	
    Loss 4514.4947837219	
    Loss 3227.1458941751	
    Loss 3177.4039691116	
    Loss 7988.9592328252	
    Loss 3093.609781804	
    Loss 2403.4943420691	
    Loss 4039.7501946958	
    Loss 3427.0233752324	
    Loss 3249.4092554455	
    Loss 6918.5152899059	
Epoch 3	
 128482
      0
      0
      0
      0
      0
      0
      0
   3326
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10013049283807	
Grad norm	10.75374668249	
    Loss 3068.1233061979	
    Loss 2874.9447660141	
    Loss 3097.8316244719	
    Loss 4234.225675709	
    Loss 2580.756560345	
    Loss 3785.2784554265	
    Loss 3154.639529838	
    Loss 3405.1050327503	
    Loss 3309.6484282643	
    Loss 3006.8193190071	
    Loss 3732.1388560152	
    Loss 3994.6465904222	
    Loss 3624.77672529	
    Loss 3161.4153581501	
    Loss 3659.1073199208	
    Loss 4713.9560595103	
    Loss 2989.5245834823	
    Loss 3147.5153150713	
    Loss 4503.9897910043	
    Loss 3397.1331911836	
    Loss 5088.708591387	
    Loss 3773.2282528292	
    Loss 4122.8045238814	
    Loss 2925.5737685945	
    Loss 4748.555339301	
    Loss 4518.4253790185	
    Loss 5932.4018469989	
    Loss 3289.0243266535	
    Loss 4085.9986222444	
    Loss 5111.214173941	
    Loss 3984.3474605601	
    Loss 4233.1971322054	
    Loss 3629.3010247127	
    Loss 3541.5930104118	
    Loss 6613.3117054828	
    Loss 3372.3181586544	
    Loss 2794.5673390365	
    Loss 3455.1731324625	
    Loss 3015.0301166885	
    Loss 4561.3501392275	
    Loss 2402.4731893039	
    Loss 4224.7490427134	
    Loss 4555.7569308044	
    Loss 4490.3839211624	
    Loss 3592.9909490321	
    Loss 4307.3300219444	
    Loss 3667.5784923978	
    Loss 4262.7464913853	
    Loss 3144.0681212033	
    Loss 2903.7338974829	
    Loss 2761.7031455468	
    Loss 2777.5637216961	
    Loss 5781.3693527211	
    Loss 4692.5474388504	
    Loss 2577.4980834059	
    Loss 2342.6852034727	
    Loss 5086.7402275165	
    Loss 3911.0671904172	
    Loss 5023.6763033368	
    Loss 4213.5200257682	
    Loss 4807.655973935	
    Loss 5191.4816117598	
    Loss 6051.4628665627	
    Loss 3031.3770379515	
    Loss 3215.6639006459	
    Loss 7310.4985823423	
    Loss 4744.7022294106	
    Loss 3357.5551476033	
    Loss 3313.1491449336	
    Loss 3453.484037339	
    Loss 4992.2550377505	
    Loss 4043.8485804649	
Epoch 4	
      0
      1
   3754
      0
 116457
      0
      0
      0
      0
      0
      0
      0
      0
      0
      0
      0
    623
  10961
     12
[torch.DoubleTensor of size 19]

Validation accuracy:	0.05879005826657	
Grad norm	10.17185061363	
    Loss 3924.3342770544	
    Loss 3519.5121661142	
    Loss 3459.3691549875	
    Loss 3541.722305261	
    Loss 3629.5447664902	
    Loss 3624.8139007752	
    Loss 5278.4931456918	
    Loss 6436.3233492068	
    Loss 2792.4066746643	
    Loss 3740.0255121817	
    Loss 3151.2419037278	
    Loss 5784.4481431619	
    Loss 4106.9415625856	
    Loss 3353.741143946	
    Loss 3796.1491301558	
    Loss 3204.2231245123	
    Loss 4622.0609649323	
    Loss 3918.500040862	
    Loss 5538.9154707243	
    Loss 3835.6431886692	
    Loss 3855.8055499231	
    Loss 2925.2468778913	
    Loss 3515.3859531808	
    Loss 3517.3777357874	
    Loss 4705.1925530139	
    Loss 5109.0973979469	
    Loss 4963.1018523614	
    Loss 4028.268325656	
    Loss 3575.2599179635	
    Loss 4282.6444516685	
    Loss 3386.7875078588	
    Loss 3165.1562475993	
    Loss 6652.666482262	
    Loss 4165.9997793731	
    Loss 4900.8091477439	
    Loss 2823.7779513101	
    Loss 3557.9666121828	
    Loss 3090.6140771292	
    Loss 3481.938365807	
    Loss 5186.7864661696	
    Loss 3499.2691676789	
    Loss 4137.8876867165	
    Loss 4224.0291123211	
    Loss 4355.2363054609	
    Loss 2947.842873106	
    Loss 4228.5627811969	
    Loss 3461.0343098438	
    Loss 4765.3369249418	
    Loss 2824.610287741	
    Loss 5018.2951537546	
    Loss 5082.1650782831	
    Loss 3518.1425881326	
    Loss 4181.8271808385	
    Loss 4220.1225734038	
    Loss 4050.3339761066	
    Loss 4131.079778114	
    Loss 4319.6274215163	
    Loss 5256.9839828737	
    Loss 6363.8425413911	
    Loss 3413.0019126106	
    Loss 4486.9107905933	
    Loss 3055.9068107195	
    Loss 4150.525814489	
    Loss 2671.1674083308	
    Loss 2871.2745621021	
    Loss 4073.6904247239	
    Loss 3306.9575566541	
    Loss 2565.724266949	
    Loss 4232.7185135494	
    Loss 4251.5930781762	
    Loss 5506.99294052	
    Loss 4187.9986915081	
Epoch 5	
      0
   2401
      0
      0
      0
      0
      0
      0
      1
 129406
[torch.DoubleTensor of size 10]

Validation accuracy:	0.099804260742899	
Grad norm	10.23347845712	
    Loss 3493.126614877	
    Loss 5396.5764000986	
    Loss 3709.4163911687	
    Loss 4376.9732451811	
    Loss 5430.5687127179	
    Loss 4618.8696962648	
    Loss 3914.0946550653	
    Loss 4252.6337648696	
    Loss 3189.4105521549	
    Loss 3228.8897393705	
    Loss 4095.4133459572	
    Loss 6030.4769825246	
    Loss 2751.6003976958	
    Loss 3931.997184744	
    Loss 4166.1939459387	
    Loss 4731.9412304894	
    Loss 3196.7514533117	
    Loss 2859.8052642015	
    Loss 4040.0985247538	
    Loss 2850.9402507736	
    Loss 5382.3598065418	
    Loss 2554.3366123486	
    Loss 3613.3823866002	
    Loss 2859.0583286841	
    Loss 3080.8876084241	
    Loss 4645.6333937019	
    Loss 5461.4167199711	
    Loss 2907.9272397659	
    Loss 3602.1192335731	
    Loss 2966.2353991417	
    Loss 4776.3857688422	
    Loss 3795.2801079738	
    Loss 3945.2881123968	
    Loss 5563.1446509863	
    Loss 4307.3788172788	
    Loss 5087.2184033159	
    Loss 5075.7694577741	
    Loss 4885.3653456721	
    Loss 4539.6654572011	
    Loss 4011.1118808071	
    Loss 3522.8082638075	
    Loss 5182.4067234253	
    Loss 3702.3777069047	
    Loss 3406.6294997251	
    Loss 4267.5070689552	
    Loss 4029.0989555762	
    Loss 2559.8431181507	
    Loss 4242.0913291822	
    Loss 3899.0935180852	
    Loss 3361.0021508889	
    Loss 2503.6668825996	
    Loss 2734.333981868	
    Loss 3747.59948318	
    Loss 4242.3149354342	
    Loss 3647.1628109069	
    Loss 2946.1867847503	
    Loss 5252.2531859368	
    Loss 4156.9459879973	
    Loss 5610.8088148612	
    Loss 8165.6035948725	
    Loss 3298.8886786739	
    Loss 2867.8033040721	
    Loss 5189.376990647	
    Loss 2560.4112491366	
    Loss 3170.6465201311	
    Loss 4213.4833644985	
    Loss 6167.0933132053	
    Loss 4077.606542181	
    Loss 4492.0395065263	
    Loss 5661.3074006257	
    Loss 2887.2729064892	
    Loss 3124.0481196468	
Epoch 6	
 125333
      0
      0
      0
   2981
      0
      0
     20
      0
   3474
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10197408351542	
Grad norm	10.206643199468	
    Loss 3855.541880554	
    Loss 3228.6816271325	
    Loss 4180.3183737085	
    Loss 3085.5198262346	
    Loss 2864.8069443753	
    Loss 2926.4821410755	
    Loss 4890.6244112586	
    Loss 2510.0477832461	
    Loss 3083.6572920182	
    Loss 4517.773237111	
    Loss 3141.8177319099	
    Loss 3202.3511560456	
    Loss 6202.8822173406	
    Loss 2903.1094862303	
    Loss 3555.3399235746	
    Loss 4699.9545003847	
    Loss 2771.9717315909	
    Loss 6187.6308972677	
    Loss 5044.729286711	
    Loss 2925.9146203178	
    Loss 3089.6940887943	
    Loss 3403.0795558632	
    Loss 2481.2586817269	
    Loss 4691.9909886295	
    Loss 4628.2764207636	
    Loss 4538.8331892036	
    Loss 6148.8884064671	
    Loss 3792.4578774434	
    Loss 2751.5592035401	
    Loss 2605.6437468675	
    Loss 5379.7307238015	
    Loss 3822.5443236062	
    Loss 3449.6621816888	
    Loss 5667.3093897425	
    Loss 3647.1126197149	
    Loss 3723.8544702295	
    Loss 3230.0154151522	
    Loss 4206.9164426155	
    Loss 5637.6934976516	
    Loss 4531.7882513108	
    Loss 3720.200813442	
    Loss 5677.9469065095	
    Loss 4359.9231671046	
    Loss 3030.4172647704	
    Loss 4583.0057507281	
    Loss 5495.6061368843	
    Loss 4245.6931938525	
    Loss 3639.4418199724	
    Loss 4399.8364518121	
    Loss 2681.8414133709	
    Loss 3138.7228726059	
    Loss 3087.8775038728	
    Loss 4162.847367091	
    Loss 3135.837492556	
    Loss 3162.3870007902	
    Loss 4042.805438119	
    Loss 5134.9093413682	
    Loss 3373.8126165585	
    Loss 7068.6725776621	
    Loss 2687.5731880956	
    Loss 4369.8763470134	
    Loss 2705.236243309	
    Loss 2657.3381754438	
    Loss 2790.7330939016	
    Loss 3517.8755452328	
    Loss 6287.8409553537	
    Loss 2364.6518736579	
    Loss 2994.2573593718	
    Loss 2653.6413894262	
    Loss 2920.9689998143	
    Loss 3242.8493868647	
    Loss 4155.0877800977	
Epoch 7	
      0
      0
      0
      0
 112271
      0
      0
  19537
[torch.DoubleTensor of size 8]

Validation accuracy:	0.065929230395727	
Grad norm	9.737884665378	
    Loss 3135.5608385371	
    Loss 2704.7659047339	
    Loss 3468.6733027045	
    Loss 4558.8890965534	
    Loss 4302.5360230238	
    Loss 2870.9786731859	
    Loss 4905.9284786387	
    Loss 3116.881203078	
    Loss 3650.8492330122	
    Loss 3080.55395953	
    Loss 6294.6172740737	
    Loss 3785.3163460142	
    Loss 3645.2133238841	
    Loss 5620.8815765137	
    Loss 3743.6792417542	
    Loss 3701.844214056	
    Loss 3428.7109515702	
    Loss 3483.0743638218	
    Loss 3633.6655996401	
    Loss 3552.250680177	
    Loss 4019.9632341168	
    Loss 5188.7875181422	
    Loss 4257.6037054371	
    Loss 5093.6134896719	
    Loss 5009.4015530347	
    Loss 6738.9668146479	
    Loss 4210.3593064634	
    Loss 2874.2671187669	
    Loss 3342.2990472482	
    Loss 3137.227973437	
    Loss 3220.8932058839	
    Loss 5619.8369077612	
    Loss 3080.3219356104	
    Loss 3864.2415452551	
    Loss 2924.9686123324	
    Loss 5389.1006833776	
    Loss 4005.7382282421	
    Loss 2481.0324167108	
    Loss 4479.9743042761	
    Loss 4300.3943652443	
    Loss 5263.0340745433	
    Loss 3962.3897049953	
    Loss 3071.3590300706	
    Loss 2827.3262375951	
    Loss 3423.5573159287	
    Loss 3047.4331976413	
    Loss 3256.0588189366	
    Loss 6199.2250120443	
    Loss 3252.6711726554	
    Loss 5133.1567864914	
    Loss 3988.4280229366	
    Loss 4537.3686426506	
    Loss 4944.1299933756	
    Loss 2668.0324015955	
    Loss 2811.8462575516	
    Loss 3584.0160175206	
    Loss 4975.8374273685	
    Loss 3365.4283868585	
    Loss 4864.4895301959	
    Loss 3885.0629985254	
    Loss 2567.0245256913	
    Loss 3472.5102916335	
    Loss 3785.4462956877	
    Loss 4004.3847826569	
    Loss 4205.3806619488	
    Loss 5244.4155031003	
    Loss 4068.0410400106	
    Loss 3602.7932636391	
    Loss 2709.4467596024	
    Loss 3955.8182578261	
    Loss 3041.4838835541	
    Loss 4495.3337377554	
Epoch 8	
      0
     17
     20
 129920
      0
      0
      0
      0
      0
      0
   1851
[torch.DoubleTensor of size 11]

Validation accuracy:	0.060504673464433	
Grad norm	10.504428020626	
    Loss 4208.940343868	
    Loss 4735.8178863458	
    Loss 4620.3842480717	
    Loss 5270.6697714857	
    Loss 4609.7668993391	
    Loss 3274.8416587108	
    Loss 2615.2296580748	
    Loss 4045.1136226856	
    Loss 5271.0102285937	
    Loss 4025.9118609237	
    Loss 6078.2391425592	
    Loss 4396.1533296194	
    Loss 4357.9277138424	
    Loss 3929.4487416773	
    Loss 4121.2820528819	
    Loss 3060.8720993104	
    Loss 3963.8450652734	
    Loss 3357.5865544699	
    Loss 5385.1049915259	
    Loss 3610.8700657771	
    Loss 3686.4017271577	
    Loss 3283.6015360055	
    Loss 2877.1416031021	
    Loss 3088.4669810053	
    Loss 2702.8020371877	
    Loss 7367.8999221013	
    Loss 3948.2247898692	
    Loss 5427.3913545384	
    Loss 3740.9423469443	
    Loss 5563.9688105448	
    Loss 6883.9156484652	
    Loss 4416.8278285854	
    Loss 2887.5791435967	
    Loss 3815.652649692	
    Loss 4680.8026419713	
    Loss 4376.5084375271	
    Loss 3240.4489323983	
    Loss 3681.4429983179	
    Loss 2877.1204402034	
    Loss 2617.0771097425	
    Loss 6910.6678142276	
    Loss 5506.3490727444	
    Loss 5474.6319958207	
    Loss 2590.8410025535	
    Loss 4800.3810004311	
    Loss 4401.2110219811	
    Loss 3469.0025617937	
    Loss 3656.4185164319	
    Loss 4355.4510560298	
    Loss 5202.2184751084	
    Loss 3977.7415052311	
    Loss 4327.7230338783	
    Loss 3451.7118945312	
    Loss 3772.8772269207	
    Loss 2407.5005673304	
    Loss 4579.5729037266	
    Loss 3917.1490976458	
    Loss 3202.0549089176	
    Loss 5870.1074778115	
    Loss 4357.1375307464	
    Loss 2590.6259006877	
    Loss 4569.3554762068	
    Loss 3496.1063597328	
    Loss 3743.3288681251	
    Loss 6552.1284731356	
    Loss 2828.0057225802	
    Loss 2835.4711079322	
    Loss 4318.6455406897	
    Loss 3408.1440121256	
    Loss 4401.1327012287	
    Loss 4185.5801825413	
    Loss 2694.5288262169	
Epoch 9	
     0
     0
 12771
 73408
     0
     0
     0
     0
     0
     0
 28865
  6631
     0
     0
   552
  5207
   499
  2869
  1006
[torch.DoubleTensor of size 19]

Validation accuracy:	0.040384498664724	
Grad norm	10.361976693725	
    Loss 4354.891704488	
    Loss 4762.164518186	
    Loss 3798.5251979104	
    Loss 3505.7768463291	
    Loss 5264.7308479371	
    Loss 3850.735218786	
    Loss 3212.8451478011	
    Loss 3766.8093795369	
    Loss 3843.9204465884	
    Loss 4182.7115170081	
    Loss 4990.9182084232	
    Loss 3857.2931972284	
    Loss 2695.4295154536	
    Loss 3855.6401058632	
    Loss 4424.8593695802	
    Loss 4388.492683452	
    Loss 3006.9370997748	
    Loss 4754.3510579359	
    Loss 6828.1150293829	
    Loss 5806.5772456118	
    Loss 3021.9083031952	
    Loss 6106.1246359816	
    Loss 4554.0324351609	
    Loss 3581.0221889223	
    Loss 5074.23459649	
    Loss 3940.916454001	
    Loss 3264.3120252927	
    Loss 6256.7446499546	
    Loss 2847.867790017	
    Loss 3862.9551403156	
    Loss 4665.4977694796	
    Loss 4391.3902569715	
    Loss 3305.6101390818	
    Loss 3232.9634123233	
    Loss 5267.1016290552	
    Loss 3114.532832609	
    Loss 2650.5560242717	
    Loss 3363.5055434645	
    Loss 3442.9894610673	
    Loss 4053.3433385423	
    Loss 2470.7222704985	
    Loss 4855.5746074148	
    Loss 3109.7561017932	
    Loss 3630.4216237	
    Loss 3258.3613740712	
    Loss 3375.2854813222	
    Loss 2837.7379189211	
    Loss 3443.1348721284	
    Loss 4665.5877449571	
    Loss 2688.9373916994	
    Loss 3297.1673816736	
    Loss 3729.2810818955	
    Loss 2777.4957979653	
    Loss 4333.3334038392	
    Loss 5340.0336020901	
    Loss 3510.5499643963	
    Loss 4238.335348421	
    Loss 2622.3166865153	
    Loss 4113.99600078	
    Loss 3450.2151920572	
    Loss 3792.3659850657	
    Loss 3199.6458940983	
    Loss 7926.5422011918	
    Loss 3185.227108502	
    Loss 3775.1205704552	
    Loss 5235.1901515564	
    Loss 3334.4079730593	
    Loss 3742.4639469044	
    Loss 3997.2452504883	
    Loss 3756.3685632424	
    Loss 3539.4083184309	
    Loss 4467.2902091352	
Epoch 10	
  6353
   745
     0
     0
 91545
     0
     0
     0
     0
 33165
[torch.DoubleTensor of size 10]

Validation accuracy:	0.091747086671522	
Grad norm	9.8130230590415	
    Loss 3357.6783476738	
    Loss 3467.1207240981	
    Loss 5780.4899065626	
    Loss 3803.6596206186	
    Loss 2577.1544917356	
    Loss 3500.9004393276	
    Loss 5866.7477028394	
    Loss 5910.7834626217	
    Loss 3012.4392295313	
    Loss 3406.1715807542	
    Loss 4945.6993915905	
    Loss 3135.6436171372	
    Loss 2987.9573863945	
    Loss 3206.8262927812	
    Loss 7120.919652676	
    Loss 4871.4794936182	
    Loss 4139.528945637	
    Loss 6402.7464792827	
    Loss 3546.5818549593	
    Loss 2971.0995205704	
    Loss 5243.2327063645	
    Loss 6986.4855162542	
    Loss 2731.3773525881	
    Loss 3378.983910543	
    Loss 2350.7305946989	
    Loss 3241.9760331995	
    Loss 5335.1121147228	
    Loss 5336.883450152	
    Loss 5516.2921033631	
    Loss 3971.6451570471	
    Loss 3806.868144192	
    Loss 2803.4289179638	
    Loss 2907.0814447139	
    Loss 2361.0674115443	
    Loss 4903.4602888723	
    Loss 6677.9505749405	
    Loss 3689.9972516735	
    Loss 5429.3543683708	
    Loss 4164.0539053481	
    Loss 3879.6062534835	
    Loss 3483.9272971573	
    Loss 5916.9059913047	
    Loss 3374.7854330405	
    Loss 4472.8838600637	
    Loss 6257.6207545649	
    Loss 3663.2520554852	
    Loss 2371.0322070432	
    Loss 4274.7099167485	
    Loss 4243.7461271612	
    Loss 4168.6293238896	
    Loss 3347.0127995781	
    Loss 3477.8919955145	
    Loss 5165.7072040921	
    Loss 4793.2474913838	
    Loss 3365.8879356609	
    Loss 4007.6626759206	
    Loss 3751.1692489793	
    Loss 3420.7418523381	
    Loss 4340.3153175114	
    Loss 4875.8512636171	
    Loss 2811.0936649393	
    Loss 3939.1952838538	
    Loss 5987.7514301343	
    Loss 3438.8287191695	
    Loss 3056.252424137	
    Loss 6480.0697070944	
    Loss 3562.2764124899	
    Loss 4367.126764543	
    Loss 3620.9335869026	
    Loss 3904.8456460594	
    Loss 3910.3510865833	
    Loss 4905.2946071735	
Epoch 11	
     0
     0
   461
  1187
 80910
     0
     0
 39682
     0
     0
    17
    43
     0
     0
     2
    55
    21
  9361
    69
[torch.DoubleTensor of size 19]

Validation accuracy:	0.053448956057295	
Grad norm	10.058214450726	
    Loss 5443.3750500615	
    Loss 4166.4065853377	
    Loss 2618.991236111	
    Loss 4303.0834872096	
    Loss 4597.4692836027	
    Loss 4398.6021555834	
    Loss 3695.332815445	
    Loss 2620.0235083912	
    Loss 3034.1615300405	
    Loss 4208.7366494082	
    Loss 3476.2374827887	
    Loss 4160.9499973816	
    Loss 3342.0348635529	
    Loss 3232.1068041003	
    Loss 3210.0540919736	
    Loss 3138.6799912767	
    Loss 3794.7497786329	
    Loss 3314.9762728744	
    Loss 2726.9241151377	
    Loss 3892.0358486487	
    Loss 3350.6420127872	
    Loss 4670.5126971536	
    Loss 5184.6091804219	
    Loss 2744.4772629402	
    Loss 4636.7681481526	
    Loss 6645.9750508099	
    Loss 4060.3675343965	
    Loss 3593.6060927822	
    Loss 2929.1901352286	
    Loss 3501.8257395842	
    Loss 2595.6103908993	
    Loss 4822.1709961512	
    Loss 4439.6533656453	
    Loss 4974.3532794461	
    Loss 3993.2280435673	
    Loss 3537.3392366625	
    Loss 5291.805616926	
    Loss 6230.6942041884	
    Loss 4493.8470643929	
    Loss 3621.7032372217	
    Loss 4741.3612789759	
    Loss 5368.5215547493	
    Loss 4512.0667994913	
    Loss 2715.9861482025	
    Loss 2944.6371921146	
    Loss 4837.8197000121	
    Loss 3484.7931429916	
    Loss 3822.3971411451	
    Loss 2928.7109846086	
    Loss 4039.9607562783	
    Loss 4055.9367450582	
    Loss 4057.1331216058	
    Loss 4793.6746109632	
    Loss 4974.7002855202	
    Loss 3130.9200073102	
    Loss 3339.6100923402	
