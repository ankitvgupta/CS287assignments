[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	50	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2924
 4882
 5066
 3791
 2833
 3008
 2951
 3408
 4567
 3701
 1097
 6068
 1784
  905
  925
 2169
 3064
 1344
 1619
 1813
 2453
 2321
 8875
 4299
 1821
 5863
 1874
 3813
 2845
 3656
 2479
 4485
 2584
 1545
 2628
 1869
 1510
 1154
 4210
 1466
 3351
 1648
 1363
 1149
 4628
[torch.DoubleTensor of size 45]

Validation accuracy:	0.028002852634135	
Grad norm	0	
    Loss 2279522.3279971	
    Loss 1131088.6562873	
    Loss 562920.87350833	
    Loss 281280.51166692	
    Loss 142419.87893847	
    Loss 73486.287844761	
    Loss 39615.284922729	
    Loss 23280.57546924	
    Loss 14331.807606887	
    Loss 10434.155331115	
    Loss 8053.0987326739	
    Loss 7097.27460152	
    Loss 6724.0850084386	
    Loss 6533.7830852541	
    Loss 6204.3859076359	
    Loss 6254.2150240848	
    Loss 6241.993341649	
    Loss 6716.5637006838	
    Loss 6791.1948314121	
    Loss 6276.2430709242	
    Loss 6550.682793313	
    Loss 6187.7241479404	
    Loss 6494.3225606334	
    Loss 6351.3192403049	
    Loss 6451.9779059295	
    Loss 6048.8098541637	
    Loss 6483.0073834253	
    Loss 6243.6650370757	
    Loss 6277.2823247721	
    Loss 6443.3282186792	
    Loss 6361.273453567	
    Loss 5964.0491483051	
    Loss 5958.6088904958	
    Loss 6375.1619410978	
    Loss 6110.8895965325	
    Loss 6207.5602200695	
    Loss 6385.6090652854	
    Loss 6420.0985667912	
    Loss 6005.6359290734	
    Loss 6128.18801728	
    Loss 6341.2801943052	
    Loss 5897.627580644	
    Loss 6007.2105773405	
    Loss 6379.9425140611	
    Loss 6398.312720372	
    Loss 6336.0127753816	
    Loss 6474.323632715	
    Loss 6614.7256241125	
    Loss 6362.4431025441	
    Loss 5712.6101429506	
    Loss 6035.5260919934	
    Loss 6115.4805205666	
    Loss 6100.319226001	
    Loss 6058.0282652278	
    Loss 5993.1099075266	
    Loss 6174.5028538139	
    Loss 6028.2971738755	
    Loss 6236.4961658813	
    Loss 6066.5808841141	
    Loss 6306.7150441413	
    Loss 5908.000969047	
    Loss 6357.3218245345	
    Loss 5829.7855911141	
    Loss 6495.6794810636	
    Loss 6160.8336574384	
    Loss 6273.9568431175	
    Loss 6349.0782197101	
    Loss 6220.1225719042	
    Loss 6443.0872898316	
    Loss 6663.8855727812	
    Loss 6224.8583772162	
    Loss 6162.70470337	
    Loss 6076.9290740788	
    Loss 6342.0187266457	
    Loss 5946.2781686406	
    Loss 6074.946993166	
    Loss 5979.9277517686	
    Loss 5948.6862176333	
    Loss 5963.5443619622	
    Loss 6009.5293274534	
    Loss 5984.0383224868	
    Loss 6062.9863082366	
    Loss 6431.7492798948	
    Loss 6038.8555260931	
    Loss 6867.0399937507	
    Loss 5942.9950289059	
    Loss 5934.2411835946	
    Loss 6705.555425385	
    Loss 6270.8092251713	
    Loss 6607.2953627425	
    Loss 6591.6362179845	
    Loss 6253.2164345505	
    Loss 6200.4369783352	
    Loss 5930.6300445371	
    Loss 5861.0464827747	
    Loss 6234.6357018467	
    Loss 6078.5271500365	
    Loss 6460.5487218503	
    Loss 6124.5741839515	
    Loss 6393.556145037	
    Loss 6235.4224838637	
    Loss 6231.257085332	
    Loss 6214.6838833381	
    Loss 6396.6335070055	
    Loss 6423.5402645164	
    Loss 6077.3991437866	
    Loss 5990.4541838117	
    Loss 6212.8154097187	
    Loss 6341.550919569	
    Loss 5972.4359278835	
    Loss 6264.3640340754	
    Loss 6164.053300484	
    Loss 6610.0963771181	
    Loss 6089.6607577004	
    Loss 6113.7702574215	
    Loss 6322.1454401454	
    Loss 6800.4732965727	
    Loss 6213.6904337668	
    Loss 6256.8178844057	
    Loss 6331.2376254505	
    Loss 5963.4524205266	
    Loss 6139.4720629814	
    Loss 5941.3865822196	
    Loss 6158.668665605	
    Loss 5850.7557620279	
    Loss 5975.6576441885	
    Loss 6427.1506871717	
    Loss 6927.92729827	
    Loss 6035.8042312432	
    Loss 6604.1739250379	
    Loss 6131.460762106	
    Loss 6210.6552606032	
    Loss 5766.1689488471	
    Loss 6430.6340872316	
    Loss 6054.6926480124	
    Loss 6512.6531954516	
    Loss 6198.1772538135	
    Loss 6465.2375233681	
    Loss 6757.1351692425	
    Loss 6216.4002358096	
    Loss 6484.8532098921	
    Loss 6547.7017642352	
    Loss 6105.1026499851	
Epoch 2	
  7180
     0
 33385
  4117
     0
   419
  1095
   226
     0
  1497
 44277
    40
   238
   283
  1568
 37238
   245
[torch.DoubleTensor of size 17]

Validation accuracy:	0.017935178441369	
Grad norm	9.213703004016	
    Loss 6049.1537249024	
    Loss 6527.0440511814	
    Loss 6488.5355150482	
    Loss 5890.2340325865	
    Loss 6600.5315676744	
    Loss 6037.3638832548	
    Loss 6613.4644955665	
    Loss 7114.7717576718	
    Loss 6185.1501991097	
    Loss 6052.8384696898	
    Loss 5996.0522925401	
    Loss 5964.4022547455	
    Loss 6071.5809549565	
    Loss 6245.3067919395	
    Loss 6028.2028890947	
    Loss 5931.7805710737	
    Loss 6228.5895049197	
    Loss 6626.1129350277	
    Loss 6003.9232509245	
    Loss 6484.444885534	
    Loss 6091.9115034175	
    Loss 6318.0974202218	
    Loss 6193.1637943624	
    Loss 5981.3529599487	
    Loss 6163.9695126264	
    Loss 6445.2482879413	
    Loss 6381.9141438114	
    Loss 6010.2577794811	
    Loss 5860.6062536563	
    Loss 5863.6474478061	
    Loss 5996.6664963456	
    Loss 6089.5716506843	
    Loss 6228.6241156352	
    Loss 6092.1501911783	
    Loss 6008.5801502184	
    Loss 6374.1459386639	
    Loss 6137.7852684437	
    Loss 6260.4605360309	
    Loss 6535.5731041269	
    Loss 6375.8311294678	
    Loss 6251.9776749365	
    Loss 5989.3158306844	
    Loss 6325.6332813151	
    Loss 6444.8583056554	
    Loss 6316.186690969	
    Loss 6280.4945647966	
    Loss 6415.4331467363	
    Loss 6398.3667981794	
    Loss 5939.563679378	
    Loss 6039.5454922968	
    Loss 6417.8913040944	
    Loss 6246.2829233347	
    Loss 6384.9828180311	
    Loss 6326.3162173239	
    Loss 6290.7582126298	
    Loss 6522.1204308701	
    Loss 6482.1222303298	
    Loss 6312.0708814244	
    Loss 6303.7981071509	
    Loss 6000.5266694297	
    Loss 6168.6694282102	
    Loss 5944.5688689349	
    Loss 5985.7285313038	
    Loss 6009.4531122207	
    Loss 5926.8747361754	
    Loss 6226.7977035152	
    Loss 6337.6905259122	
    Loss 6424.6859893442	
    Loss 6266.5539196673	
    Loss 6534.6042394285	
    Loss 6150.063124525	
    Loss 6207.2296860397	
    Loss 6087.3446642949	
    Loss 6350.1668459851	
    Loss 6123.0332979672	
    Loss 6137.8643570962	
    Loss 6115.2469202492	
    Loss 6088.762456387	
    Loss 6431.027261409	
    Loss 6246.2677909587	
    Loss 6045.0073388316	
    Loss 6292.8776148771	
    Loss 5962.0282252519	
    Loss 6087.5483262603	
    Loss 6427.5500733529	
    Loss 6306.6204486291	
    Loss 5726.737676983	
    Loss 5837.1991227748	
    Loss 6073.4984723687	
    Loss 6104.467738118	
    Loss 6068.5107262918	
    Loss 6449.0635501335	
    Loss 6075.0559122215	
    Loss 6233.5397136323	
    Loss 5987.7247199197	
    Loss 6162.9924198662	
    Loss 6408.4641407867	
    Loss 6014.2805199502	
    Loss 6380.4866344957	
    Loss 6541.7471498638	
    Loss 6422.7122870639	
    Loss 6262.6511300449	
    Loss 6307.907317053	
    Loss 6099.0839202174	
    Loss 6379.5645268888	
    Loss 6343.6341084868	
    Loss 6238.2937885202	
    Loss 6257.2919037173	
    Loss 6584.1043290094	
    Loss 6194.9401176439	
    Loss 6488.0970452954	
    Loss 6231.9227021672	
    Loss 6245.8906214831	
    Loss 6297.0661043301	
    Loss 5933.622096955	
    Loss 5860.7435713123	
    Loss 6322.4395715005	
    Loss 6296.87695448	
    Loss 6385.3236780716	
    Loss 6429.4951124924	
    Loss 6041.904263384	
    Loss 5986.8010188041	
    Loss 5616.5012965687	
    Loss 5745.9792505825	
    Loss 5898.6252684527	
    Loss 6719.6647143305	
    Loss 6699.6320664142	
    Loss 6578.0318012412	
    Loss 6149.7417426359	
    Loss 6282.4033224167	
    Loss 6988.9736339437	
    Loss 6183.3820858815	
    Loss 5693.7708293857	
    Loss 6367.1616028437	
    Loss 6086.6691722257	
    Loss 6537.6435570807	
    Loss 6066.5872381674	
    Loss 6490.5838199055	
    Loss 6712.3895277406	
    Loss 6324.8804424315	
    Loss 6338.5737738461	
    Loss 6315.1737071533	
    Loss 6540.3633885314	
Epoch 3	
 106670
      0
    224
     56
      0
      0
      0
      0
  24639
      0
    160
      0
     11
     48
[torch.DoubleTensor of size 14]

Validation accuracy:	0.072962187424132	
Grad norm	9.2879977633238	
    Loss 5921.2594233736	
    Loss 6616.1361418921	
    Loss 6325.4228346311	
    Loss 6469.196577795	
    Loss 6483.7694131627	
    Loss 6385.6700524592	
    Loss 6143.799619614	
    Loss 6650.4164337045	
    Loss 6210.083313826	
    Loss 6295.4382778827	
    Loss 6210.5059067993	
    Loss 6266.6098940638	
    Loss 6023.4985218969	
    Loss 6828.7703989534	
    Loss 6239.0767050527	
    Loss 6004.2732211381	
    Loss 6538.918092394	
    Loss 6468.7057080997	
    Loss 6266.9196998751	
    Loss 6151.6117211638	
    Loss 6760.3356178439	
    Loss 6581.2773518619	
    Loss 6618.8383687353	
    Loss 6745.8805101817	
    Loss 6241.9174261476	
    Loss 5997.2671888762	
    Loss 6139.406273425	
    Loss 6209.6271066877	
    Loss 6350.3329520832	
    Loss 6270.142593701	
    Loss 6142.798969886	
    Loss 6287.8832944842	
    Loss 6031.397068305	
    Loss 5908.0393658478	
    Loss 6152.1618193155	
    Loss 6522.9503944103	
    Loss 6657.3184619619	
    Loss 6053.372261369	
    Loss 6395.3859433276	
    Loss 6123.6521552354	
    Loss 6276.5742944344	
    Loss 5994.4969541685	
    Loss 6235.3896647732	
    Loss 6311.4231943035	
    Loss 6273.6386125056	
    Loss 6389.3343152686	
    Loss 6256.4855008834	
    Loss 6336.8639920043	
    Loss 6519.2759824554	
    Loss 6204.1837865669	
    Loss 6825.4797610703	
    Loss 6085.3049043181	
    Loss 6129.305431944	
    Loss 6156.7320009036	
    Loss 6241.803466372	
    Loss 6140.2322205463	
    Loss 6269.6584705667	
    Loss 6328.6309242312	
    Loss 6084.2886379384	
    Loss 6065.585315823	
    Loss 6342.3444683758	
    Loss 6214.5830908337	
    Loss 6152.7041428147	
    Loss 6285.4461945855	
    Loss 6273.7771146965	
    Loss 6085.401605739	
    Loss 6512.9871363871	
    Loss 6285.0825626675	
    Loss 6243.649202911	
    Loss 7060.3417293711	
    Loss 6247.4668660791	
    Loss 6083.318666026	
    Loss 6263.3251595513	
    Loss 6414.9447898899	
    Loss 6052.610283773	
    Loss 6440.0568674558	
    Loss 6204.4540929031	
    Loss 6509.3107096927	
    Loss 6233.2376597517	
    Loss 6213.8513185881	
    Loss 6442.8221257607	
    Loss 6269.9695200527	
    Loss 6284.4432450818	
    Loss 6457.2949262154	
    Loss 6293.1341469553	
    Loss 6052.4598315734	
    Loss 6020.9893855178	
    Loss 6000.1213850737	
    Loss 6351.1109307591	
    Loss 6003.2681688333	
    Loss 6258.8847215164	
    Loss 7163.0274677429	
    Loss 6006.4555737044	
    Loss 5937.9687275322	
    Loss 5988.5364227817	
    Loss 6528.9129536829	
    Loss 6070.8685732919	
    Loss 6497.4765149592	
    Loss 6182.543839182	
    Loss 6197.4245940721	
    Loss 6451.2589858018	
    Loss 6310.5172021299	
    Loss 5884.3273678603	
    Loss 6394.9162460622	
    Loss 6307.246181894	
    Loss 6647.9382988597	
    Loss 6227.0404026527	
    Loss 5847.4838757194	
    Loss 6189.8818157554	
    Loss 6249.2084980202	
    Loss 6244.4922947218	
    Loss 6359.6115737744	
    Loss 6060.10418052	
    Loss 6158.8463986422	
    Loss 5809.1775589201	
    Loss 5944.2994156441	
    Loss 6619.6241877033	
    Loss 6719.8640833178	
    Loss 6165.5233968737	
    Loss 6272.2674528353	
    Loss 6283.3216863564	
    Loss 6212.6801056524	
    Loss 5999.1681704093	
    Loss 5887.408936174	
    Loss 6025.9694988441	
    Loss 6451.7280714101	
    Loss 6169.8947389871	
    Loss 6035.2874058382	
    Loss 6381.1913631744	
    Loss 7204.6452325489	
    Loss 7340.7704152707	
    Loss 6341.637044809	
    Loss 6125.601231549	
    Loss 5917.2636396003	
    Loss 6260.5214532638	
    Loss 6566.9684194435	
    Loss 6277.1041558559	
    Loss 6422.5763023352	
    Loss 6744.4079198173	
    Loss 6132.0394897186	
    Loss 6361.1653443656	
    Loss 6271.1472638691	
    Loss 6346.9516833329	
Epoch 4	
  3727
   196
  8747
  2853
 61380
   358
   644
 31068
     0
     0
 18852
   535
   521
  1840
    43
   645
   399
[torch.DoubleTensor of size 17]

Validation accuracy:	0.045414542364652	
Grad norm	10.117424445453	
    Loss 6530.2267380039	
    Loss 6541.0485916559	
    Loss 6427.8687169586	
    Loss 6391.493891687	
    Loss 6538.236693421	
    Loss 6338.8844955166	
    Loss 6157.6128975817	
    Loss 6252.346300388	
    Loss 6067.6153342018	
    Loss 6271.2593035974	
    Loss 6290.0978386307	
    Loss 6328.9069850461	
    Loss 6126.004010525	
    Loss 6190.2409996829	
    Loss 5894.4937101436	
    Loss 6192.0920749092	
    Loss 6389.9904837739	
    Loss 6443.163645813	
    Loss 6246.8310596171	
    Loss 5969.4757567963	
    Loss 6786.7934577553	
    Loss 6385.223156841	
    Loss 6535.8777991715	
    Loss 6423.2139387049	
    Loss 6074.6688822689	
    Loss 6282.4855550303	
    Loss 6124.3137882269	
    Loss 6303.0718177645	
    Loss 6015.6911300924	
    Loss 6280.7756861908	
    Loss 6629.8536954341	
    Loss 6179.2581464378	
    Loss 6120.8608097565	
    Loss 6227.3658217318	
    Loss 6354.4542780195	
    Loss 6284.5425271541	
    Loss 6464.8479048839	
    Loss 6104.1411188747	
    Loss 6211.2802342143	
    Loss 6217.0532905571	
    Loss 6673.5955306337	
    Loss 5982.3151357553	
    Loss 6196.1167876243	
    Loss 6037.8624855321	
    Loss 6196.3536823253	
    Loss 7261.9225243129	
    Loss 6327.4979689269	
    Loss 6038.2119856209	
    Loss 6068.1749060902	
    Loss 6160.1235277458	
    Loss 6280.2793430387	
    Loss 6227.4589489079	
    Loss 6085.0377469256	
    Loss 6518.5059594928	
    Loss 6128.0660487975	
    Loss 6083.2624644516	
    Loss 6235.8374552629	
    Loss 6378.2322254034	
    Loss 6450.9599306474	
    Loss 6189.8984903398	
    Loss 5960.2346124921	
    Loss 6000.0133598742	
    Loss 6293.296023282	
    Loss 6617.4398792192	
    Loss 6301.7866422208	
    Loss 6203.8545355854	
    Loss 6349.4087140397	
    Loss 6167.8935077063	
    Loss 6287.6022340029	
    Loss 6512.7597024155	
    Loss 6545.3892370545	
    Loss 6344.8138659396	
    Loss 6104.7665985304	
    Loss 6339.7545798007	
    Loss 6191.8521326406	
    Loss 5968.0834109056	
    Loss 6052.7010680518	
    Loss 6038.5153709497	
    Loss 6477.3990526429	
    Loss 6552.2599419892	
    Loss 6382.0804480032	
    Loss 6196.1106854305	
    Loss 6015.582242143	
    Loss 6138.0745247685	
    Loss 6209.8444809741	
    Loss 6000.5890328286	
    Loss 6123.7220002186	
    Loss 5961.3778154707	
    Loss 6018.4378449436	
    Loss 6204.6244883667	
    Loss 5981.2286757797	
    Loss 6484.2464397465	
    Loss 5983.9760615706	
    Loss 5779.4379406409	
    Loss 6029.1856138508	
    Loss 6174.2776658109	
    Loss 5965.3876713623	
    Loss 5993.2373696309	
    Loss 6117.0828810643	
    Loss 6764.517326852	
    Loss 6525.1629003932	
    Loss 6079.5300547285	
    Loss 6266.1358013354	
    Loss 6236.4875987516	
    Loss 6510.350052766	
    Loss 6173.7419155883	
    Loss 6604.6351908599	
    Loss 6118.4066002291	
    Loss 5868.2954989477	
    Loss 5980.3631255604	
    Loss 6254.9148790698	
    Loss 6197.7282986834	
    Loss 6167.4214865637	
    Loss 6171.5612914599	
    Loss 5856.1252308761	
    Loss 5993.4131454699	
    Loss 6315.1673359228	
    Loss 6337.8437769634	
    Loss 6331.8782791151	
    Loss 6207.9356176434	
    Loss 5960.3774941664	
    Loss 6169.8469681678	
    Loss 5845.477553909	
    Loss 6039.120889852	
    Loss 6159.5987864144	
    Loss 6114.044757748	
    Loss 6214.0533821378	
    Loss 6080.4657060162	
    Loss 6602.3972897298	
    Loss 6222.662847778	
    Loss 6955.6350902662	
    Loss 6039.5663651272	
    Loss 6335.3836819328	
    Loss 6032.6472674608	
    Loss 6030.5479253587	
    Loss 6161.9356483552	
    Loss 6177.9663202298	
    Loss 5970.9167611208	
    Loss 6356.5213713515	
    Loss 6120.9557431513	
    Loss 6315.3059956516	
    Loss 6402.9947997045	
    Loss 6371.7978280786	
Epoch 5	
   143
  7419
    15
     0
 68071
    33
    15
     0
 54887
     0
  1049
     0
   129
    37
     0
     0
    10
[torch.DoubleTensor of size 17]

Validation accuracy:	0.077719106579267	
Grad norm	9.9422896125429	
    Loss 5890.5568672115	
    Loss 6267.5110289992	
    Loss 6167.9898315859	
    Loss 6291.8613871733	
    Loss 6515.6202844943	
    Loss 6082.1157020738	
    Loss 6387.4340758973	
    Loss 6635.5329807796	
    Loss 6225.0299464967	
    Loss 6035.04525305	
    Loss 6301.446722643	
    Loss 6199.2405057979	
    Loss 6083.0484771358	
    Loss 6568.2785251895	
    Loss 6211.5644081892	
    Loss 6331.7921193242	
    Loss 6238.9184897895	
    Loss 6187.9958602911	
    Loss 6330.3591590649	
    Loss 6601.8384682951	
    Loss 6405.6199450547	
    Loss 6123.9662236446	
    Loss 6233.4485802205	
    Loss 6203.8076014321	
    Loss 6050.7106521175	
    Loss 6239.560770846	
    Loss 6349.3762810401	
    Loss 6497.8929041905	
    Loss 6137.8751588772	
    Loss 5955.7512738981	
    Loss 6239.5160365794	
    Loss 6160.5289607954	
    Loss 5892.2809046887	
    Loss 6422.2229971784	
    Loss 6472.2574565164	
    Loss 6145.4214228647	
    Loss 6060.1274783153	
    Loss 6575.4328932079	
    Loss 6175.7531497463	
    Loss 6303.2162941426	
    Loss 6031.8082351262	
    Loss 5855.0006291177	
    Loss 6698.2322841763	
    Loss 6476.6550560674	
    Loss 6157.9904666606	
    Loss 6240.0155341779	
    Loss 6408.5960503987	
    Loss 6275.7944390806	
    Loss 6513.3832563964	
    Loss 6072.6043256463	
    Loss 6475.1520092117	
    Loss 6277.0696600436	
    Loss 6321.9031068757	
    Loss 6400.2651677043	
    Loss 6683.3560047278	
    Loss 6788.9897856993	
    Loss 6352.7890753017	
    Loss 6617.1546514414	
    Loss 6251.7196690555	
    Loss 6333.5619139111	
    Loss 5961.1299825669	
    Loss 5989.5935518253	
    Loss 6370.7212746105	
    Loss 6402.78736809	
    Loss 6343.6247546572	
    Loss 6244.8942492274	
    Loss 6698.0942171211	
    Loss 6123.4808268517	
    Loss 6596.4539478325	
    Loss 6550.7559244274	
    Loss 5987.7800868163	
    Loss 5908.6054303414	
    Loss 6202.7601323245	
    Loss 6319.2243903967	
    Loss 6073.4250908624	
    Loss 6459.7588183631	
    Loss 6129.6040171426	
    Loss 5867.2478317571	
    Loss 6355.9951725605	
    Loss 6525.0012947839	
    Loss 6480.1489504293	
    Loss 6331.8029592751	
    Loss 6051.2326498997	
    Loss 6282.9615355257	
    Loss 6012.482309765	
    Loss 6031.1510586591	
    Loss 6543.272677725	
    Loss 6050.2702101251	
    Loss 6321.7152869164	
    Loss 6090.9095191137	
    Loss 6309.8432622095	
    Loss 6593.9308028817	
    Loss 6105.9480863396	
    Loss 6046.7182044886	
    Loss 5949.7890606919	
    Loss 6288.9258831044	
    Loss 6145.9582019624	
    Loss 5929.1269042389	
    Loss 6348.0560618697	
    Loss 6496.9051731574	
    Loss 6308.5281648903	
    Loss 5984.4737121165	
    Loss 6292.7133598076	
    Loss 6251.346497458	
    Loss 6296.6151298535	
    Loss 6027.9748176115	
    Loss 6123.4234064073	
    Loss 6020.3740554595	
    Loss 5944.0671208947	
    Loss 6280.0068505382	
    Loss 6027.4945069412	
    Loss 6261.3816642692	
    Loss 6320.2871273349	
    Loss 5978.9321944188	
    Loss 6241.7694180874	
    Loss 6120.6411790912	
    Loss 6133.3471872464	
    Loss 6588.519297463	
    Loss 6276.6638776506	
    Loss 6095.4390886957	
    Loss 5751.6525565154	
    Loss 6368.5981119161	
    Loss 5859.4659812151	
    Loss 5995.9482057881	
    Loss 6051.561928066	
    Loss 6291.4686680817	
    Loss 6147.7739238039	
    Loss 6266.7081980078	
    Loss 6488.091637092	
    Loss 6721.730302821	
    Loss 6523.6727846258	
    Loss 6374.0193988095	
    Loss 6149.6252383824	
    Loss 6053.4478947664	
    Loss 5674.8367731576	
    Loss 6753.3568633982	
    Loss 6220.4934325809	
    Loss 6538.828060093	
    Loss 6477.9146713379	
    Loss 6504.1234464073	
    Loss 6475.9267075483	
    Loss 6138.0459042168	
    Loss 6515.7187730671	
Epoch 6	
      0
      0
    165
     27
     37
    267
      4
 113622
    150
      0
  14144
      8
     95
    200
     51
   2818
    132
     88
[torch.DoubleTensor of size 18]

Validation accuracy:	0.064343590677349	
Grad norm	9.2634173382985	
    Loss 6418.303360823	
    Loss 6374.3303280545	
    Loss 6474.8988023109	
    Loss 5926.3176567487	
    Loss 6417.9470381766	
    Loss 6152.0982445067	
    Loss 6032.3462634274	
    Loss 6652.3986314967	
    Loss 6417.6411438955	
    Loss 6125.2033627694	
    Loss 5919.3323302313	
    Loss 6044.0207652237	
    Loss 6497.4643743261	
    Loss 6731.9694350114	
    Loss 6003.7286965332	
    Loss 6134.5323982356	
    Loss 6382.1259465734	
    Loss 6573.7886058167	
    Loss 5959.0986783978	
    Loss 5872.9680640984	
    Loss 6294.2290989769	
    Loss 6471.1898331882	
    Loss 6442.0173070449	
    Loss 6180.680147452	
    Loss 6059.3261389915	
    Loss 5819.6983230425	
    Loss 6031.9697707468	
    Loss 5961.5263628477	
    Loss 6378.3375843454	
    Loss 6182.9372203983	
    Loss 5987.5620060163	
    Loss 6164.2792147568	
    Loss 6302.0016977012	
    Loss 6174.5714027132	
    Loss 6014.4139686358	
    Loss 6213.8833924255	
    Loss 6209.0983799577	
    Loss 5948.6649973855	
    Loss 6404.3830170866	
    Loss 6420.8234494928	
    Loss 6486.0040360916	
    Loss 6118.2270681226	
    Loss 6171.5732518965	
    Loss 6189.5935198032	
    Loss 6160.9807829151	
    Loss 6721.1239086869	
    Loss 6125.8669271107	
    Loss 6177.7734902847	
    Loss 6710.0867061263	
    Loss 5801.1200950708	
    Loss 6486.9024361369	
    Loss 6201.9139139162	
    Loss 6028.5196521774	
    Loss 6713.4914217021	
    Loss 6621.1695590519	
    Loss 6282.3610405948	
    Loss 6593.5972046277	
    Loss 6085.1147720161	
    Loss 6243.7627069153	
    Loss 6030.7803114994	
    Loss 6301.4211556172	
    Loss 6150.1278445145	
    Loss 6103.3473071013	
    Loss 6249.2935321946	
    Loss 6146.1855048276	
    Loss 6391.1945866332	
    Loss 6265.8445849197	
    Loss 6152.477446577	
    Loss 6621.2555680799	
    Loss 6884.2937240231	
    Loss 6110.5302555391	
    Loss 6154.9555252021	
    Loss 6305.4798208119	
    Loss 6456.8281643186	
    Loss 6059.7429534419	
    Loss 6118.6377699721	
    Loss 6029.3039213064	
    Loss 5886.4804310289	
    Loss 6055.1166718214	
    Loss 6228.2651032443	
    Loss 6193.6735835035	
    Loss 6619.6003743903	
    Loss 6094.5485216766	
    Loss 5887.0818826329	
    Loss 6076.7132225949	
    Loss 5972.3289837428	
    Loss 6392.3060220126	
    Loss 6002.6963664136	
    Loss 6413.3265704445	
    Loss 6712.4026260355	
    Loss 6173.3371661767	
    Loss 6143.0223258059	
    Loss 6359.3419678123	
    Loss 5899.7745730052	
    Loss 6132.4324640229	
    Loss 5749.3251492893	
    Loss 6188.1580442681	
    Loss 6079.4907633118	
    Loss 6284.8976532394	
    Loss 6134.9539297639	
    Loss 6145.3704349366	
    Loss 5977.5778128492	
    Loss 6023.6582469075	
    Loss 6024.3612363461	
    Loss 6198.2336188408	
    Loss 6446.6854997312	
    Loss 6066.6273894135	
    Loss 5944.8758798356	
    Loss 5876.8489717493	
    Loss 6042.1157331523	
    Loss 6798.6912269772	
    Loss 6413.3381883998	
    Loss 6141.257626092	
    Loss 6272.5560382976	
    Loss 6212.1815278059	
    Loss 6217.9730937137	
    Loss 6256.9602237226	
    Loss 6057.2537646094	
    Loss 6335.0555047495	
    Loss 6030.5510810512	
    Loss 5902.8395757802	
    Loss 6007.266027146	
    Loss 5673.9238032354	
    Loss 5842.8016649581	
    Loss 5926.2299050536	
    Loss 6248.3899029788	
    Loss 6057.8432325725	
    Loss 6091.4521410922	
    Loss 6192.3357901459	
    Loss 6464.9048737531	
    Loss 6699.3896679191	
    Loss 5977.2177048708	
    Loss 5990.6285009879	
    Loss 6124.2335630783	
    Loss 5978.5874132435	
    Loss 6393.163719569	
    Loss 5954.7546424779	
    Loss 6096.3666021569	
    Loss 6163.7110583737	
    Loss 6405.2180541826	
    Loss 6056.8578302169	
    Loss 6361.1780946896	
    Loss 6234.8849629105	
Epoch 7	
 88015
 16869
     0
   280
     0
    21
    82
 14024
 12413
     0
     0
     1
    40
    25
     9
     0
    18
    11
[torch.DoubleTensor of size 18]

Validation accuracy:	0.085396941005098	
Grad norm	9.4568899799906	
    Loss 6287.3324760155	
    Loss 6137.1445603638	
    Loss 6264.3493362531	
    Loss 6128.5552602884	
    Loss 6640.4425988568	
    Loss 6152.96346841	
    Loss 6485.3439519115	
    Loss 6468.6283134648	
    Loss 6252.0483132607	
    Loss 6719.7333036278	
    Loss 5959.0804473672	
    Loss 6384.6100054779	
    Loss 6100.2697071731	
    Loss 6264.7719116296	
    Loss 6563.9096424597	
    Loss 5969.1018299569	
    Loss 6303.724752906	
    Loss 6467.7755371481	
    Loss 6538.3581555775	
    Loss 5998.1966310661	
    Loss 6482.2372793063	
    Loss 6597.6065485704	
    Loss 6449.7494476623	
    Loss 6567.7307299155	
    Loss 6002.9610808675	
    Loss 5836.1167568531	
    Loss 5836.8889837594	
    Loss 5956.3556691022	
    Loss 6490.0546204009	
    Loss 5973.0273907503	
    Loss 6265.3099444374	
    Loss 6038.9405103008	
    Loss 6240.3793805866	
    Loss 6056.9257352604	
    Loss 6403.2859710715	
    Loss 5996.6318673703	
    Loss 6163.6164819294	
    Loss 6169.2785467738	
    Loss 6164.4150691514	
    Loss 6181.6930574216	
    Loss 6739.062836281	
    Loss 5874.0103011976	
    Loss 6669.246434398	
    Loss 6029.5720249194	
    Loss 6748.2719595005	
    Loss 6700.2076777073	
    Loss 6097.8564084448	
    Loss 6303.38337575	
    Loss 6332.7536705766	
    Loss 5958.9478366653	
    Loss 6688.6433940366	
    Loss 5977.92750408	
    Loss 6001.7022823352	
    Loss 6253.7873674213	
    Loss 5873.9150772441	
    Loss 6349.8473974136	
    Loss 6159.3639909858	
    Loss 6597.324902247	
    Loss 5759.0333054188	
    Loss 6091.9072769936	
    Loss 5996.7143262978	
    Loss 5988.293495181	
    Loss 6430.6312819222	
    Loss 6036.8998206509	
    Loss 6394.2788289658	
    Loss 6138.8729544343	
    Loss 6252.6858290875	
    Loss 5981.5637746108	
    Loss 6034.7774893507	
    Loss 6275.5363614768	
    Loss 6202.63573698	
    Loss 6140.8660340082	
    Loss 6358.5163078269	
    Loss 6136.0257521355	
    Loss 5889.8950399867	
    Loss 6193.6724077008	
    Loss 6268.3635872646	
    Loss 5823.4605923812	
    Loss 6402.3655840043	
    Loss 6081.6880959515	
    Loss 6127.6738420376	
    Loss 6179.299237657	
    Loss 6006.8193322488	
    Loss 5889.6941496568	
    Loss 5825.5607052033	
    Loss 6276.2018561294	
    Loss 5666.2542493215	
    Loss 6404.8062581953	
    Loss 6193.6087459234	
    Loss 6325.0352135772	
    Loss 6161.4734669425	
    Loss 6134.5896969545	
    Loss 6140.2413737927	
    Loss 5726.0158217064	
    Loss 6396.2510177207	
    Loss 6314.3344251968	
    Loss 5811.350104778	
    Loss 5981.4253491982	
    Loss 5993.1534252847	
    Loss 6582.0723218041	
    Loss 6166.609350509	
    Loss 5972.0717978579	
    Loss 6158.8859137499	
    Loss 6236.1114442694	
    Loss 6538.5226289118	
    Loss 6408.3928176277	
    Loss 5958.7039373498	
    Loss 6268.7073395758	
    Loss 5952.668915452	
    Loss 5972.6770916015	
    Loss 5982.2701639162	
    Loss 6321.6489715045	
    Loss 6408.8943847416	
    Loss 5965.7866437589	
    Loss 5799.0656497564	
    Loss 6174.4780523127	
    Loss 5922.7373855941	
    Loss 6694.8739921028	
    Loss 6490.114322096	
    Loss 6170.8286755798	
    Loss 5765.449793219	
    Loss 6240.262890851	
    Loss 6296.8968284473	
    Loss 6043.6114971363	
    Loss 5906.8120197917	
    Loss 6002.9706562842	
    Loss 6646.4456201594	
    Loss 6157.900384813	
    Loss 6564.1209829159	
    Loss 6840.964629026	
    Loss 6688.6139482836	
    Loss 6042.377512525	
    Loss 6036.3426896804	
    Loss 6522.0364870867	
    Loss 6013.7968563573	
    Loss 7023.20533066	
    Loss 6374.5756580829	
    Loss 6232.5079896957	
    Loss 6599.2541622709	
    Loss 6369.1467553065	
    Loss 6277.8853789245	
    Loss 6357.5179403652	
    Loss 6787.2651601596	
Epoch 8	
     0
     0
  1252
    88
 86815
     0
     5
     0
 40705
     0
  2190
     0
     0
     0
     0
   751
     1
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.071133770332605	
Grad norm	8.9989534226791	
    Loss 6345.4111827801	
    Loss 6158.5407629602	
    Loss 5982.2695568669	
    Loss 6057.5510401947	
    Loss 6285.1491156271	
    Loss 6163.1930202051	
    Loss 5853.7710841184	
    Loss 6651.6756040477	
    Loss 6489.8393469362	
    Loss 6666.0399172831	
    Loss 6262.5982542112	
    Loss 5944.3888610703	
    Loss 6048.7502632204	
    Loss 6463.3409896285	
    Loss 6020.0236395929	
    Loss 6270.5925685812	
    Loss 6130.6883718894	
    Loss 6742.4632206712	
    Loss 6502.5973261824	
    Loss 5964.7411802776	
    Loss 6235.1907703092	
    Loss 6218.834074394	
    Loss 6363.7523206598	
    Loss 6266.5398421888	
    Loss 6125.7933790796	
    Loss 6210.0272419267	
    Loss 5764.8378130577	
    Loss 6305.3698560242	
    Loss 6646.4521359626	
    Loss 6466.5696826981	
    Loss 6506.9291326607	
    Loss 6311.9863980746	
    Loss 6330.8095324059	
    Loss 6303.1064467236	
    Loss 6581.4645120039	
    Loss 6626.934991181	
    Loss 6289.436033473	
    Loss 6060.1058372116	
    Loss 6045.6565640449	
    Loss 6151.3737929039	
    Loss 6536.1472195375	
    Loss 6210.2388722335	
    Loss 6333.2997396026	
    Loss 6168.9397285096	
    Loss 6452.3349499824	
    Loss 6282.8240313516	
    Loss 6203.4738389114	
    Loss 6330.9412961365	
    Loss 5858.5077600312	
    Loss 5954.2106543469	
    Loss 6307.2563728452	
    Loss 6124.6216183253	
    Loss 6349.4272089137	
    Loss 6310.2612883431	
    Loss 6154.5265314627	
    Loss 6166.6192636571	
    Loss 6042.3482059374	
    Loss 6043.4737190627	
    Loss 6228.2718578678	
    Loss 6287.5545426919	
    Loss 5968.1323887872	
    Loss 6558.4916287728	
    Loss 6310.3962634537	
    Loss 6161.5513859472	
    Loss 6261.6923256984	
    Loss 6154.6335540364	
    Loss 6626.8905462156	
    Loss 6090.5352477024	
    Loss 6394.7094097287	
    Loss 6696.0576020987	
    Loss 6491.3604640306	
    Loss 6716.5493713125	
    Loss 6300.8335135793	
    Loss 6363.097538154	
    Loss 6111.7794182927	
    Loss 6261.9932470876	
    Loss 6355.5420597313	
    Loss 6119.3043169865	
    Loss 6313.0452494501	
    Loss 6042.0877471075	
    Loss 5899.1763210979	
    Loss 6650.2483938018	
    Loss 6163.0225905401	
    Loss 6171.0463883736	
    Loss 6346.6428873513	
    Loss 6040.063012905	
    Loss 5650.6038459553	
    Loss 6121.382284879	
    Loss 6154.4637544972	
    Loss 6407.2054025254	
    Loss 6499.8606996188	
    Loss 6300.3549074486	
    Loss 6075.675938317	
    Loss 5802.8057566001	
    Loss 6230.848391322	
    Loss 6162.8216740474	
    Loss 5949.403539308	
    Loss 5949.9839563385	
    Loss 6152.3658054302	
    Loss 6301.6412276175	
    Loss 6131.8321004745	
    Loss 6052.2310918674	
    Loss 5952.5420682434	
    Loss 6569.1970094665	
    Loss 6829.5307994039	
    Loss 6546.5240650698	
    Loss 6360.4962724128	
    Loss 6435.0861021776	
    Loss 6028.6324254601	
    Loss 6163.3877039281	
    Loss 6465.1609326202	
    Loss 6072.6405463611	
    Loss 6126.2074675136	
    Loss 5941.6072626799	
    Loss 5969.1527767369	
    Loss 6283.2621843932	
    Loss 6636.4185968758	
    Loss 6119.7554649526	
    Loss 6520.479150329	
    Loss 6217.847263238	
    Loss 6176.4137935779	
    Loss 5849.1341575211	
    Loss 6339.8719177605	
    Loss 6194.6776704898	
    Loss 6271.9791071331	
    Loss 6501.6290247056	
    Loss 6028.0158260933	
    Loss 6497.7837966144	
    Loss 6153.4238276279	
    Loss 6139.4459691771	
    Loss 7167.1268001961	
    Loss 6229.3638934589	
    Loss 6279.8907501123	
    Loss 6437.3776347531	
    Loss 5904.7817889651	
    Loss 6820.9631707018	
    Loss 5985.1403330737	
    Loss 6237.6956480228	
    Loss 6465.2364938107	
    Loss 6197.0240562684	
    Loss 6312.483112344	
    Loss 6692.1767283647	
    Loss 6704.7224861477	
Epoch 9	
      0
      0
   5955
   1054
      5
      1
    299
     23
 116870
      0
   6940
      0
     16
      5
     90
    550
[torch.DoubleTensor of size 16]

Validation accuracy:	0.12460548676863	
Grad norm	9.5395922088883	
    Loss 5964.2392432909	
    Loss 7247.5157745015	
    Loss 6122.6600158879	
    Loss 5866.0619635282	
    Loss 6100.5408121886	
    Loss 6501.8996805303	
    Loss 6208.200887551	
    Loss 6674.9395171228	
    Loss 6504.9490299693	
    Loss 6453.1877537282	
    Loss 5975.792280473	
    Loss 6086.3798960714	
    Loss 5972.502530946	
    Loss 6034.298259841	
    Loss 6084.4657399555	
    Loss 5914.476729426	
    Loss 6238.480477809	
    Loss 6317.2929021831	
    Loss 6397.0520969669	
    Loss 6076.5296481495	
    Loss 6304.0042302674	
    Loss 6091.0402627146	
    Loss 6261.5935399639	
    Loss 6737.1144117032	
    Loss 5964.438359888	
    Loss 6064.4154584615	
    Loss 6250.852358714	
    Loss 6729.5904519366	
    Loss 5981.2743218229	
    Loss 6241.2266442566	
    Loss 6193.0880790014	
    Loss 5963.6605745803	
    Loss 5965.0621716856	
    Loss 6508.6007840817	
    Loss 6302.9667935479	
    Loss 6199.2037663486	
    Loss 6143.236572355	
    Loss 6077.269503015	
    Loss 6488.6672355692	
    Loss 5980.964930763	
    Loss 6040.2508077856	
    Loss 6079.6796371334	
    Loss 6445.9829827563	
    Loss 6295.1933653819	
    Loss 6396.539359964	
    Loss 6745.0770639947	
    Loss 6187.5227054487	
    Loss 6215.3651097986	
    Loss 6717.3606161514	
    Loss 6425.6430967486	
    Loss 6260.485459684	
    Loss 6240.784492184	
    Loss 5892.0048923031	
    Loss 6141.3127057969	
    Loss 6177.3330027969	
    Loss 6252.8085343476	
    Loss 5986.6061320576	
    Loss 6610.7623498134	
    Loss 6392.6653039428	
    Loss 6161.8088606154	
    Loss 5856.6161418544	
    Loss 6328.7148401574	
    Loss 6654.585210427	
    Loss 6123.6356471755	
    Loss 6121.9761402003	
    Loss 6276.7335308477	
    Loss 6124.5285330546	
    Loss 5979.0012916225	
    Loss 6317.7903309553	
    Loss 6716.230363501	
    Loss 6302.2351997918	
    Loss 6001.40971091	
    Loss 6000.8382609366	
    Loss 6172.6524836764	
    Loss 6055.257436262	
    Loss 6072.0569967367	
    Loss 6595.2516469243	
    Loss 5981.3268192204	
    Loss 5939.5605496052	
    Loss 5905.6848794592	
    Loss 5896.2240686743	
    Loss 6365.6732526376	
    Loss 5838.1478411324	
    Loss 6144.4728274538	
    Loss 6061.8398464266	
    Loss 6066.5295222735	
    Loss 6141.2510318208	
    Loss 6245.0152198381	
    Loss 6122.9863206088	
    Loss 6172.1932885889	
    Loss 6185.8867878122	
    Loss 6357.3193663686	
    Loss 6051.2125635304	
    Loss 5742.3120870117	
    Loss 5801.6256544106	
    Loss 6084.7252204356	
    Loss 5885.2666700553	
    Loss 5939.3633238175	
    Loss 6191.1573437169	
    Loss 6284.4716653599	
    Loss 6425.7253666478	
    Loss 6062.6033272605	
    Loss 6044.1497501853	
    Loss 6066.9928097953	
    Loss 6437.4083628772	
    Loss 6503.5900506238	
    Loss 6105.5129148933	
    Loss 6818.7762422874	
    Loss 6273.5517920237	
    Loss 5936.8907888002	
    Loss 6051.4634821233	
    Loss 6230.6625982705	
    Loss 6249.7833006834	
    Loss 6108.9598614686	
    Loss 5879.8162278255	
    Loss 6123.302898074	
    Loss 6251.6084814406	
    Loss 6643.7526783014	
    Loss 6107.92332308	
    Loss 6314.8865214692	
    Loss 5896.8453699248	
    Loss 5968.3271065747	
    Loss 5873.2859324648	
    Loss 5891.8435660844	
    Loss 6012.5259214536	
    Loss 6359.8577164383	
    Loss 6347.074211613	
    Loss 6360.1502046886	
    Loss 6838.5058000885	
    Loss 6495.0300539609	
    Loss 6650.3966428025	
    Loss 6178.5445075232	
    Loss 5854.4261129576	
    Loss 6265.7123949027	
    Loss 5951.9989352169	
    Loss 6652.7871345206	
    Loss 6494.065031608	
    Loss 6146.4043427818	
    Loss 6425.1247810783	
    Loss 6028.0543652824	
    Loss 6988.6887556076	
    Loss 5943.7266681043	
    Loss 6310.1862670927	
Epoch 10	
 91897
 20159
     6
     2
     0
    70
    90
 14645
     0
    55
  4692
     1
    28
   145
     1
     3
    12
     2
[torch.DoubleTensor of size 18]

Validation accuracy:	0.091064275309541	
Grad norm	9.4962323514701	
    Loss 6003.7343410164	
    Loss 6720.6070609247	
    Loss 6129.0724934317	
    Loss 6061.9746600332	
    Loss 6503.7289772247	
    Loss 6715.3798718482	
    Loss 6072.6448530801	
    Loss 6556.5201545941	
    Loss 6410.1135716372	
    Loss 6188.7980361649	
    Loss 6385.5252996516	
    Loss 5922.158765938	
    Loss 5968.0464340968	
    Loss 6178.3825152423	
    Loss 5986.0299536444	
    Loss 5991.8840585407	
    Loss 6364.0046530855	
    Loss 6387.9467160231	
    Loss 6687.1768438974	
    Loss 6032.0713658289	
    Loss 6354.3159670924	
    Loss 6092.0444973525	
    Loss 6467.5818088572	
    Loss 6576.7094168734	
    Loss 6334.710531284	
    Loss 5954.0223895752	
    Loss 6035.235066438	
    Loss 6032.0537344218	
    Loss 6094.1178858306	
    Loss 6232.7065390538	
    Loss 6256.5379949976	
    Loss 6656.9588801843	
    Loss 6290.8965542576	
    Loss 6781.8217920854	
    Loss 6060.3641802634	
    Loss 6492.4942511873	
    Loss 6623.4272333224	
    Loss 6230.4860943793	
    Loss 6390.8407655584	
    Loss 5925.5898501439	
    Loss 6685.7066988116	
    Loss 5995.2005954622	
    Loss 6125.3751529906	
    Loss 6292.8269247076	
    Loss 6394.1878386955	
    Loss 6462.8200372922	
    Loss 6181.8964456967	
    Loss 6170.3037312572	
    Loss 6182.1673284272	
    Loss 6065.0357154738	
    Loss 6413.1970212844	
    Loss 6509.027587757	
    Loss 6280.4468148991	
    Loss 6529.2402300741	
    Loss 6327.1327977788	
    Loss 6654.4842382076	
    Loss 6368.4356656532	
    Loss 6422.5031879686	
    Loss 6112.8132622442	
    Loss 6218.563424514	
    Loss 6051.3593465746	
    Loss 6245.5578391853	
