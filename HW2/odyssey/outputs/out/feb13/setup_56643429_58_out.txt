[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	0.25	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2166
 5045
 1620
 1593
 2086
 2005
 1965
 2344
 1132
 2055
 4921
 2124
 5059
 1620
 2787
 1152
 3442
 1634
 3823
 3126
 5896
 2499
 5268
 3113
 1429
 3695
 4158
 6570
 2481
 2374
 3101
 4418
 3294
  872
 3567
 2881
 2057
  806
 4520
 3040
 2616
 1303
 6775
 1545
 1831
[torch.DoubleTensor of size 45]

Validation accuracy:	0.030081633891721	
Grad norm	0	
    Loss 22766019.550456	
    Loss 21223506.015488	
    Loss 19785567.036866	
    Loss 18445051.867643	
    Loss 17195367.16171	
    Loss 16030348.465912	
    Loss 14944269.329307	
    Loss 13931774.420557	
    Loss 12987880.914181	
    Loss 12107942.66209	
    Loss 11287618.551094	
    Loss 10522873.378389	
    Loss 9809948.2584382	
    Loss 9145326.1196817	
    Loss 8525735.2014502	
    Loss 7948119.9165177	
    Loss 7409644.3289523	
    Loss 6907648.905854	
    Loss 6439664.5985913	
    Loss 6003395.5197845	
    Loss 5596681.051285	
    Loss 5217523.6628039	
    Loss 4864053.1834682	
    Loss 4534533.4121328	
    Loss 4227341.5651602	
    Loss 3940963.1802613	
    Loss 3673986.1299009	
    Loss 3425099.5191275	
    Loss 3193072.8333412	
    Loss 2976765.7851592	
    Loss 2775118.6418252	
    Loss 2587132.5255452	
    Loss 2411880.205195	
    Loss 2248503.275811	
    Loss 2096195.3650125	
    Loss 1954206.0651721	
    Loss 1821839.8194829	
    Loss 1698441.3101427	
    Loss 1583402.7799802	
    Loss 1476156.5951788	
    Loss 1376178.3210874	
    Loss 1282972.1802886	
    Loss 1196082.098464	
    Loss 1115078.8170145	
    Loss 1039563.2117628	
    Loss 969165.79304117	
    Loss 903538.08800172	
    Loss 842355.01289145	
    Loss 785317.33214048	
    Loss 732144.05413717	
    Loss 682573.7356739	
    Loss 636361.62413551	
    Loss 593280.63207652	
    Loss 553118.74923033	
    Loss 515677.13501259	
    Loss 480771.84459582	
    Loss 448232.22812898	
    Loss 417898.22454289	
    Loss 389618.92522905	
    Loss 363254.32888458	
    Loss 338676.89806566	
    Loss 315764.74088595	
    Loss 294404.29462564	
    Loss 274491.08709347	
    Loss 255927.49212127	
    Loss 238620.40437954	
    Loss 222487.03465665	
    Loss 207446.97547133	
    Loss 193425.65403869	
    Loss 180353.69717245	
    Loss 168167.47551225	
    Loss 156807.34117593	
Epoch 2	
 86476
    72
     0
   443
    52
     0
     0
     9
 44321
   435
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10984917455693	
Grad norm	422.89924436313	
    Loss 153551.30661003	
    Loss 143181.38414728	
    Loss 133514.21533624	
    Loss 124501.8801439	
    Loss 116100.16518119	
    Loss 108268.04655638	
    Loss 100965.55393114	
    Loss 94157.407335711	
    Loss 87811.494460955	
    Loss 81895.937029686	
    Loss 76380.422038912	
    Loss 71239.016591605	
    Loss 66445.833936977	
    Loss 61976.513988217	
    Loss 57810.551443732	
    Loss 53927.021353968	
    Loss 50307.052376783	
    Loss 46931.740929117	
    Loss 43784.262419558	
    Loss 40851.300604519	
    Loss 38116.541390037	
    Loss 35567.662680193	
    Loss 33191.20692269	
    Loss 30975.571409922	
    Loss 28910.052462872	
    Loss 26984.169844314	
    Loss 25188.658312972	
    Loss 23515.757285547	
    Loss 21956.353991862	
    Loss 20501.720370633	
    Loss 19146.198575035	
    Loss 17881.761406201	
    Loss 16703.481364333	
    Loss 15605.23446733	
    Loss 14580.898294452	
    Loss 13625.503566762	
    Loss 12735.425532412	
    Loss 11905.415871571	
    Loss 11132.013666495	
    Loss 10411.110788412	
    Loss 9739.2974776899	
    Loss 9112.2074179555	
    Loss 8527.9575278576	
    Loss 7982.4383496284	
    Loss 7474.4257393417	
    Loss 7002.2137386542	
    Loss 6561.437899728	
    Loss 6149.2987489218	
    Loss 5765.680318335	
    Loss 5407.6765369219	
    Loss 5074.7000061127	
    Loss 4763.9293183362	
    Loss 4474.3130081223	
    Loss 4204.3082160682	
    Loss 3952.8996517172	
    Loss 3717.6041657213	
    Loss 3498.5800067976	
    Loss 3296.1041211754	
    Loss 3106.5639544907	
    Loss 2928.7094609791	
    Loss 2763.8833931299	
    Loss 2609.6332510086	
    Loss 2465.7525158092	
    Loss 2331.2031216301	
    Loss 2206.672509697	
    Loss 2089.2732984113	
    Loss 1980.8212198038	
    Loss 1880.1730909823	
    Loss 1785.9850338113	
    Loss 1697.8742030282	
    Loss 1615.9201777714	
    Loss 1539.4052871788	
Epoch 3	
 120266
      0
      0
      0
      0
      0
      0
      0
  11542
[torch.DoubleTensor of size 9]

Validation accuracy:	0.092437484826414	
Grad norm	39.613143045637	
    Loss 1517.0184879869	
    Loss 1447.3293026576	
    Loss 1382.8319139837	
    Loss 1322.3071886268	
    Loss 1265.9975848487	
    Loss 1214.020558099	
    Loss 1164.3468091025	
    Loss 1117.7237824619	
    Loss 1075.2607993795	
    Loss 1035.9162845927	
    Loss 998.71770150663	
    Loss 964.66669698174	
    Loss 932.37422724341	
    Loss 901.61262485957	
    Loss 873.45048341795	
    Loss 847.60556135772	
    Loss 823.76147029177	
    Loss 801.02188273201	
    Loss 778.91455862993	
    Loss 759.45206406147	
    Loss 740.86652848003	
    Loss 724.16997482889	
    Loss 708.33273648008	
    Loss 693.39032399654	
    Loss 679.37703516942	
    Loss 666.01523486158	
    Loss 653.52918635493	
    Loss 642.72472851653	
    Loss 633.06476658307	
    Loss 623.17348950736	
    Loss 614.32296656076	
    Loss 605.32630929213	
    Loss 597.6689723792	
    Loss 590.60459263661	
    Loss 583.51007403986	
    Loss 576.4579905686	
    Loss 570.37613714754	
    Loss 564.43823487272	
    Loss 559.33530782188	
    Loss 554.80059734701	
    Loss 550.73252971939	
    Loss 546.20833576448	
    Loss 542.32139310288	
    Loss 537.79938154867	
    Loss 534.16158135182	
    Loss 532.10286040515	
    Loss 529.62905067348	
    Loss 526.12168372919	
    Loss 523.46496936854	
    Loss 520.62121109609	
    Loss 518.73121649492	
    Loss 516.62348865591	
    Loss 514.75200367839	
    Loss 512.98916445483	
    Loss 511.72289853787	
    Loss 509.58587653016	
    Loss 507.88616839697	
    Loss 508.05041465966	
    Loss 507.4072582028	
    Loss 505.66370892708	
    Loss 505.02204317882	
    Loss 503.77310252318	
    Loss 502.57427179671	
    Loss 501.00446726026	
    Loss 500.46465266034	
    Loss 498.66458051095	
    Loss 497.97208902001	
    Loss 497.79142806282	
    Loss 497.24783700766	
    Loss 496.46097169141	
    Loss 495.92531649797	
    Loss 495.26518563826	
Epoch 4	
 121639
      0
      0
      0
      0
      0
      0
      0
  10169
[torch.DoubleTensor of size 9]

Validation accuracy:	0.092551286720078	
Grad norm	8.6044349493213	
    Loss 494.62429021259	
    Loss 494.19313841713	
    Loss 494.28299495317	
    Loss 493.94861695992	
    Loss 493.75969145552	
    Loss 494.10811134837	
    Loss 493.20540782537	
    Loss 492.05546534992	
    Loss 491.98040194616	
    Loss 492.14059384946	
    Loss 491.78606230258	
    Loss 492.09762601531	
    Loss 491.80615641166	
    Loss 490.89656340913	
    Loss 490.56076901868	
    Loss 490.67273515742	
    Loss 491.01040996533	
    Loss 490.81977389549	
    Loss 489.72820137317	
    Loss 489.85077173735	
    Loss 489.52702489179	
    Loss 489.85909762937	
    Loss 489.89385369083	
    Loss 489.74986353764	
    Loss 489.52662863678	
    Loss 489.01959772236	
    Loss 488.52361690864	
    Loss 488.88704635005	
    Loss 489.65790497666	
    Loss 489.49101143206	
    Loss 489.69173262472	
    Loss 489.1322034065	
    Loss 489.35923708335	
    Loss 489.63471682929	
    Loss 489.38204826245	
    Loss 488.70977171511	
    Loss 488.56843731457	
    Loss 488.16532384445	
    Loss 488.2267430574	
    Loss 488.51568020159	
    Loss 488.93750349669	
    Loss 488.60336135076	
    Loss 488.62199357424	
    Loss 487.73665372488	
    Loss 487.49080908978	
    Loss 488.59048072802	
    Loss 489.06182733286	
    Loss 488.30098046862	
    Loss 488.2062348899	
    Loss 487.75196618261	
    Loss 488.08824697041	
    Loss 488.05663929657	
    Loss 488.12003914015	
    Loss 488.16079160292	
    Loss 488.58150055306	
    Loss 488.01415564206	
    Loss 487.77453514239	
    Loss 489.3029929454	
    Loss 489.93108865611	
    Loss 489.37308375266	
    Loss 489.83719344033	
    Loss 489.61442782087	
    Loss 489.37592993302	
    Loss 488.69857586527	
    Loss 488.99222245411	
    Loss 487.96991099994	
    Loss 488.00190224973	
    Loss 488.49725162238	
    Loss 488.58247336697	
    Loss 488.38378249201	
    Loss 488.39724472694	
    Loss 488.24532636701	
Epoch 5	
 121638
      0
      0
      0
      0
      0
      0
      0
  10170
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091967103665938	
Grad norm	6.3402749473724	
    Loss 487.7506252955	
    Loss 487.78455988182	
    Loss 488.30982237074	
    Loss 488.37949168468	
    Loss 488.56805833258	
    Loss 489.26885362527	
    Loss 488.69396140958	
    Loss 487.85004746389	
    Loss 488.05985285747	
    Loss 488.48472185155	
    Loss 488.37825140053	
    Loss 488.9222059548	
    Loss 488.84470972945	
    Loss 488.13604752959	
    Loss 487.98738632697	
    Loss 488.27502318515	
    Loss 488.77517107547	
    Loss 488.73636463722	
    Loss 487.7858723961	
    Loss 488.03949855086	
    Loss 487.83819455438	
    Loss 488.2848014666	
    Loss 488.42606013619	
    Loss 488.38148601771	
    Loss 488.25050697027	
    Loss 487.82931115105	
    Loss 487.41388349419	
    Loss 487.85167945452	
    Loss 488.69339245601	
    Loss 488.59261418272	
    Loss 488.85380722665	
    Loss 488.3505192613	
    Loss 488.63150129326	
    Loss 488.95644771332	
    Loss 488.74985062046	
    Loss 488.12064827949	
    Loss 488.01890792949	
    Loss 487.65243762421	
    Loss 487.74837357042	
    Loss 488.07020349371	
    Loss 488.52214807658	
    Loss 488.21640758788	
    Loss 488.26152178956	
    Loss 487.40050193769	
    Loss 487.17743596532	
    Loss 488.29805627629	
    Loss 488.78901383237	
    Loss 488.04646819877	
    Loss 487.96893314123	
    Loss 487.53081368437	
    Loss 487.88203137067	
    Loss 487.86440068604	
    Loss 487.94078376885	
    Loss 487.99364490438	
    Loss 488.42607582039	
    Loss 487.86941613851	
    Loss 487.63948192054	
    Loss 489.17721046557	
    Loss 489.8139430952	
    Loss 489.26398821367	
    Loss 489.73565335703	
    Loss 489.51956748484	
    Loss 489.2875875921	
    Loss 488.61608927443	
    Loss 488.91530467337	
    Loss 487.89824869772	
    Loss 487.93509421687	
    Loss 488.43501287725	
    Loss 488.52438158769	
    Loss 488.32971370687	
    Loss 488.34698787965	
    Loss 488.19833174177	
Epoch 6	
 121642
      0
      0
      0
      0
      0
      0
      0
  10166
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091921582908473	
Grad norm	6.2139135779223	
    Loss 487.70460534594	
    Loss 487.74160993346	
    Loss 488.26988956057	
    Loss 488.34221155122	
    Loss 488.53332529545	
    Loss 489.23653053971	
    Loss 488.66382946505	
    Loss 487.82198718037	
    Loss 488.03368861426	
    Loss 488.46025356177	
    Loss 488.35547524349	
    Loss 488.90109689356	
    Loss 488.82494039244	
    Loss 488.11763747102	
    Loss 487.97023474901	
    Loss 488.25913901298	
    Loss 488.76036794661	
    Loss 488.72259266954	
    Loss 487.77302414434	
    Loss 488.02747857389	
    Loss 487.82696748102	
    Loss 488.27434672019	
    Loss 488.41630384999	
    Loss 488.37238885269	
    Loss 488.24199257455	
    Loss 487.82132123386	
    Loss 487.4064284536	
    Loss 487.8446679235	
    Loss 488.6869141283	
    Loss 488.58664092198	
    Loss 488.84820896929	
    Loss 488.34525705635	
    Loss 488.62667490111	
    Loss 488.95196072063	
    Loss 488.74567741616	
    Loss 488.11677839343	
    Loss 488.01527556511	
    Loss 487.6490052944	
    Loss 487.74515542584	
    Loss 488.0672411908	
    Loss 488.51938178889	
    Loss 488.2138485976	
    Loss 488.25915860543	
    Loss 487.39829132737	
    Loss 487.17537418852	
    Loss 488.29611098826	
    Loss 488.7871850821	
    Loss 488.04474678756	
    Loss 487.96732524213	
    Loss 487.52932112429	
    Loss 487.88063612368	
    Loss 487.86310001086	
    Loss 487.93956766359	
    Loss 487.99250922729	
    Loss 488.42504926862	
    Loss 487.86847204054	
    Loss 487.63859160766	
    Loss 489.1763892908	
    Loss 489.81318809216	
    Loss 489.26329272852	
    Loss 489.73501758076	
    Loss 489.51895903761	
    Loss 489.28702772724	
    Loss 488.61555739691	
    Loss 488.91480707441	
    Loss 487.8977885964	
    Loss 487.93466562311	
    Loss 488.43461662475	
    Loss 488.52400658969	
    Loss 488.32937085666	
    Loss 488.34667996976	
    Loss 488.19803320586	
Epoch 7	
 121644
      0
      0
      0
      0
      0
      0
      0
  10164
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091929169701384	
Grad norm	6.2100291757535	
    Loss 487.70431250878	
    Loss 487.7413330466	
    Loss 488.26964052674	
    Loss 488.34197501024	
    Loss 488.53310668018	
    Loss 489.23633161583	
    Loss 488.66364434359	
    Loss 487.82181687118	
    Loss 488.0335292909	
    Loss 488.4600987289	
    Loss 488.35533391611	
    Loss 488.90097529565	
    Loss 488.82481993638	
    Loss 488.11752649125	
    Loss 487.97013237295	
    Loss 488.25905193341	
    Loss 488.76028734265	
    Loss 488.72251954017	
    Loss 487.77295497862	
    Loss 488.02741075876	
    Loss 487.82690255607	
    Loss 488.27428721743	
    Loss 488.41624777811	
    Loss 488.37233641632	
    Loss 488.24194113376	
    Loss 487.82126890108	
    Loss 487.40637921773	
    Loss 487.84461733156	
    Loss 488.68687169139	
    Loss 488.58660696398	
    Loss 488.84817499935	
    Loss 488.34522185163	
    Loss 488.62664845163	
    Loss 488.951936983	
    Loss 488.74565605945	
    Loss 488.11676019096	
    Loss 488.01525665421	
    Loss 487.64898403282	
    Loss 487.74513411668	
    Loss 488.06722427969	
    Loss 488.51936564479	
    Loss 488.21383508804	
    Loss 488.25914785152	
    Loss 487.39828074666	
    Loss 487.17536415573	
    Loss 488.29609972622	
    Loss 488.78717340487	
    Loss 488.04473445234	
    Loss 487.96731345585	
    Loss 487.5293106871	
    Loss 487.88062608082	
    Loss 487.86309061139	
    Loss 487.93955859207	
    Loss 487.99250065934	
    Loss 488.42504387919	
    Loss 487.86846809404	
    Loss 487.63858708124	
    Loss 489.1763857239	
    Loss 489.81318568161	
    Loss 489.26329111024	
    Loss 489.73501703382	
    Loss 489.51895726991	
    Loss 489.28702666859	
    Loss 488.61555563381	
    Loss 488.91480526669	
    Loss 487.89778723444	
    Loss 487.93466441499	
    Loss 488.43461574843	
    Loss 488.52400532891	
    Loss 488.32937019735	
    Loss 488.34668028144	
    Loss 488.19803256256	
Epoch 8	
 121644
      0
      0
      0
      0
      0
      0
      0
  10164
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091929169701384	
Grad norm	6.2099946168644	
    Loss 487.7043118284	
    Loss 487.74133209752	
    Loss 488.26964040299	
    Loss 488.34197453744	
    Loss 488.53310639398	
    Loss 489.23633176294	
    Loss 488.6636445188	
    Loss 487.82181720006	
    Loss 488.0335295402	
    Loss 488.46009844454	
    Loss 488.35533391147	
    Loss 488.90097612351	
    Loss 488.8248201493	
    Loss 488.11752677545	
    Loss 487.97013273999	
    Loss 488.25905293894	
    Loss 488.76028833966	
    Loss 488.72252061282	
    Loss 487.77295588362	
    Loss 488.02741133865	
    Loss 487.8269029614	
    Loss 488.27428768088	
    Loss 488.41624817574	
    Loss 488.37233677262	
    Loss 488.24194126808	
    Loss 487.82126866283	
    Loss 487.40637896565	
    Loss 487.84461673644	
    Loss 488.68687151896	
    Loss 488.58660727398	
    Loss 488.8481751049	
    Loss 488.34522166139	
    Loss 488.62664878717	
    Loss 488.95193736318	
    Loss 488.74565647426	
    Loss 488.11676071765	
    Loss 488.01525699113	
    Loss 487.64898405953	
    Loss 487.74513401993	
    Loss 488.06722442063	
    Loss 488.51936574549	
    Loss 488.21383530297	
    Loss 488.25914820127	
    Loss 487.39828103173	
    Loss 487.17536440127	
    Loss 488.29609979865	
    Loss 488.78717338081	
    Loss 488.04473430598	
    Loss 487.96731329369	
    Loss 487.52931058228	
    Loss 487.88062595773	
    Loss 487.86309049039	
    Loss 487.93955845252	
    Loss 487.99250052289	
    Loss 488.42504396167	
    Loss 487.86846826108	
    Loss 487.63858716726	
    Loss 489.17638585323	
    Loss 489.81318587917	
    Loss 489.26329134257	
    Loss 489.73501732405	
    Loss 489.51895743916	
    Loss 489.28702687349	
    Loss 488.61555576351	
    Loss 488.91480537222	
    Loss 487.89778736033	
    Loss 487.93466454008	
    Loss 488.43461588381	
    Loss 488.52400542012	
    Loss 488.32937032172	
    Loss 488.34668047094	
    Loss 488.19803266289	
Epoch 9	
 121644
      0
      0
      0
      0
      0
      0
      0
  10164
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091929169701384	
Grad norm	6.2099928891856	
    Loss 487.70431192221	
    Loss 487.74133215969	
    Loss 488.26964052417	
    Loss 488.3419746216	
    Loss 488.53310648448	
    Loss 489.23633188207	
    Loss 488.66364463358	
    Loss 487.82181731881	
    Loss 488.03352964522	
    Loss 488.46009850021	
    Loss 488.35533398568	
    Loss 488.90097626094	
    Loss 488.82482023329	
    Loss 488.1175268592	
    Loss 487.97013282739	
    Loss 488.25905307308	
    Loss 488.76028847046	
    Loss 488.7225207449	
    Loss 487.77295599813	
    Loss 488.02741142451	
    Loss 487.82690303069	
    Loss 488.2742877525	
    Loss 488.41624824072	
    Loss 488.37233683196	
    Loss 488.24194130791	
    Loss 487.8212686699	
    Loss 487.4063789703	
    Loss 487.84461671299	
    Loss 488.68687152794	
    Loss 488.58660732168	
    Loss 488.84817513467	
    Loss 488.34522166561	
    Loss 488.62664883247	
    Loss 488.9519374103	
    Loss 488.74565652291	
    Loss 488.1167607744	
    Loss 488.01525703175	
    Loss 487.64898407459	
    Loss 487.74513402399	
    Loss 488.06722444253	
    Loss 488.51936576342	
    Loss 488.21383532929	
    Loss 488.25914823787	
    Loss 487.39828106282	
    Loss 487.17536442815	
    Loss 488.29609981094	
    Loss 488.78717338513	
    Loss 488.04473429955	
    Loss 487.96731328542	
    Loss 487.52931057842	
    Loss 487.88062595209	
    Loss 487.86309048438	
    Loss 487.9395584446	
    Loss 487.99250051514	
    Loss 488.42504397138	
    Loss 487.86846827767	
    Loss 487.63858717703	
    Loss 489.17638586608	
    Loss 489.81318589751	
    Loss 489.26329136342	
    Loss 489.73501734914	
    Loss 489.51895745434	
    Loss 489.28702689142	
    Loss 488.61555577534	
    Loss 488.91480538181	
    Loss 487.8977873716	
    Loss 487.93466455142	
    Loss 488.43461589579	
    Loss 488.52400542854	
    Loss 488.32937033256	
    Loss 488.34668048691	
    Loss 488.19803267151	
Epoch 10	
 121644
      0
      0
      0
      0
      0
      0
      0
  10164
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091929169701384	
Grad norm	6.2099927473971	
    Loss 487.70431193028	
    Loss 487.7413321652	
    Loss 488.26964053453	
    Loss 488.34197462896	
    Loss 488.53310649225	
    Loss 489.23633189222	
    Loss 488.66364464337	
    Loss 487.82181732874	
    Loss 488.03352965394	
    Loss 488.4600985049	
    Loss 488.35533399195	
    Loss 488.90097627237	
    Loss 488.82482024046	
    Loss 488.11752686622	
    Loss 487.97013283477	
    Loss 488.25905308414	
    Loss 488.7602884813	
    Loss 488.72252075572	
    Loss 487.77295600745	
    Loss 488.02741143154	
    Loss 487.82690303639	
    Loss 488.27428775838	
    Loss 488.41624824613	
    Loss 488.3723368369	
    Loss 488.24194131129	
    Loss 487.82126867057	
    Loss 487.4063789708	
    Loss 487.84461671131	
    Loss 488.68687152886	
    Loss 488.58660732581	
    Loss 488.84817513733	
    Loss 488.34522166618	
    Loss 488.62664883633	
    Loss 488.95193741425	
    Loss 488.74565652697	
    Loss 488.1167607791	
    Loss 488.01525703515	
    Loss 487.64898407595	
    Loss 487.74513402444	
    Loss 488.06722444437	
    Loss 488.51936576494	
    Loss 488.21383533146	
    Loss 488.25914824087	
    Loss 487.39828106539	
    Loss 487.17536443034	
    Loss 488.29609981194	
    Loss 488.78717338551	
    Loss 488.04473429904	
    Loss 487.96731328475	
    Loss 487.5293105781	
    Loss 487.88062595163	
    Loss 487.86309048387	
    Loss 487.93955844393	
    Loss 487.99250051449	
    Loss 488.42504397215	
    Loss 487.86846827902	
    Loss 487.63858717781	
    Loss 489.1763858671	
    Loss 489.81318589897	
    Loss 489.26329136508	
    Loss 489.73501735113	
    Loss 489.51895745553	
    Loss 489.28702689283	
    Loss 488.61555577625	
    Loss 488.91480538253	
    Loss 487.89778737248	
    Loss 487.93466455232	
    Loss 488.43461589673	
    Loss 488.5240054292	
    Loss 488.32937033341	
    Loss 488.34668048816	
    Loss 488.19803267216	
Epoch 11	
 121644
      0
      0
      0
      0
      0
      0
      0
  10164
[torch.DoubleTensor of size 9]

Validation accuracy:	0.091929169701384	
Grad norm	6.2099927357606	
    Loss 487.70431193089	
    Loss 487.74133216561	
    Loss 488.26964053534	
    Loss 488.34197462954	
    Loss 488.53310649285	
    Loss 489.23633189302	
    Loss 488.66364464414	
    Loss 487.82181732952	
    Loss 488.03352965461	
    Loss 488.46009850525	
    Loss 488.35533399244	
    Loss 488.90097627328	
    Loss 488.82482024104	
    Loss 488.11752686677	
    Loss 487.97013283536	
    Loss 488.25905308503	
    Loss 488.76028848217	
    Loss 488.72252075658	
    Loss 487.7729560082	
    Loss 488.02741143209	
    Loss 487.82690303684	
    Loss 488.27428775884	
    Loss 488.41624824656	
    Loss 488.3723368373	
    Loss 488.24194131156	
    Loss 487.82126867062	
    Loss 487.40637897083	
    Loss 487.84461671116	
    Loss 488.68687152893	
    Loss 488.58660732614	
    Loss 488.84817513755	
    Loss 488.34522166623	
    Loss 488.62664883664	
    Loss 488.95193741458	
    Loss 488.7456565273	
    Loss 488.11676077948	
    Loss 488.01525703543	
    Loss 487.64898407606	
    Loss 487.74513402448	
