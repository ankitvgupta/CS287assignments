[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	1	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2574
 2545
 1375
 4063
 3553
 1815
 3507
  740
 3827
 3758
 1826
 9263
 1137
 1397
 1681
 3948
 3205
  900
 7462
 3616
  733
 3493
  529
  889
 1707
 3335
  861
 2341
 2053
 4447
 1718
 5254
 4368
 1275
 2356
 2285
 2765
 6310
 3733
 4950
 3022
 3878
 1011
 3864
 2439
[torch.DoubleTensor of size 45]

Validation accuracy:	0.024437059966011	
Grad norm	0	
    Loss 22775275.766731	
    Loss 17199836.845943	
    Loss 12989324.029009	
    Loss 9809587.8704074	
    Loss 7408278.9940832	
    Loss 5594814.1354482	
    Loss 4225319.4455997	
    Loss 3191085.955014	
    Loss 2410031.6907472	
    Loss 1820180.7614891	
    Loss 1374721.0241088	
    Loss 1038318.2636852	
    Loss 784264.70743455	
    Loss 592403.28029646	
    Loss 447509.6660649	
    Loss 338083.85113681	
    Loss 255450.25245894	
    Loss 193042.69634693	
    Loss 145910.10667326	
    Loss 110320.03052541	
    Loss 83438.513382507	
    Loss 63139.368021592	
    Loss 47808.3673031	
    Loss 36229.511017476	
    Loss 27486.392251245	
    Loss 20880.705581808	
    Loss 15891.649709627	
    Loss 12129.387146189	
    Loss 9287.7316611957	
    Loss 7138.4726327569	
    Loss 5516.0172414604	
    Loss 4289.2045792812	
    Loss 3366.7765816471	
    Loss 2667.3654084526	
    Loss 2138.8014302695	
    Loss 1738.6190145074	
    Loss 1437.7010236119	
    Loss 1209.6232413979	
    Loss 1038.3082007028	
    Loss 910.63348747709	
    Loss 812.92777066828	
    Loss 736.8090253103	
    Loss 682.50650375117	
    Loss 638.19740898291	
    Loss 606.41057809037	
    Loss 586.18755396141	
    Loss 567.66674064109	
    Loss 550.50697931304	
    Loss 540.61076405423	
    Loss 532.52314790384	
    Loss 527.40072343563	
    Loss 523.21708311745	
    Loss 520.0445399965	
    Loss 518.94318780734	
    Loss 517.97709497041	
    Loss 513.97081666156	
    Loss 512.12123832157	
    Loss 515.74802477158	
    Loss 515.47693453411	
    Loss 513.54515029644	
    Loss 515.01554608942	
    Loss 512.63462973363	
    Loss 512.33120154522	
    Loss 510.53428446141	
    Loss 511.48544018228	
    Loss 509.18297498929	
    Loss 508.92874591601	
    Loss 510.12283322328	
    Loss 510.23996293126	
    Loss 509.62779042374	
    Loss 509.10565802774	
    Loss 509.39580782688	
Epoch 2	
 104023
      1
      0
      0
      0
      0
      0
      0
  27784
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071778647730032	
Grad norm	6.7207451061842	
    Loss 507.47399324541	
    Loss 509.1097199243	
    Loss 510.42545164596	
    Loss 510.36324867978	
    Loss 510.75030880985	
    Loss 512.27601438917	
    Loss 509.99480274898	
    Loss 507.74413158482	
    Loss 509.6156604278	
    Loss 512.36602201237	
    Loss 510.22030795216	
    Loss 511.76610592789	
    Loss 511.2593753757	
    Loss 509.08913528601	
    Loss 509.34165234375	
    Loss 510.04805200036	
    Loss 511.0136306788	
    Loss 510.51344043409	
    Loss 508.20856525736	
    Loss 511.50501723235	
    Loss 510.0255329694	
    Loss 511.20468745234	
    Loss 511.72929847249	
    Loss 511.17830494074	
    Loss 511.21686174896	
    Loss 508.84542293755	
    Loss 506.1620758647	
    Loss 509.8795493845	
    Loss 512.56071492922	
    Loss 511.60124051473	
    Loss 511.39021515835	
    Loss 509.73915311707	
    Loss 512.09003326057	
    Loss 511.38087750231	
    Loss 510.63610502093	
    Loss 509.07834972521	
    Loss 509.14578585243	
    Loss 508.34978337757	
    Loss 508.66699567683	
    Loss 510.65275012091	
    Loss 510.83607912709	
    Loss 508.65976051146	
    Loss 510.19096411193	
    Loss 508.0561401625	
    Loss 508.12920795591	
    Loss 511.98741430355	
    Loss 511.62842424562	
    Loss 508.19754596164	
    Loss 508.65664618913	
    Loss 508.39244653347	
    Loss 509.17518498409	
    Loss 509.45458831096	
    Loss 509.64903395616	
    Loss 511.0956660308	
    Loss 512.05211676887	
    Loss 509.49911171141	
    Loss 508.74673129582	
    Loss 513.19338949955	
    Loss 513.54580662526	
    Loss 512.08599360641	
    Loss 513.91141615734	
    Loss 511.80049382781	
    Loss 511.7040206444	
    Loss 510.06212356765	
    Loss 511.13088485134	
    Loss 508.91464163203	
    Loss 508.72600183535	
    Loss 509.96888897713	
    Loss 510.12554111713	
    Loss 509.54191538067	
    Loss 509.0415851788	
    Loss 509.3481386305	
Epoch 3	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682606225705	
    Loss 507.43020901858	
    Loss 509.07612618447	
    Loss 510.40014627645	
    Loss 510.34409969922	
    Loss 510.73605857998	
    Loss 512.26547452791	
    Loss 509.98666633428	
    Loss 507.73802107391	
    Loss 509.61102205894	
    Loss 512.3625230499	
    Loss 510.21768655416	
    Loss 511.7641165274	
    Loss 511.257895917	
    Loss 509.08801083727	
    Loss 509.34081153495	
    Loss 510.04751504631	
    Loss 511.01314677322	
    Loss 510.51303936183	
    Loss 508.20823357575	
    Loss 511.50476627624	
    Loss 510.02533296088	
    Loss 511.20454616405	
    Loss 511.7292049735	
    Loss 511.17824228334	
    Loss 511.21680745624	
    Loss 508.84539207427	
    Loss 506.16203391549	
    Loss 509.87950958473	
    Loss 512.56068786189	
    Loss 511.60122853577	
    Loss 511.39020659694	
    Loss 509.73915059474	
    Loss 512.09001644441	
    Loss 511.38086174706	
    Loss 510.63609452875	
    Loss 509.07834425362	
    Loss 509.14578246821	
    Loss 508.34978054239	
    Loss 508.6669914317	
    Loss 510.65274655922	
    Loss 510.83607548087	
    Loss 508.65975791949	
    Loss 510.19096131355	
    Loss 508.05613775223	
    Loss 508.12920617947	
    Loss 511.98741381498	
    Loss 511.62842386787	
    Loss 508.19754613104	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854241	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698555	
    Loss 509.49911201037	
    Loss 508.74673165925	
    Loss 513.19338952632	
    Loss 513.54580654403	
    Loss 512.08599351489	
    Loss 513.91141600133	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527925	
    Loss 509.34813873096	
Epoch 4	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073859	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295948	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 5	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 6	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 7	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 8	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655867	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 9	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425351	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 10	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
    Loss 511.38086174667	
    Loss 510.6360945285	
    Loss 509.07834425352	
    Loss 509.14578246817	
    Loss 508.34978054235	
    Loss 508.66699143158	
    Loss 510.6527465591	
    Loss 510.83607548073	
    Loss 508.6597579194	
    Loss 510.19096131345	
    Loss 508.05613775214	
    Loss 508.1292061794	
    Loss 511.98741381497	
    Loss 511.62842386786	
    Loss 508.19754613106	
    Loss 508.65664624382	
    Loss 508.3924466725	
    Loss 509.17518500923	
    Loss 509.45458854242	
    Loss 509.64903401493	
    Loss 511.09566625137	
    Loss 512.05211698556	
    Loss 509.49911201039	
    Loss 508.74673165927	
    Loss 513.19338952632	
    Loss 513.54580654402	
    Loss 512.08599351489	
    Loss 513.91141600132	
    Loss 511.80049370404	
    Loss 511.70402067636	
    Loss 510.06212365825	
    Loss 511.13088499844	
    Loss 508.9146417291	
    Loss 508.7260019104	
    Loss 509.96888898868	
    Loss 510.12554119729	
    Loss 509.54191546568	
    Loss 509.04158527926	
    Loss 509.34813873097	
Epoch 11	
 104035
      1
      0
      0
      0
      0
      0
      0
  27772
[torch.DoubleTensor of size 9]

Validation accuracy:	0.071854515659141	
Grad norm	6.5682603073847	
    Loss 507.43020911077	
    Loss 509.07612623506	
    Loss 510.40014631769	
    Loss 510.34409972506	
    Loss 510.736058608	
    Loss 512.26547455621	
    Loss 509.98666634814	
    Loss 507.73802108703	
    Loss 509.61102206674	
    Loss 512.36252305505	
    Loss 510.21768655868	
    Loss 511.76411653061	
    Loss 511.25789592034	
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	
