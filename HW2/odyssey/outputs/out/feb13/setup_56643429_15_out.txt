[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.1	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
  773
 5093
 1263
 3649
 2443
 2475
 4256
 3120
 5917
 1313
 5161
 2154
 1511
 2621
 1170
 1395
 2390
 1620
 2950
 1199
 4778
 1058
 3420
 1881
 4585
  802
 2444
 3901
 3137
 3622
 3041
 3544
 3203
 2861
 2103
 2505
 3925
 2675
 3463
 2483
 3028
 7866
 2013
 3008
 3989
[torch.DoubleTensor of size 45]

Validation accuracy:	0.023746661811119	
Grad norm	0	
    Loss 2278238.0053148	
    Loss 2271832.3315814	
    Loss 2265448.8382918	
    Loss 2259083.9012281	
    Loss 2252738.1487573	
    Loss 2246410.0352554	
    Loss 2240099.0628926	
    Loss 2233806.7098504	
    Loss 2227532.2260468	
    Loss 2221274.7855736	
    Loss 2215035.2343194	
    Loss 2208813.0625884	
    Loss 2202608.8627481	
    Loss 2196422.3511109	
    Loss 2190252.8613917	
    Loss 2184101.3715461	
    Loss 2177966.008492	
    Loss 2171848.7300866	
    Loss 2165748.5699896	
    Loss 2159665.6799961	
    Loss 2153599.5159144	
    Loss 2147550.4504865	
    Loss 2141518.6146708	
    Loss 2135503.4320861	
    Loss 2129506.0375918	
    Loss 2123525.2017779	
    Loss 2117560.7682534	
    Loss 2111613.6512037	
    Loss 2105683.3384455	
    Loss 2099769.8694285	
    Loss 2093872.6013065	
    Loss 2087991.7098872	
    Loss 2082128.1282995	
    Loss 2076280.519328	
    Loss 2070449.3929528	
    Loss 2064634.7561251	
    Loss 2058836.7478978	
    Loss 2053054.8763169	
    Loss 2047289.0530807	
    Loss 2041539.883204	
    Loss 2035806.921634	
    Loss 2030089.7240557	
    Loss 2024389.1702497	
    Loss 2018704.4331867	
    Loss 2013035.5646964	
    Loss 2007382.6076093	
    Loss 2001745.5189539	
    Loss 1996124.8839165	
    Loss 1990519.2767612	
    Loss 1984930.0730233	
    Loss 1979356.0749839	
    Loss 1973797.9684741	
    Loss 1968255.6805707	
    Loss 1962728.633865	
    Loss 1957217.3426252	
    Loss 1951721.210804	
    Loss 1946240.797165	
    Loss 1940775.6958334	
    Loss 1935326.1791336	
    Loss 1929891.9658244	
    Loss 1924472.7530905	
    Loss 1919069.320456	
    Loss 1913680.8727313	
    Loss 1908307.9979261	
    Loss 1902949.3299296	
    Loss 1897606.2435696	
    Loss 1892278.990339	
    Loss 1886965.5707948	
    Loss 1881666.9828929	
    Loss 1876383.452762	
    Loss 1871114.9276727	
    Loss 1865861.1700653	
Epoch 2	
 25036
 12392
  2473
 14407
 12295
   122
   487
 10517
 24483
 15397
  3369
  2236
   609
    99
  1276
  2291
  1411
  1153
  1248
   110
   132
    85
   162
     3
     1
     0
     1
     0
     1
     2
     0
     0
     0
     4
     0
     1
     0
     3
     0
     0
     0
     0
     1
     1
[torch.DoubleTensor of size 44]

Validation accuracy:	0.082240835154164	
Grad norm	466.30937282106	
    Loss 1864288.1235925	
    Loss 1859053.6702998	
    Loss 1853834.0386479	
    Loss 1848628.4055881	
    Loss 1843438.3854065	
    Loss 1838262.5235659	
    Loss 1833101.0144019	
    Loss 1827954.4416205	
    Loss 1822822.3030917	
    Loss 1817704.2844998	
    Loss 1812600.5982922	
    Loss 1807511.0762462	
    Loss 1802436.2607529	
    Loss 1797376.0638463	
    Loss 1792329.624588	
    Loss 1787297.8307407	
    Loss 1782279.1505327	
    Loss 1777275.0759195	
    Loss 1772285.1189605	
    Loss 1767309.178252	
    Loss 1762347.1265094	
    Loss 1757398.7507071	
    Loss 1752464.5607273	
    Loss 1747543.90074	
    Loss 1742637.8405603	
    Loss 1737745.3852601	
    Loss 1732866.2236685	
    Loss 1728001.0856708	
    Loss 1723149.5989624	
    Loss 1718311.9900459	
    Loss 1713487.6172662	
    Loss 1708676.6676423	
    Loss 1703879.7561187	
    Loss 1699095.9698449	
    Loss 1694325.4949147	
    Loss 1689568.6690348	
    Loss 1684825.3371495	
    Loss 1680095.2881426	
    Loss 1675378.2305073	
    Loss 1670674.7588923	
    Loss 1665984.4713472	
    Loss 1661307.1849389	
    Loss 1656643.5674324	
    Loss 1651992.8841541	
    Loss 1647354.975976	
    Loss 1642730.0171951	
    Loss 1638118.1408996	
    Loss 1633519.7145957	
    Loss 1628933.5233558	
    Loss 1624360.7838486	
    Loss 1619800.3675925	
    Loss 1615252.9897602	
    Loss 1610718.5068123	
    Loss 1606196.5432166	
    Loss 1601687.4308976	
    Loss 1597190.7101158	
    Loss 1592706.8517449	
    Loss 1588235.3641363	
    Loss 1583776.6967609	
    Loss 1579330.6394533	
    Loss 1574896.7244926	
    Loss 1570475.802419	
    Loss 1566067.1158097	
    Loss 1561671.119775	
    Loss 1557286.7100191	
    Loss 1552915.162495	
    Loss 1548556.5518963	
    Loss 1544209.1077201	
    Loss 1539873.8755608	
    Loss 1535550.8859723	
    Loss 1531240.1787974	
    Loss 1526941.5068702	
Epoch 3	
 28034
 11875
  1713
 14841
 12263
    64
   320
 10064
 26233
 15590
  2753
  1640
   439
    73
   963
  1848
  1067
   805
   905
    72
    87
    54
   103
     0
     0
     0
     0
     0
     0
     1
     0
     0
     0
     0
     0
     0
     0
     1
[torch.DoubleTensor of size 38]

Validation accuracy:	0.08431202961884	
Grad norm	422.27365611646	
    Loss 1525654.4875699	
    Loss 1521371.6776195	
    Loss 1517100.9190712	
    Loss 1512841.6497049	
    Loss 1508595.0862375	
    Loss 1504360.1581231	
    Loss 1500137.0158992	
    Loss 1495926.0660884	
    Loss 1491726.8151856	
    Loss 1487539.2136055	
    Loss 1483363.2524963	
    Loss 1479198.9052733	
    Loss 1475046.5765344	
    Loss 1470906.2359501	
    Loss 1466777.1428116	
    Loss 1462659.9827684	
    Loss 1458553.5775353	
    Loss 1454459.0344703	
    Loss 1450376.0943764	
    Loss 1446304.5782816	
    Loss 1442244.4968991	
    Loss 1438195.5200799	
    Loss 1434158.2313985	
    Loss 1430131.9455017	
    Loss 1426117.6474384	
    Loss 1422114.4962559	
    Loss 1418122.1820025	
    Loss 1414141.310035	
    Loss 1410171.5612117	
    Loss 1406213.2673623	
    Loss 1402265.7404695	
    Loss 1398329.2101118	
    Loss 1394404.1517898	
    Loss 1390489.8314964	
    Loss 1386586.3465696	
    Loss 1382694.0973599	
    Loss 1378812.8649216	
    Loss 1374942.5173168	
    Loss 1371082.7715172	
    Loss 1367234.1386728	
    Loss 1363396.2259884	
    Loss 1359569.0251659	
    Loss 1355753.0119813	
    Loss 1351947.6083287	
    Loss 1348152.5759219	
    Loss 1344368.1124979	
    Loss 1340594.4200061	
    Loss 1336831.6970059	
    Loss 1333078.9959695	
    Loss 1329337.2904198	
    Loss 1325605.6505211	
    Loss 1321884.6834234	
    Loss 1318174.2442481	
    Loss 1314474.0783508	
    Loss 1310784.4092931	
    Loss 1307104.8816777	
    Loss 1303435.8753141	
    Loss 1299776.955149	
    Loss 1296128.5570166	
    Loss 1292490.4745452	
    Loss 1288862.2946551	
    Loss 1285244.8008538	
    Loss 1281637.298372	
    Loss 1278040.1501052	
    Loss 1274452.4653999	
    Loss 1270875.3563593	
    Loss 1267308.8475127	
    Loss 1263751.3849781	
    Loss 1260203.9668432	
    Loss 1256666.5310836	
    Loss 1253139.1472892	
    Loss 1249621.5821562	
Epoch 4	
 31197
 11821
  1243
 14583
 11830
    43
   212
  9617
 27786
 15468
  2191
  1185
   299
    31
   715
  1475
   740
   546
   668
    40
    45
    24
    49
[torch.DoubleTensor of size 23]

Validation accuracy:	0.086125273124545	
Grad norm	382.46938666001	
    Loss 1248568.4629235	
    Loss 1245063.9264484	
    Loss 1241569.2072332	
    Loss 1238083.9035895	
    Loss 1234608.9734528	
    Loss 1231143.6043485	
    Loss 1227687.8748679	
    Loss 1224242.1298506	
    Loss 1220805.8884802	
    Loss 1217379.2368809	
    Loss 1213962.0742516	
    Loss 1210554.4215287	
    Loss 1207156.6119128	
    Loss 1203768.6102844	
    Loss 1200389.8253802	
    Loss 1197020.7670231	
    Loss 1193660.5240664	
    Loss 1190309.9541511	
    Loss 1186968.8940336	
    Loss 1183637.1659578	
    Loss 1180314.8321045	
    Loss 1177001.5321665	
    Loss 1173697.8489799	
    Loss 1170403.1366961	
    Loss 1167118.2662035	
    Loss 1163842.5168909	
    Loss 1160575.6072507	
    Loss 1157318.0583405	
    Loss 1154069.5759159	
    Loss 1150830.5455865	
    Loss 1147600.2654283	
    Loss 1144378.9969656	
    Loss 1141167.1103032	
    Loss 1137964.0091761	
    Loss 1134769.7427661	
    Loss 1131584.7082228	
    Loss 1128408.6839863	
    Loss 1125241.5651967	
    Loss 1122083.1139973	
    Loss 1118933.7622454	
    Loss 1115793.1313923	
    Loss 1112661.3116094	
    Loss 1109538.6569277	
    Loss 1106424.6973439	
    Loss 1103319.1801523	
    Loss 1100222.3026015	
    Loss 1097134.2743822	
    Loss 1094055.1992409	
    Loss 1090984.3381504	
    Loss 1087922.4588541	
    Loss 1084868.8195272	
    Loss 1081823.9082559	
    Loss 1078787.5999817	
    Loss 1075759.7168418	
    Loss 1072740.4099913	
    Loss 1069729.4016526	
    Loss 1066727.0050514	
    Loss 1063732.855377	
    Loss 1060747.3352782	
    Loss 1057770.2399587	
    Loss 1054801.2333054	
    Loss 1051841.0081539	
    Loss 1048888.9384241	
    Loss 1045945.3316301	
    Loss 1043009.4518447	
    Loss 1040082.2533309	
    Loss 1037163.7549185	
    Loss 1034252.594173	
    Loss 1031349.6857163	
    Loss 1028454.9263442	
    Loss 1025568.3932059	
    Loss 1022689.8778135	
Epoch 5	
 35046
 11350
   914
 13889
 11187
    21
   136
  9097
 29900
 14597
  1722
   850
   177
    16
   479
  1051
   485
   367
   444
    20
    28
     5
    27
[torch.DoubleTensor of size 23]

Validation accuracy:	0.087764020393299	
Grad norm	346.47436794301	
    Loss 1021828.1067569	
    Loss 1018960.2860624	
    Loss 1016100.4724074	
    Loss 1013248.3633827	
    Loss 1010404.7437079	
    Loss 1007568.9767525	
    Loss 1004741.0781894	
    Loss 1001921.3598138	
    Loss 999109.3761292	
    Loss 996305.27923263	
    Loss 993508.92713121	
    Loss 990720.35699693	
    Loss 987939.85509155	
    Loss 985167.37134452	
    Loss 982402.44416854	
    Loss 979645.44890516	
    Loss 976895.67477022	
    Loss 974153.80185712	
    Loss 971419.71324552	
    Loss 968693.25669948	
    Loss 965974.510297	
    Loss 963263.11777448	
    Loss 960559.63134441	
    Loss 957863.47017507	
    Loss 955175.38751273	
    Loss 952494.76406766	
    Loss 949821.35208237	
    Loss 947155.60704117	
    Loss 944497.25395625	
    Loss 941846.69869869	
    Loss 939203.24860813	
    Loss 936567.18413891	
    Loss 933938.79668085	
    Loss 931317.59681579	
    Loss 928703.60597361	
    Loss 926097.1937665	
    Loss 923498.15335693	
    Loss 920906.39912659	
    Loss 918321.7333982	
    Loss 915744.52305347	
    Loss 913174.4095091	
    Loss 910611.53707038	
    Loss 908056.17795591	
    Loss 905507.93672893	
    Loss 902966.57863694	
    Loss 900432.29071738	
    Loss 897905.26094971	
    Loss 895385.54420899	
    Loss 892872.55894	
    Loss 890366.9088389	
    Loss 887868.0125538	
    Loss 885376.24922014	
    Loss 882891.52086073	
    Loss 880413.69856065	
    Loss 877942.88539587	
    Loss 875478.85861222	
    Loss 873021.88556246	
    Loss 870571.66473803	
    Loss 868128.51870484	
    Loss 865692.24786364	
    Loss 863262.58741458	
    Loss 860840.14051983	
    Loss 858424.34782043	
    Loss 856015.4806505	
    Loss 853612.92197859	
    Loss 851217.48107256	
    Loss 848829.18713498	
    Loss 846446.84769334	
    Loss 844071.28482252	
    Loss 841702.378673	
    Loss 839340.20422461	
    Loss 836984.5818577	
Epoch 6	
 37506
 10936
   619
 13590
 10619
     7
    88
  8549
 32015
 14028
  1307
   583
   101
     8
   301
   728
   280
   219
   282
    12
    14
     1
    15
[torch.DoubleTensor of size 23]

Validation accuracy:	0.088955146880311	
Grad norm	313.91956106742	
    Loss 836279.36927115	
    Loss 833932.52267427	
    Loss 831592.21134013	
    Loss 829258.19659874	
    Loss 826931.14030757	
    Loss 824610.53186911	
    Loss 822296.33804783	
    Loss 819988.84366967	
    Loss 817687.6544181	
    Loss 815392.94914927	
    Loss 813104.56704069	
    Loss 810822.55165542	
    Loss 808547.15206945	
    Loss 806278.30444334	
    Loss 804015.65315204	
    Loss 801759.47280219	
    Loss 799509.20593955	
    Loss 797265.39713992	
    Loss 795027.95671111	
    Loss 792796.76195046	
    Loss 790571.89143348	
    Loss 788353.00891027	
    Loss 786140.62377772	
    Loss 783934.22393765	
    Loss 781734.45535273	
    Loss 779540.78415242	
    Loss 777352.99706964	
    Loss 775171.49363042	
    Loss 772996.0176652	
    Loss 770826.97556886	
    Loss 768663.69718775	
    Loss 766506.47165369	
    Loss 764355.5322166	
    Loss 762210.47142845	
    Loss 760071.29656006	
    Loss 757938.33985366	
    Loss 755811.41366529	
    Loss 753690.45107981	
    Loss 751575.28572545	
    Loss 749466.23155419	
    Loss 747362.95321196	
    Loss 745265.62311813	
    Loss 743174.45496025	
    Loss 741089.10732346	
    Loss 739009.37783297	
    Loss 736935.43894854	
    Loss 734867.44571027	
    Loss 732805.42864335	
    Loss 730748.92784656	
    Loss 728698.41602806	
    Loss 726653.44513305	
    Loss 724614.3006535	
    Loss 722580.91235655	
    Loss 720553.18178775	
    Loss 718531.18159613	
    Loss 716514.72744118	
    Loss 714504.05416179	
    Loss 712498.91448106	
    Loss 710499.57360198	
    Loss 708505.83970752	
    Loss 706517.50991092	
    Loss 704535.10440215	
    Loss 702558.12648971	
    Loss 700586.82122686	
    Loss 698620.66609903	
    Loss 696660.34117141	
    Loss 694705.891192	
    Loss 692756.27332304	
    Loss 690812.21961297	
    Loss 688873.60577099	
    Loss 686940.50001089	
    Loss 685012.75200432	
Epoch 7	
 40659
 10349
   407
 12954
 10104
     3
    38
  7941
 33373
 13470
   928
   379
    57
     4
   172
   508
   155
   132
   163
     6
     5
     0
     1
[torch.DoubleTensor of size 23]

Validation accuracy:	0.091845714979364	
Grad norm	284.47320938944	
    Loss 684435.64209992	
    Loss 682515.09422625	
    Loss 680599.88274388	
    Loss 678689.81134042	
    Loss 676785.45046659	
    Loss 674886.38262608	
    Loss 672992.53950421	
    Loss 671104.1803601	
    Loss 669220.9676901	
    Loss 667343.0852232	
    Loss 665470.3652356	
    Loss 663602.85379353	
    Loss 661740.76846546	
    Loss 659884.03498922	
    Loss 658032.38298774	
    Loss 656186.01153293	
    Loss 654344.48206127	
    Loss 652508.23232908	
    Loss 650677.19130621	
    Loss 648851.26329683	
    Loss 647030.52075008	
    Loss 645214.65447865	
    Loss 643404.12750601	
    Loss 641598.49166493	
    Loss 639798.30033228	
    Loss 638003.0908831	
    Loss 636212.68341933	
    Loss 634427.42810736	
    Loss 632647.0876473	
    Loss 630872.0575814	
    Loss 629101.69789691	
    Loss 627336.29898152	
    Loss 625576.05018285	
    Loss 623820.60792623	
    Loss 622069.97361478	
    Loss 620324.43933768	
    Loss 618583.83719521	
    Loss 616848.11783936	
    Loss 615117.13972693	
    Loss 613391.17283638	
    Loss 611669.90660014	
    Loss 609953.52563072	
    Loss 608242.20000059	
    Loss 606535.62939267	
    Loss 604833.64833185	
    Loss 603136.41307364	
    Loss 601444.04387126	
    Loss 599756.55934046	
    Loss 598073.59566017	
    Loss 596395.52039724	
    Loss 594721.99462753	
    Loss 593053.2261549	
    Loss 591389.1700012	
    Loss 589729.74604498	
    Loss 588075.00897098	
    Loss 586424.80112758	
    Loss 584779.33316937	
    Loss 583138.40373628	
    Loss 581502.22637006	
    Loss 579870.61888985	
    Loss 578243.43068882	
    Loss 576621.1072351	
    Loss 575003.20979231	
    Loss 573389.96242133	
    Loss 571780.91901877	
    Loss 570176.64734695	
    Loss 568577.20781463	
    Loss 566981.68851248	
    Loss 565390.73836762	
    Loss 563804.23492514	
    Loss 562222.23758593	
    Loss 560644.62390266	
Epoch 8	
 43569
  9694
   255
 12484
  9362
     2
    14
  7265
 34849
 12730
   644
   230
    22
     1
    97
   323
    87
    78
    97
     2
     3
[torch.DoubleTensor of size 21]

Validation accuracy:	0.093795520757465	
Grad norm	257.83787169209	
    Loss 560172.33754592	
    Loss 558600.63111272	
    Loss 557033.28356217	
    Loss 555470.12746849	
    Loss 553911.6627821	
    Loss 552357.54189551	
    Loss 550807.67332905	
    Loss 549262.2918443	
    Loss 547721.11544743	
    Loss 546184.32095324	
    Loss 544651.74085778	
    Loss 543123.4216566	
    Loss 541599.55381419	
    Loss 540080.05589175	
    Loss 538564.72576443	
    Loss 537053.70559299	
    Loss 535546.64983158	
    Loss 534043.91059538	
    Loss 532545.43119142	
    Loss 531051.13853387	
    Loss 529561.09663636	
    Loss 528075.02537899	
    Loss 526593.34233709	
    Loss 525115.65566088	
    Loss 523642.44089628	
    Loss 522173.29446843	
    Loss 520708.06761346	
    Loss 519247.06670561	
    Loss 517790.07294273	
    Loss 516337.46455782	
    Loss 514888.63611565	
    Loss 513443.87383728	
    Loss 512003.33299786	
    Loss 510566.72093306	
    Loss 509134.03775826	
    Loss 507705.53633142	
    Loss 506281.06777425	
    Loss 504860.59804179	
    Loss 503444.00610313	
    Loss 502031.52474332	
    Loss 500622.86860292	
    Loss 499218.22453469	
    Loss 497817.72925154	
    Loss 496421.11493664	
    Loss 495028.25256436	
    Loss 493639.28196975	
    Loss 492254.29113253	
    Loss 490873.29286919	
    Loss 489496.00030441	
    Loss 488122.69646084	
    Loss 486753.13085994	
    Loss 485387.44833765	
    Loss 484025.62538047	
    Loss 482667.5924996	
    Loss 481313.39482055	
    Loss 479962.89331094	
    Loss 478616.27968136	
    Loss 477273.39131547	
    Loss 475934.39651881	
    Loss 474599.12400014	
    Loss 473267.46649802	
    Loss 471939.80328019	
    Loss 470615.74858126	
    Loss 469295.50839728	
    Loss 467978.69918578	
    Loss 466665.79275386	
    Loss 465356.86295097	
    Loss 464051.11211944	
    Loss 462749.11429759	
    Loss 461450.75202622	
    Loss 460156.07531984	
    Loss 458864.98725452	
Epoch 9	
 46324
  8961
   153
 11861
  8718
     0
     6
  6471
 36379
 11979
   452
   133
     7
     0
    53
   173
    49
    39
    49
     0
     1
[torch.DoubleTensor of size 21]

Validation accuracy:	0.096117079388201	
Grad norm	233.74413974612	
    Loss 458478.47837176	
    Loss 457192.23703396	
    Loss 455909.55712712	
    Loss 454630.29141672	
    Loss 453354.88381661	
    Loss 452083.03982661	
    Loss 450814.65422666	
    Loss 449549.93878444	
    Loss 448288.66214943	
    Loss 447030.98830433	
    Loss 445776.75432267	
    Loss 444526.00583576	
    Loss 443278.90997265	
    Loss 442035.38076338	
    Loss 440795.27063236	
    Loss 439558.67869552	
    Loss 438325.33269075	
    Loss 437095.51526027	
    Loss 435869.18167014	
    Loss 434646.27689545	
    Loss 433426.85631466	
    Loss 432210.66813737	
    Loss 430998.08758658	
    Loss 429788.77115201	
    Loss 428583.12969262	
    Loss 427380.80858333	
    Loss 426181.68730766	
    Loss 424986.03350456	
    Loss 423793.64606311	
    Loss 422604.8824493	
    Loss 421419.1741711	
    Loss 420236.79931667	
    Loss 419057.88611725	
    Loss 417882.18336266	
    Loss 416709.6920993	
    Loss 415540.62876254	
    Loss 414374.86231041	
    Loss 413212.37124802	
    Loss 412053.05205099	
    Loss 410897.10566147	
    Loss 409744.27154589	
    Loss 408594.73260293	
    Loss 407448.59955865	
    Loss 406305.63105973	
    Loss 405165.73178914	
    Loss 404029.02568522	
    Loss 402895.5726996	
    Loss 401765.38269419	
    Loss 400638.23089967	
    Loss 399514.33241597	
    Loss 398393.50787912	
    Loss 397275.85192363	
    Loss 396161.35845135	
    Loss 395049.9638874	
    Loss 393941.70919856	
    Loss 392836.46854098	
    Loss 391734.41804682	
    Loss 390635.42776401	
    Loss 389539.62701714	
    Loss 388446.85611056	
    Loss 387357.04324184	
    Loss 386270.5096923	
    Loss 385186.91762758	
    Loss 384106.45669373	
    Loss 383028.79520847	
    Loss 381954.32385412	
    Loss 380883.12686769	
    Loss 379814.50620126	
    Loss 378748.96925104	
    Loss 377686.40501348	
    Loss 376626.85441404	
    Loss 375570.24324977	
Epoch 10	
 50154
  8016
    89
 11026
  7841
     0
     2
  5597
 37490
 11089
   264
    58
     2
     0
    28
    91
    18
    21
    21
     0
     1
[torch.DoubleTensor of size 21]

Validation accuracy:	0.097467528526341	
Grad norm	211.94971464335	
    Loss 375253.92809121	
    Loss 374201.29384576	
    Loss 373151.57024635	
    Loss 372104.62457851	
    Loss 371060.85500237	
    Loss 370020.00791947	
    Loss 368981.97136541	
    Loss 367946.93598491	
    Loss 366914.71482684	
    Loss 365885.4565011	
    Loss 364859.00524534	
    Loss 363835.40501011	
    Loss 362814.80285223	
    Loss 361797.1103185	
    Loss 360782.22337591	
    Loss 359770.20852045	
    Loss 358760.85155374	
    Loss 357754.37825275	
    Loss 356750.75437557	
    Loss 355749.93891925	
    Loss 354751.97935156	
    Loss 353756.65014904	
    Loss 352764.28813863	
    Loss 351774.59080721	
    Loss 350787.91540289	
    Loss 349803.94809396	
    Loss 348822.59342435	
    Loss 347844.08493208	
    Loss 346868.23811546	
    Loss 345895.38809433	
    Loss 344925.00422688	
    Loss 343957.3536457	
    Loss 342992.54273419	
    Loss 342030.35468623	
    Loss 341070.79227724	
    Loss 340114.0391607	
    Loss 339159.98077793	
    Loss 338208.60545119	
    Loss 337259.82489734	
    Loss 336313.81259116	
    Loss 335370.33188041	
    Loss 334429.55796219	
    Loss 333491.58103916	
    Loss 332556.18178389	
    Loss 331623.29494595	
    Loss 330693.02947921	
    Loss 329765.42197072	
    Loss 328840.48050638	
    Loss 327918.03065535	
    Loss 326998.23307753	
    Loss 326080.96519096	
    Loss 325166.28211976	
    Loss 324254.19140564	
    Loss 323344.63260558	
    Loss 322437.64581904	
    Loss 321533.1148002	
    Loss 320631.20232908	
    Loss 319731.80600065	
    Loss 318835.02188567	
    Loss 317940.70212628	
    Loss 317048.80363292	
    Loss 316159.59729818	
    Loss 315272.7883002	
    Loss 314388.55104806	
    Loss 313506.59738133	
    Loss 312627.24980948	
    Loss 311750.59986367	
    Loss 310876.0363236	
    Loss 310004.00765448	
    Loss 309134.4098419	
    Loss 308267.27550636	
    Loss 307402.55022639	
Epoch 11	
 53449
  7104
    44
 10070
  6944
     0
     0
  4825
 38851
 10246
   151
    34
     1
     0
    15
    38
     9
    16
    11
[torch.DoubleTensor of size 19]

Validation accuracy:	0.10006979849478	
Grad norm	192.23482669024	
    Loss 307143.67737633	
    Loss 306282.2156315	
    Loss 305423.1333139	
    Loss 304566.30851851	
    Loss 303712.10112924	
    Loss 302860.28957194	
    Loss 302010.76005912	
    Loss 301163.68415122	
    Loss 300318.9128843	
    Loss 299476.57886919	
    Loss 298636.53491504	
    Loss 297798.82281777	
    Loss 296963.57236437	
    Loss 296130.69418017	
    Loss 295300.11892584	
    Loss 294471.88837815	
    Loss 293645.8345579	
    Loss 292822.13638069	
    Loss 292000.76902637	
    Loss 291181.70219133	
    Loss 290364.9764686	
    Loss 289550.39003002	
    Loss 288738.24541776	
    Loss 287928.2747486	
    Loss 287120.79073257	
    Loss 286315.51382831	
    Loss 285512.37041201	
    Loss 284711.56412965	
    Loss 283912.92586743	
    Loss 283116.76801687	
    Loss 282322.59764263	
    Loss 281530.66901396	
    Loss 280741.07060796	
    Loss 279953.61465553	
    Loss 279168.3056599	
    Loss 278385.29834968	
    Loss 277604.49326803	
    Loss 276825.88609559	
    Loss 276049.40195811	
    Loss 275275.19053251	
    Loss 274503.03773427	
    Loss 273733.10892469	
    Loss 272965.47792587	
    Loss 272199.94404352	
    Loss 271436.46826098	
    Loss 270675.14565245	
    Loss 269915.993895	
    Loss 269159.01965165	
    Loss 268404.08937771	
    Loss 267651.32023867	
    Loss 266900.63468445	
    Loss 266152.05709663	
    Loss 265405.60537981	
    Loss 264661.22066304	
    Loss 263918.9440459	
    Loss 263178.66654054	
    Loss 262440.53910327	
    Loss 261704.48258894	
    Loss 260970.56488499	
    Loss 260238.64985715	
    Loss 259508.71747799	
    Loss 258780.99508572	
    Loss 258055.22620613	
    Loss 257331.57085339	
    Loss 256609.77781357	
    Loss 255890.11248381	
    Loss 255172.67092878	
    Loss 254456.91761995	
    Loss 253743.24906279	
    Loss 253031.56827384	
    Loss 252321.90058968	
    Loss 251614.20848448	
Epoch 12	
 56461
  6220
    18
  9189
  6043
     0
     0
  3994
 40424
  9335
    74
    14
     0
     0
     9
    21
     1
     2
     3
[torch.DoubleTensor of size 19]

Validation accuracy:	0.1012002306385	
Grad norm	174.40194942624	
    Loss 251402.34372588	
    Loss 250697.3300243	
    Loss 249994.26205674	
    Loss 249293.02629739	
    Loss 248593.94991685	
    Loss 247896.83690446	
    Loss 247201.57493498	
    Loss 246508.31852567	
    Loss 245816.95071664	
    Loss 245127.58854454	
    Loss 244440.09401104	
    Loss 243754.50653503	
    Loss 243070.94101583	
    Loss 242389.30834888	
    Loss 241709.5667544	
    Loss 241031.73931856	
    Loss 240355.69499223	
    Loss 239681.57388292	
    Loss 239009.35967509	
    Loss 238339.03015575	
    Loss 237670.62009521	
    Loss 237003.94871645	
    Loss 236339.28819452	
    Loss 235676.39991696	
    Loss 235015.55939243	
    Loss 234356.51619547	
    Loss 233699.21544052	
    Loss 233043.83465971	
    Loss 232390.21886239	
    Loss 231738.65808976	
    Loss 231088.69621923	
    Loss 230440.57348023	
    Loss 229794.36348823	
    Loss 229149.90334509	
    Loss 228507.19887238	
    Loss 227866.37960962	
    Loss 227227.35986348	
    Loss 226590.14057798	
    Loss 225954.65873299	
    Loss 225321.04333224	
    Loss 224689.10133152	
    Loss 224058.98718684	
    Loss 223430.76154096	
    Loss 222804.23965076	
    Loss 222179.40506295	
    Loss 221556.34014709	
    Loss 220935.0476511	
    Loss 220315.53278438	
    Loss 219697.69524334	
    Loss 219081.61757224	
    Loss 218467.25768116	
    Loss 217854.61630305	
    Loss 217243.71898508	
    Loss 216634.50761794	
    Loss 216027.02523976	
    Loss 215421.16861224	
    Loss 214817.07796668	
    Loss 214214.69383744	
    Loss 213614.06032531	
    Loss 213015.05265689	
    Loss 212417.66919788	
    Loss 211822.09976583	
    Loss 211228.1219029	
    Loss 210635.88197286	
    Loss 210045.16044685	
    Loss 209456.17493035	
    Loss 208869.02373298	
    Loss 208283.23732183	
    Loss 207699.16643662	
    Loss 207116.72091494	
    Loss 206535.91990857	
    Loss 205956.74007048	
Epoch 13	
 59698
  5297
     6
  8265
  5152
     0
     0
  3250
 41678
  8415
    30
     6
     0
     0
     5
     5
     0
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.10109401553775	
Grad norm	158.27118362013	
    Loss 205783.34506959	
    Loss 205206.36363626	
    Loss 204630.97382313	
    Loss 204057.06888752	
    Loss 203484.94753038	
    Loss 202914.43458425	
    Loss 202345.42124841	
    Loss 201778.04693571	
    Loss 201212.22138153	
    Loss 200648.04701867	
    Loss 200085.39514789	
    Loss 199524.30276087	
    Loss 198964.87189101	
    Loss 198407.01460609	
    Loss 197850.7109204	
    Loss 197295.96985591	
    Loss 196742.68995962	
    Loss 196190.97908925	
    Loss 195640.82857009	
    Loss 195092.22229203	
    Loss 194545.19014654	
    Loss 193999.56990811	
    Loss 193455.60695571	
    Loss 192913.08750447	
    Loss 192372.25577072	
    Loss 191832.8863069	
    Loss 191294.94011178	
    Loss 190758.57196004	
    Loss 190223.63973508	
    Loss 189690.41198026	
    Loss 189158.46797481	
    Loss 188628.03330263	
    Loss 188099.16910672	
    Loss 187571.73405932	
    Loss 187045.73472488	
    Loss 186521.27889356	
    Loss 185998.29338093	
    Loss 185476.78266455	
    Loss 184956.69422483	
    Loss 184438.13907794	
    Loss 183920.94359545	
    Loss 183405.25095519	
    Loss 182891.11113492	
    Loss 182378.35316018	
    Loss 181866.97979742	
    Loss 181357.06188367	
    Loss 180848.59065865	
    Loss 180341.56969399	
    Loss 179835.92572069	
    Loss 179331.71383947	
    Loss 178828.91966384	
    Loss 178327.52596348	
    Loss 177827.56373708	
    Loss 177328.9752731	
    Loss 176831.80589878	
    Loss 176335.9571625	
    Loss 175841.55940331	
    Loss 175348.56984919	
    Loss 174857.01257552	
    Loss 174366.77364755	
    Loss 173877.86600225	
    Loss 173390.44719754	
    Loss 172904.32483225	
    Loss 172419.63234969	
    Loss 171936.17762971	
    Loss 171454.13827992	
    Loss 170973.6130516	
    Loss 170494.18996941	
    Loss 170016.1794609	
    Loss 169539.49785275	
    Loss 169064.15919207	
    Loss 168590.15182957	
Epoch 14	
 63017
  4488
     3
  7221
  4279
     0
     0
  2617
 42697
  7469
    11
     2
     0
     0
     1
     2
     0
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.10263413449866	
Grad norm	143.67965243252	
    Loss 168448.23950341	
    Loss 167976.0367208	
    Loss 167505.13639949	
    Loss 167035.43752369	
    Loss 166567.21365049	
    Loss 166100.30698166	
    Loss 165634.61342148	
    Loss 165170.2590589	
    Loss 164707.17575411	
    Loss 164245.45255373	
    Loss 163784.96983641	
    Loss 163325.76222974	
    Loss 162867.92061641	
    Loss 162411.35891247	
    Loss 161956.0742601	
    Loss 161502.0651814	
    Loss 161049.25399747	
    Loss 160597.72170299	
    Loss 160147.46642377	
    Loss 159698.47667108	
    Loss 159250.77825274	
    Loss 158804.22523328	
    Loss 158359.03937386	
    Loss 157915.02805369	
    Loss 157472.40896551	
    Loss 157030.97829837	
    Loss 156590.71037307	
    Loss 156151.74016742	
    Loss 155713.93723925	
    Loss 155277.54976866	
    Loss 154842.19068755	
    Loss 154408.07076604	
    Loss 153975.24057465	
    Loss 153543.57768831	
    Loss 153113.0888101	
    Loss 152683.86305452	
    Loss 152255.83859161	
    Loss 151829.02203431	
    Loss 151403.37017082	
    Loss 150978.97835158	
    Loss 150555.69072318	
    Loss 150133.63926236	
    Loss 149712.86528275	
    Loss 149293.20976186	
    Loss 148874.69172936	
    Loss 148457.37161435	
    Loss 148041.23201303	
    Loss 147626.27464131	
    Loss 147212.44821688	
    Loss 146799.78618618	
    Loss 146388.29560891	
    Loss 145977.94581701	
    Loss 145568.77153368	
    Loss 145160.71533691	
    Loss 144753.82483656	
    Loss 144348.00575547	
    Loss 143943.37921185	
    Loss 143539.9166345	
    Loss 143137.62526452	
    Loss 142736.40139003	
    Loss 142336.26941128	
    Loss 141937.35917776	
    Loss 141539.50476541	
    Loss 141142.82744283	
    Loss 140747.15897116	
    Loss 140352.64344902	
    Loss 139959.37870587	
    Loss 139567.00298523	
    Loss 139175.7911819	
    Loss 138785.6657281	
    Loss 138396.63657132	
    Loss 138008.70154733	
Epoch 15	
 66158
  3690
     0
  6302
  3500
     0
     0
  2013
 43659
  6479
     5
     1
     0
     0
     1
[torch.DoubleTensor of size 15]

Validation accuracy:	0.10303623452294	
Grad norm	130.48025274814	
    Loss 137892.55412632	
    Loss 137506.10049048	
    Loss 137120.7131871	
    Loss 136736.29621755	
    Loss 136353.10067482	
    Loss 135970.98354109	
    Loss 135589.84599495	
    Loss 135209.80239942	
    Loss 134830.80287666	
    Loss 134452.92438213	
    Loss 134076.05595245	
    Loss 133700.2299808	
    Loss 133325.52765903	
    Loss 132951.86524619	
    Loss 132579.25323623	
    Loss 132207.68240291	
    Loss 131837.09419822	
    Loss 131467.54720729	
    Loss 131099.04554255	
    Loss 130731.58124562	
    Loss 130365.17669158	
    Loss 129999.7002359	
    Loss 129635.35281496	
    Loss 129271.95995175	
    Loss 128909.71668643	
    Loss 128548.43811162	
    Loss 128188.10961617	
    Loss 127828.84897842	
    Loss 127470.5362993	
    Loss 127113.40068358	
    Loss 126757.08676351	
    Loss 126401.790654	
    Loss 126047.55388373	
    Loss 125694.27073155	
    Loss 125341.94748029	
    Loss 124990.65726275	
    Loss 124640.34855323	
    Loss 124291.02898822	
    Loss 123942.66360794	
    Loss 123595.33411063	
    Loss 123248.90081705	
    Loss 122903.48486238	
    Loss 122559.12050648	
    Loss 122215.65913931	
    Loss 121873.13349045	
    Loss 121531.59463252	
    Loss 121191.01887217	
    Loss 120851.40587455	
    Loss 120512.72223498	
    Loss 120174.98440132	
    Loss 119838.21585532	
    Loss 119502.37607067	
    Loss 119167.50215935	
    Loss 118833.53701575	
    Loss 118500.53032282	
    Loss 118168.39158723	
    Loss 117837.23327883	
    Loss 117507.03894359	
    Loss 117177.80167944	
    Loss 116849.42733619	
    Loss 116521.94926289	
    Loss 116195.47351579	
    Loss 115869.85744186	
    Loss 115545.21089954	
    Loss 115221.38658333	
