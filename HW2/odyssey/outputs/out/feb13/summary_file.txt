==> setup_56632541_10_out.txt <==
[?1034h
==> setup_56632541_11_out.txt <==
[?1034h
==> setup_56632541_12_out.txt <==
[?1034h
==> setup_56632541_13_out.txt <==
[?1034h
==> setup_56632541_14_out.txt <==
[?1034h
==> setup_56632541_15_out.txt <==
[?1034h
==> setup_56632541_16_out.txt <==
[?1034h
==> setup_56632541_17_out.txt <==
[?1034h
==> setup_56632541_18_out.txt <==
[?1034h
==> setup_56632541_19_out.txt <==
[?1034h
==> setup_56632541_1_out.txt <==
[?1034h
==> setup_56632541_20_out.txt <==
[?1034h
==> setup_56632541_21_out.txt <==
[?1034h
==> setup_56632541_2_out.txt <==
[?1034h
==> setup_56632541_3_out.txt <==
[?1034h
==> setup_56632541_4_out.txt <==
[?1034h
==> setup_56632541_5_out.txt <==
[?1034h
==> setup_56632541_6_out.txt <==
[?1034h
==> setup_56632541_7_out.txt <==
[?1034h
==> setup_56632541_8_out.txt <==
[?1034h
==> setup_56632541_9_out.txt <==
[?1034h
==> setup_56632705_10_out.txt <==
[?1034h
==> setup_56632705_11_out.txt <==

==> setup_56632705_12_out.txt <==

==> setup_56632705_13_out.txt <==
[?1034h
==> setup_56632705_14_out.txt <==
[?1034h
==> setup_56632705_15_out.txt <==
[?1034h
==> setup_56632705_16_out.txt <==
[?1034h
==> setup_56632705_17_out.txt <==
[?1034h
==> setup_56632705_18_out.txt <==
[?1034h
==> setup_56632705_19_out.txt <==
[?1034h
==> setup_56632705_1_out.txt <==
[?1034h
==> setup_56632705_20_out.txt <==
[?1034h
==> setup_56632705_21_out.txt <==
[?1034h
==> setup_56632705_2_out.txt <==
[?1034h
==> setup_56632705_3_out.txt <==
[?1034h
==> setup_56632705_4_out.txt <==
[?1034h
==> setup_56632705_5_out.txt <==
[?1034h
==> setup_56632705_6_out.txt <==
[?1034h
==> setup_56632705_7_out.txt <==
[?1034h
==> setup_56632705_8_out.txt <==
[?1034h
==> setup_56632705_9_out.txt <==
[?1034h
==> setup_56632816_10_out.txt <==
[?1034h
==> setup_56632816_11_out.txt <==
[?1034h
==> setup_56632816_12_out.txt <==
[?1034h
==> setup_56632816_13_out.txt <==
[?1034h
==> setup_56632816_14_out.txt <==
[?1034h
==> setup_56632816_15_out.txt <==
[?1034h
==> setup_56632816_16_out.txt <==
[?1034h
==> setup_56632816_17_out.txt <==
[?1034h
==> setup_56632816_18_out.txt <==
[?1034h
==> setup_56632816_19_out.txt <==
[?1034h
==> setup_56632816_1_out.txt <==
[?1034h
==> setup_56632816_20_out.txt <==
[?1034h
==> setup_56632816_21_out.txt <==
[?1034h
==> setup_56632816_2_out.txt <==
[?1034h
==> setup_56632816_3_out.txt <==
[?1034h
==> setup_56632816_4_out.txt <==
[?1034h
==> setup_56632816_5_out.txt <==
[?1034h
==> setup_56632816_6_out.txt <==
[?1034h
==> setup_56632816_7_out.txt <==
[?1034h
==> setup_56632816_8_out.txt <==
[?1034h
==> setup_56632816_9_out.txt <==
[?1034h
==> setup_56633021_10_out.txt <==
[?1034h
==> setup_56633021_11_out.txt <==
[?1034h
==> setup_56633021_12_out.txt <==
[?1034h
==> setup_56633021_13_out.txt <==
[?1034h
==> setup_56633021_14_out.txt <==
[?1034h
==> setup_56633021_15_out.txt <==
[?1034h
==> setup_56633021_16_out.txt <==
[?1034h
==> setup_56633021_17_out.txt <==
[?1034h
==> setup_56633021_18_out.txt <==
[?1034h
==> setup_56633021_19_out.txt <==
[?1034h
==> setup_56633021_1_out.txt <==
[?1034h
==> setup_56633021_20_out.txt <==
[?1034h
==> setup_56633021_21_out.txt <==
[?1034h
==> setup_56633021_2_out.txt <==
[?1034h
==> setup_56633021_3_out.txt <==
[?1034h
==> setup_56633021_4_out.txt <==
[?1034h
==> setup_56633021_5_out.txt <==
[?1034h
==> setup_56633021_6_out.txt <==
[?1034h
==> setup_56633021_7_out.txt <==
[?1034h
==> setup_56633021_8_out.txt <==
[?1034h
==> setup_56633021_9_out.txt <==
[?1034h
==> setup_56633378_10_out.txt <==
[?1034h
==> setup_56633378_11_out.txt <==
[?1034h
==> setup_56633378_12_out.txt <==
[?1034h
==> setup_56633378_13_out.txt <==
[?1034h
==> setup_56633378_14_out.txt <==
[?1034h
==> setup_56633378_15_out.txt <==
[?1034h
==> setup_56633378_16_out.txt <==
[?1034h
==> setup_56633378_17_out.txt <==
[?1034h
==> setup_56633378_18_out.txt <==
[?1034h
==> setup_56633378_19_out.txt <==
[?1034h
==> setup_56633378_1_out.txt <==
[?1034h
==> setup_56633378_20_out.txt <==
[?1034h
==> setup_56633378_21_out.txt <==
[?1034h
==> setup_56633378_2_out.txt <==
[?1034h
==> setup_56633378_3_out.txt <==
[?1034h
==> setup_56633378_4_out.txt <==
[?1034h
==> setup_56633378_5_out.txt <==
[?1034h
==> setup_56633378_6_out.txt <==
[?1034h
==> setup_56633378_7_out.txt <==
[?1034h
==> setup_56633378_8_out.txt <==
[?1034h
==> setup_56633378_9_out.txt <==
[?1034h
==> setup_56633543_10_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.5	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_11_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	1	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_12_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	3	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_13_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	10	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_14_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	50	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_15_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.1	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_16_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.25	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_17_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.5	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_18_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	1	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_19_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	3	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_1_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.1	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_20_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	10	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_21_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	50	Lambda:	1	Minibatch size:	128	Num Epochs:	20	

==> setup_56633543_2_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.25	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_3_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.5	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_4_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	1	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_5_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	3	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_6_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	10	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_7_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	50	Lambda:	1	Minibatch size:	32	Num Epochs:	20	

==> setup_56633543_8_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.1	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633543_9_out.txt <==
[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.25	Lambda:	1	Minibatch size:	64	Num Epochs:	20	

==> setup_56633691_10_out.txt <==
    Loss 2.5482208664173	
    Loss 2.7881016854621	
    Loss 2.5180270671892	
    Loss 2.7281644303071	
    Loss 2.6204595354784	
    Loss 2.6164557624984	
    Loss 2.4530087949135	
    Loss 2.5807347326588	
    Loss 2.5402877346825	
    Loss 2.6308528214101	
    Loss 2.7192598041521	
    Loss 2.4205059004462	
    Loss 2.6059790846133	
    Loss 2.6076323384981	
    Loss 2.5724562768019	
    Loss 2.5028336568125	
    Loss 2.7329761455827	
    Loss 2.7207156887425	
    Loss 2.6942888333883	
Done	

==> setup_56633691_11_out.txt <==
    Loss 2.5136490472102	
    Loss 2.5657204567645	
    Loss 2.4662028966568	
    Loss 2.5547606169461	
    Loss 2.4647471953973	
    Loss 2.4693941776002	
    Loss 2.4692738429886	
    Loss 2.4668483423699	
    Loss 2.5144148886299	
    Loss 2.4447088330638	
    Loss 2.4123337997217	
    Loss 2.5437477059478	
    Loss 2.3784676567497	
    Loss 2.5101823617815	
    Loss 2.5837298570466	
    Loss 2.4702192185081	
    Loss 2.4754961007547	
    Loss 2.4159897439244	
    Loss 2.4512759140809	
Done	

==> setup_56633691_12_out.txt <==
    Loss 2.1605680406306	
    Loss 2.3968037677577	
    Loss 2.2324941257389	
    Loss 2.4461973574612	
    Loss 2.3484670758571	
    Loss 2.3121300513728	
    Loss 2.510913505479	
    Loss 2.2645524652575	
    Loss 2.351251862962	
    Loss 2.2363858508734	
    Loss 2.288608519543	
    Loss 2.5276152538463	
    Loss 2.502270554975	
    Loss 2.1844703458683	
    Loss 2.2738276041314	
    Loss 2.423802643866	
    Loss 2.2952064124408	
    Loss 2.4386986266868	
    Loss 2.4171329352301	
Done	

==> setup_56633691_13_out.txt <==
    Loss 1.95258065156	
    Loss 2.0406457967475	
    Loss 1.9397600554807	
    Loss 2.3531066354424	
    Loss 2.226019264916	
    Loss 2.4042455336718	
    Loss 2.8809911716317	
    Loss 2.2942786918034	
    Loss 2.2014062429072	
    Loss 2.1656146325785	
    Loss 2.2652142610115	
    Loss 2.5444788198981	
    Loss 2.4072383832246	
    Loss 1.8854679323158	
    Loss 2.1675568815423	
    Loss 2.4265340349663	
    Loss 2.247741216003	
    Loss 2.3504358742428	
    Loss 2.4545851480208	
Done	

==> setup_56633691_14_out.txt <==
    Loss 5.7112606988289	
    Loss 6.7327529233503	
    Loss 4.2978136413114	
    Loss 6.6162625126593	
    Loss 11.348220846939	
    Loss 16.793156100757	
    Loss 19.955371604061	
    Loss 10.734617734717	
    Loss 9.1469080688855	
    Loss 7.915804914572	
    Loss 4.4884701099015	
    Loss 9.9895136796675	
    Loss 8.8192622136354	
    Loss 4.3864433809423	
    Loss 5.9392602320657	
    Loss 5.5873523795831	
    Loss 5.2961504381448	
    Loss 8.0562452367399	
    Loss 8.5202615179268	
Done	

==> setup_56633691_15_out.txt <==
    Loss 3.3854195015619	
    Loss 3.2819126099987	
    Loss 3.225875262793	
    Loss 3.249842160414	
    Loss 3.1435474274345	
    Loss 3.1149501834255	
    Loss 3.1989394681958	
    Loss 3.197546628541	
    Loss 3.1840594398365	
    Loss 3.2343695963482	
    Loss 3.2752682088088	
    Loss 3.156428687033	
    Loss 2.9433977218287	
    Loss 3.1040323980866	
    Loss 3.0671083938743	
    Loss 3.3028564911987	
    Loss 3.2728600786606	
    Loss 3.2369745573745	
    Loss 3.2796586902089	
Done	

==> setup_56633691_16_out.txt <==
    Loss 3.0884852989938	
    Loss 2.9384694147459	
    Loss 3.066949741242	
    Loss 3.2974835986136	
    Loss 3.1726971616009	
    Loss 3.0345467839452	
    Loss 3.1518302100909	
    Loss 3.0686916858651	
    Loss 2.973188498009	
    Loss 3.2303104431412	
    Loss 3.2319836409837	
    Loss 2.9195228032595	
    Loss 2.8523020882199	
    Loss 3.206355199873	
    Loss 2.9289911385172	
    Loss 3.1990407688817	
    Loss 3.1021277612994	
    Loss 2.9953948493415	
    Loss 3.0029730150398	
Done	

==> setup_56633691_17_out.txt <==
    Loss 3.161418279209	
    Loss 2.8581497740629	
    Loss 3.1104690531469	
    Loss 3.2701497605419	
    Loss 2.9690162465359	
    Loss 2.9173667379535	
    Loss 3.1803527741577	
    Loss 2.9110196545689	
    Loss 2.8380474966089	
    Loss 3.2339537086243	
    Loss 2.8864271462032	
    Loss 2.7288320451113	
    Loss 2.8095669186921	
    Loss 3.0983741979926	
    Loss 2.9918561710862	
    Loss 3.105655323907	
    Loss 2.9581841818307	
    Loss 2.9816337877695	
    Loss 2.9612679068264	
Done	

==> setup_56633691_18_out.txt <==
    Loss 3.1508429885668	
    Loss 2.6991321688191	
    Loss 2.9495892678074	
    Loss 3.1473783976465	
    Loss 2.8994854520643	
    Loss 2.779273086905	
    Loss 2.9617258964939	
    Loss 2.8374702163697	
    Loss 2.8229866475889	
    Loss 3.0216160030825	
    Loss 2.818360342905	
    Loss 2.8423748036057	
    Loss 2.789455174257	
    Loss 2.8989984750846	
    Loss 2.8120820430006	
    Loss 2.8738700149589	
    Loss 2.7961733478777	
    Loss 2.7989031790855	
    Loss 2.919648424295	
Done	

==> setup_56633691_19_out.txt <==
    Loss 2.6725507517006	
    Loss 2.695262199177	
    Loss 2.762419761733	
    Loss 2.7498806105207	
    Loss 2.7609263868347	
    Loss 2.729465513196	
    Loss 2.571952805174	
    Loss 2.7040889535224	
    Loss 2.6898300247718	
    Loss 2.619953102158	
    Loss 2.6344459538279	
    Loss 2.6857692710578	
    Loss 2.6684788899605	
    Loss 2.7840987858926	
    Loss 2.7052109714388	
    Loss 2.7832948896573	
    Loss 2.5351245828625	
    Loss 2.6369229457998	
    Loss 2.7158340545469	
Done	

==> setup_56633691_1_out.txt <==
    Loss 2.4071649820717	
    Loss 2.5943960249144	
    Loss 2.3149881823203	
    Loss 2.4648751766019	
    Loss 2.4143778815158	
    Loss 2.37261115481	
    Loss 2.3628735626059	
    Loss 2.575008729522	
    Loss 2.3228827637863	
    Loss 2.4055807201832	
    Loss 2.6503299751365	
    Loss 2.3160162826288	
    Loss 2.3741430701807	
    Loss 2.5506274875971	
    Loss 2.7960841031047	
    Loss 2.4657956761028	
    Loss 2.5242841525391	
    Loss 2.4849856217847	
    Loss 2.7511646743748	
Done	

==> setup_56633691_20_out.txt <==
    Loss 2.425652763954	
    Loss 2.5622693229467	
    Loss 2.5512388609923	
    Loss 2.4879486770038	
    Loss 2.6063387694985	
    Loss 2.7593804121366	
    Loss 2.3563426146939	
    Loss 2.7016395850143	
    Loss 2.6754036139662	
    Loss 2.281527945343	
    Loss 2.356258865566	
    Loss 2.6064917889676	
    Loss 2.6901474032032	
    Loss 2.645579908186	
    Loss 2.5690898250816	
    Loss 2.5246309494585	
    Loss 2.4267014407165	
    Loss 2.4463675478023	
    Loss 2.7201193445858	
Done	

==> setup_56633691_21_out.txt <==
    Loss 5.1151761961622	
    Loss 6.0309341373428	
    Loss 5.2189846058144	
    Loss 5.1033376332529	
    Loss 5.1235981240932	
    Loss 11.686245168139	
    Loss 4.6939462249762	
    Loss 4.8805106918342	
    Loss 6.5672697753086	
    Loss 4.0126617745753	
    Loss 3.2888031081336	
    Loss 5.4109090465721	
    Loss 8.2932994977352	
    Loss 8.1944594585339	
    Loss 7.1289514214974	
    Loss 7.2411595259235	
    Loss 4.1489605714361	
    Loss 5.7911416568996	
    Loss 6.233095880087	
Done	

==> setup_56633691_2_out.txt <==
    Loss 2.3753328162149	
    Loss 2.5935125681233	
    Loss 2.2479757830954	
    Loss 2.2714762884575	
    Loss 2.6416833088894	
    Loss 2.2476295193097	
    Loss 2.3848509241021	
    Loss 2.5378200579436	
    Loss 2.3539764639691	
    Loss 2.3679503010087	
    Loss 2.291874402697	
    Loss 2.3046066765249	
    Loss 2.4189890980713	
    Loss 2.5489542650017	
    Loss 2.3725955246985	
    Loss 2.4084272701688	
    Loss 2.2912048687339	
    Loss 2.26550340236	
    Loss 2.4163842210902	
Done	

==> setup_56633691_3_out.txt <==
    Loss 2.3228582989124	
    Loss 2.0502204691569	
    Loss 2.2800686687606	
    Loss 2.2092035085624	
    Loss 2.269203587826	
    Loss 2.3209277004409	
    Loss 2.3941085105922	
    Loss 2.2326960996836	
    Loss 2.3018533115337	
    Loss 2.0695692410417	
    Loss 2.2266146864307	
    Loss 2.293837705736	
    Loss 2.3330946603779	
    Loss 2.2371736064783	
    Loss 2.0365803019508	
    Loss 2.251241349778	
    Loss 2.0375082509648	
    Loss 2.2669687462352	
    Loss 2.2995480992007	
Done	

==> setup_56633691_4_out.txt <==
    Loss 2.327125087333	
    Loss 2.1052919363382	
    Loss 2.1205258310076	
    Loss 2.0891456990249	
    Loss 2.3154147628277	
    Loss 2.1909122312909	
    Loss 2.2260709155094	
    Loss 1.9602867546697	
    Loss 2.277482927309	
    Loss 2.2598156874941	
    Loss 2.2321290171768	
    Loss 2.0617074729317	
    Loss 2.2297890525998	
    Loss 2.0527428704478	
    Loss 1.8769588518146	
    Loss 2.2567505639521	
    Loss 2.140630696518	
    Loss 2.2134367936932	
    Loss 2.2428736817946	
Done	

==> setup_56633691_5_out.txt <==
    Loss 2.0686242325561	
    Loss 1.8188380671296	
    Loss 2.2840683023723	
    Loss 2.2315430043298	
    Loss 2.275625883487	
    Loss 2.012686751168	
    Loss 2.0552974708028	
    Loss 2.1277110241985	
    Loss 1.8463626009845	
    Loss 1.9221080889691	
    Loss 2.1269704649763	
    Loss 2.1285518727542	
    Loss 2.0359346253344	
    Loss 1.8719999788841	
    Loss 2.0441799806597	
    Loss 1.8732555249043	
    Loss 1.9278265963158	
    Loss 2.1523793709301	
    Loss 2.0784346172485	
    Loss 2.2508042318218	

==> setup_56633691_6_out.txt <==
    Loss 2.5571369138217	
    Loss 1.7187483569156	
    Loss 2.9004791260958	
    Loss 2.2105197723038	
    Loss 2.3618145411557	
    Loss 2.38198237742	
    Loss 2.3626481514568	
    Loss 2.4255394415202	
    Loss 2.3282381979743	
    Loss 2.1769225897288	
    Loss 2.5991335630447	
    Loss 2.6265217717061	
    Loss 2.1361774221379	
    Loss 2.1671219859731	
    Loss 2.5742421329099	
    Loss 2.2301290048102	
    Loss 2.306935033243	
    Loss 1.767608703385	
    Loss 2.0340690841156	
    Loss 2.476914406624	

==> setup_56633691_7_out.txt <==
    Loss 7.3868265322223	
    Loss 7.821319555944	
    Loss 7.1464776804818	
    Loss 5.601741469759	
    Loss 4.9933989259425	
    Loss 8.0365762673441	
    Loss 9.3491422290574	
    Loss 8.7551314342168	
    Loss 11.889630327545	
    Loss 5.5041344066659	
    Loss 6.8204332404742	
    Loss 8.1017528384816	
    Loss 8.3101034895091	
    Loss 9.38267599467	
    Loss 6.7134496692732	
    Loss 6.4063560428893	
    Loss 9.0822001691412	
    Loss 8.2751425322893	
    Loss 7.2283499839715	
    Loss 8.7111375171467	

==> setup_56633691_8_out.txt <==
    Loss 3.0522202130271	
    Loss 2.94393295423	
    Loss 3.0695123222344	
    Loss 2.8935025967664	
    Loss 2.7714349857995	
    Loss 2.711814777781	
    Loss 2.4841904110803	
    Loss 2.9032613537291	
    Loss 2.8287219453308	
    Loss 2.9182034440363	
    Loss 2.7600752702275	
    Loss 2.8484846980809	
    Loss 2.7116010927161	
    Loss 2.9146809551373	
    Loss 2.8065365531867	
    Loss 2.8371183776412	
    Loss 2.8451956450907	
    Loss 2.926826340814	
    Loss 2.6464185985456	
Done	

==> setup_56633691_9_out.txt <==
    Loss 2.9452693480933	
    Loss 2.6985526938196	
    Loss 2.5862143309069	
    Loss 2.6963380064046	
    Loss 2.8130040915106	
    Loss 2.6733755313128	
    Loss 2.601892855014	
    Loss 2.7682484492724	
    Loss 2.6264702450685	
    Loss 2.7020456141575	
    Loss 2.7077665790329	
    Loss 2.7303178549163	
    Loss 2.7063283145788	
    Loss 2.6947518493121	
    Loss 2.5506378310358	
    Loss 2.7028744879069	
    Loss 2.6860328476019	
    Loss 2.7633362774755	
    Loss 2.6840998318218	
Done	

==> setup_56634795_22_out.txt <==
    Loss 3.6321947506092	
    Loss 3.567723704806	
    Loss 3.5618185921942	
    Loss 3.355415630614	
    Loss 3.4685594164421	
    Loss 3.261626837499	
    Loss 3.6821717734038	
    Loss 3.8386433088079	
    Loss 3.6563167353839	
    Loss 3.6433783993386	
    Loss 3.4052850040575	
    Loss 3.4412266617475	
    Loss 3.2130097886859	
    Loss 3.5733727251824	
    Loss 3.3974765143926	
    Loss 3.4870999438099	
    Loss 3.531601631449	
    Loss 3.4691582755841	
    Loss 3.7074042513983	
Done	

==> setup_56634795_23_out.txt <==
    Loss 3.6382267436567	
    Loss 4.0340206782613	
    Loss 4.26052679863	
    Loss 3.9155973864463	
    Loss 3.8371249019565	
    Loss 3.8645336618903	
    Loss 3.9779148294662	
    Loss 3.9698347573361	
    Loss 3.9239736374417	
    Loss 3.8871115894101	
    Loss 3.7896150090302	
    Loss 3.7201280965877	
    Loss 3.8105877000859	
    Loss 3.9305045250429	
    Loss 4.1243409300855	
    Loss 3.7788037038645	
    Loss 3.9546493516629	
    Loss 4.0577301988414	
    Loss 4.1094778345759	
Done	

==> setup_56634795_24_out.txt <==
    Loss 4.4633333026009	
    Loss 4.5199002969898	
    Loss 4.3295052496024	
    Loss 4.4395152887967	
    Loss 4.501405888881	
    Loss 4.2548558286424	
    Loss 4.4115575597659	
    Loss 4.7923025545338	
    Loss 4.3776687202792	
    Loss 4.3407984211289	
    Loss 4.7745165815761	
    Loss 4.5660218478957	
    Loss 4.6115856895523	
    Loss 4.8056717512405	
    Loss 4.233555462213	
    Loss 4.6500703775648	
    Loss 4.4615865816049	
    Loss 4.7594293654508	
    Loss 4.5431715527083	
Done	

==> setup_56642754_10_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_11_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_12_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_13_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_14_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_15_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_16_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_17_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_18_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_19_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_1_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_20_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_21_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_22_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_23_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_24_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_25_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_26_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_27_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_28_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_29_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_2_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_30_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_31_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_32_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_33_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_34_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_35_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_36_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_37_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_38_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_39_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_3_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_40_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_41_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_42_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_43_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_44_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_45_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_46_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_47_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_48_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_49_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_4_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_50_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_51_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_52_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_53_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_54_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_55_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_56_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_57_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_58_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_59_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_5_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_60_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_61_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_62_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_63_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_6_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_7_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_8_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56642754_9_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_10_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_11_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_12_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_13_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_14_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_15_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_16_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_17_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_18_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_19_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_1_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_20_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_21_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_22_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_23_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_24_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_25_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_26_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_27_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_28_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_29_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_2_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_30_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_31_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_32_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_33_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_34_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_35_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_36_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_37_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_38_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_39_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_3_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_40_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_41_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_42_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_43_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_44_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_45_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_46_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_47_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_48_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_49_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_4_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_50_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_51_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_52_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_53_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_54_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_55_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_56_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_57_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_58_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_59_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_5_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_60_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_61_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_62_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_63_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_6_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_7_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_8_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643160_9_out.txt <==
[?1034h	
	
Usage: [options] 
  -datafile   data file []
  -classifier classifier to use [lr]
  -alpha      laplacian smoothing factor for NB [1]
  -eta        Learning rate [0.5]
  -lambda     regularization penalty [1]
  -minibatch  Minibatch size [1000]
  -epochs     Number of epochs of SGD [50]

==> setup_56643429_10_out.txt <==
    Loss 1686.5819598194	
    Loss 1676.7940669913	
    Loss 1667.0216392892	
    Loss 1657.442198591	
    Loss 1647.7244761562	
    Loss 1638.083906204	
    Loss 1628.6277340288	
    Loss 1619.3221639513	
    Loss 1609.8655062665	
    Loss 1600.6135732642	
    Loss 1591.3986718212	
    Loss 1582.0219275147	
    Loss 1572.9537786335	
    Loss 1563.9007935036	
    Loss 1554.9103285908	
    Loss 1546.0571931841	
    Loss 1537.177950158	
    Loss 1528.2929220217	
    Loss 1519.5031742826	
    Loss 1510.7293227462	

==> setup_56643429_11_out.txt <==
      0
      0
      8
      0
      0
      0
   2961
[torch.DoubleTensor of size 9]

Validation accuracy:	0.097148883224084	
Grad norm	5.8740641801304	
    Loss 286.65236460199	
    Loss 286.66315034475	
    Loss 287.00198322796	
    Loss 287.04044785568	
    Loss 287.11666311968	
    Loss 286.89979216273	
    Loss 287.01897002217	
    Loss 286.96786308542	
    Loss 287.14864881992	

==> setup_56643429_12_out.txt <==
    Loss 379.55575786456	
    Loss 380.63986287766	
    Loss 380.75618807405	
    Loss 380.47765161861	
    Loss 380.52453370436	
    Loss 379.7590123879	
    Loss 379.02186780593	
    Loss 378.38173730268	
    Loss 379.19610077173	
    Loss 377.8142773522	
    Loss 378.83430596807	
    Loss 379.18895224189	
    Loss 380.1519895034	
    Loss 379.78598969937	
    Loss 380.10218276404	
    Loss 381.44749382853	
    Loss 382.74760487376	
    Loss 382.03243146003	
    Loss 382.73038851756	
    Loss 383.56621139233	

==> setup_56643429_13_out.txt <==
    Loss 855.56612422625	
    Loss 853.8340986226	
    Loss 862.04154725089	
    Loss 860.03947342479	
    Loss 861.27754149058	
    Loss 860.43408294873	
    Loss 854.63781642928	
    Loss 853.01041095307	
    Loss 849.28650141686	
    Loss 857.10370511809	
    Loss 853.49068065895	
    Loss 856.50840029293	
    Loss 859.01591442357	
    Loss 859.4239117438	
    Loss 856.86008498016	
    Loss 859.75764046473	
    Loss 864.3025039696	
    Loss 865.89716782409	
    Loss 856.43400037185	
    Loss 857.31937032352	

==> setup_56643429_14_out.txt <==
    Loss 6125.3751529906	
    Loss 6292.8269247076	
    Loss 6394.1878386955	
    Loss 6462.8200372922	
    Loss 6181.8964456967	
    Loss 6170.3037312572	
    Loss 6182.1673284272	
    Loss 6065.0357154738	
    Loss 6413.1970212844	
    Loss 6509.027587757	
    Loss 6280.4468148991	
    Loss 6529.2402300741	
    Loss 6327.1327977788	
    Loss 6654.4842382076	
    Loss 6368.4356656532	
    Loss 6422.5031879686	
    Loss 6112.8132622442	
    Loss 6218.563424514	
    Loss 6051.3593465746	
    Loss 6245.5578391853	

==> setup_56643429_15_out.txt <==
    Loss 121531.59463252	
    Loss 121191.01887217	
    Loss 120851.40587455	
    Loss 120512.72223498	
    Loss 120174.98440132	
    Loss 119838.21585532	
    Loss 119502.37607067	
    Loss 119167.50215935	
    Loss 118833.53701575	
    Loss 118500.53032282	
    Loss 118168.39158723	
    Loss 117837.23327883	
    Loss 117507.03894359	
    Loss 117177.80167944	
    Loss 116849.42733619	
    Loss 116521.94926289	
    Loss 116195.47351579	
    Loss 115869.85744186	
    Loss 115545.21089954	
    Loss 115221.38658333	

==> setup_56643429_16_out.txt <==
    Loss 1827.5514540448	
    Loss 1815.8484920135	
    Loss 1804.2145965509	
    Loss 1792.6902798785	
    Loss 1781.2707072819	
    Loss 1769.8909090637	
    Loss 1758.5385720992	
    Loss 1747.2773540352	
    Loss 1736.086599618	
    Loss 1725.0060672774	
    Loss 1714.0165284696	
    Loss 1703.1358260429	
    Loss 1692.2887996407	
    Loss 1681.5542285003	
    Loss 1670.7785336107	
    Loss 1660.1736711626	
    Loss 1649.6846487002	
    Loss 1639.2606490514	
    Loss 1628.825966972	
    Loss 1618.507667776	

==> setup_56643429_17_out.txt <==
    Loss 155.25224798371	
    Loss 155.3129234224	
    Loss 155.19500670244	
    Loss 155.17144384854	
    Loss 155.33779864391	
    Loss 155.37723957477	
    Loss 155.30189449287	
    Loss 155.36307198738	
    Loss 155.33746232819	
    Loss 155.35239930969	
    Loss 155.35154859351	
    Loss 155.37061152014	
    Loss 155.32368752678	
    Loss 155.26176142594	
    Loss 155.25253865987	
    Loss 155.26501857902	
    Loss 155.23871604774	
    Loss 155.30415520878	
    Loss 155.34652861963	
Done	

==> setup_56643429_18_out.txt <==
    Loss 159.014991391	
    Loss 159.10265303154	
    Loss 159.05037878556	
    Loss 158.81032985801	
    Loss 158.81778522469	
    Loss 158.86569484489	
    Loss 158.92705896776	
    Loss 158.89652689859	
    Loss 158.80251442035	
    Loss 159.09329917928	
    Loss 159.03426342982	
    Loss 159.10097789678	
    Loss 159.26071264964	
    Loss 159.21813377729	
    Loss 159.23183767579	
    Loss 159.03800521174	
    Loss 158.66860284453	
    Loss 158.89455136609	
    Loss 159.05501243202	
    Loss 158.95703003093	

==> setup_56643429_19_out.txt <==
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555933	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	

==> setup_56643429_1_out.txt <==
    Loss 864432.38825362	
    Loss 863823.57811738	
    Loss 863214.90783141	
    Loss 862607.31629199	
    Loss 861999.72889733	
    Loss 861392.67813219	
    Loss 860785.87664927	
    Loss 860179.0220945	
    Loss 859573.50897752	
    Loss 858968.30790924	
    Loss 858363.52654612	
    Loss 857759.20066175	
    Loss 857155.24269013	
    Loss 856551.91541487	
    Loss 855948.55233058	
    Loss 855345.65590262	
    Loss 854743.42835046	
    Loss 854141.58285506	
    Loss 853539.95091133	
    Loss 852939.09809299	

==> setup_56643429_20_out.txt <==
    Loss 335.06002991826	
    Loss 334.57285770568	
    Loss 331.68958427649	
    Loss 328.94608702058	
    Loss 334.38406354643	
    Loss 333.41031475146	
    Loss 333.07056954785	
    Loss 332.57348095825	
    Loss 331.70909019141	
    Loss 331.93953311672	
    Loss 332.3880145999	
    Loss 333.64048310414	
    Loss 332.84631243698	
    Loss 326.65438348658	
    Loss 328.14837852915	
    Loss 329.20407310825	
    Loss 331.5119911193	
    Loss 330.69272489175	
    Loss 333.22584094514	
Done	

==> setup_56643429_21_out.txt <==
    Loss 2260.5016700684	
    Loss 1906.2384451433	
    Loss 1746.2947011594	
    Loss 1898.9166361191	
    Loss 2217.0655852831	
    Loss 1990.3698853292	
    Loss 2077.6421871398	
    Loss 1900.0986277501	
    Loss 2031.5928593405	
    Loss 2287.8574523556	
    Loss 1765.0681356134	
    Loss 1898.2635587137	
    Loss 1852.7159908696	
    Loss 1766.4043130087	
    Loss 2008.8612336623	
    Loss 1760.0274319074	
    Loss 1901.9950565304	
    Loss 1768.9934894506	
    Loss 2436.2811750532	
    Loss 2298.1729609698	

==> setup_56643429_22_out.txt <==
    Loss 294961.26185553	
    Loss 293930.20452272	
    Loss 292902.81597659	
    Loss 291878.8648674	
    Loss 290858.79933352	
    Loss 289842.00926931	
    Loss 288829.10405205	
    Loss 287819.65876931	
    Loss 286813.74813956	
    Loss 285811.26273553	
    Loss 284812.27908485	
    Loss 283816.71382942	
    Loss 282824.6149223	
    Loss 281836.00360898	
    Loss 280850.81050023	
    Loss 279869.4964622	
    Loss 278891.13405966	
    Loss 277916.74543576	
    Loss 276945.0890405	
    Loss 275977.06507563	

==> setup_56643429_23_out.txt <==
    Loss 2103.3217016136	
    Loss 2091.6425924222	
    Loss 2080.0533340218	
    Loss 2068.9641785198	
    Loss 2058.0239433268	
    Loss 2046.3169833185	
    Loss 2034.793449275	
    Loss 2024.3049154907	
    Loss 2013.6166207228	
    Loss 2003.2581252299	
    Loss 1992.9579191314	
    Loss 1982.0576722388	
    Loss 1971.0530663915	
    Loss 1960.4652790157	
    Loss 1950.4313623009	
    Loss 1939.7984258731	
    Loss 1929.7122952745	
    Loss 1919.428377726	
    Loss 1908.9344880161	
    Loss 1899.0920729593	

==> setup_56643429_24_out.txt <==
    Loss 823.92359160106	
    Loss 822.80259824255	
    Loss 822.90797296156	
    Loss 822.63507928139	
    Loss 822.63343687624	
    Loss 821.95585197661	
    Loss 822.68741510916	
    Loss 823.34499658654	
    Loss 822.67244226206	
    Loss 823.32863047502	
    Loss 822.73542993799	
    Loss 823.4144127162	
    Loss 824.65478836215	
    Loss 824.22492928868	
    Loss 823.17268638831	
    Loss 823.5319716372	
    Loss 823.92342392503	
    Loss 825.06142046411	
    Loss 825.79171628342	
    Loss 825.43429957689	

==> setup_56643429_25_out.txt <==
    Loss 966.49615346998	
    Loss 968.04570646231	
    Loss 965.35919927757	
    Loss 961.04810617472	
    Loss 960.0377381382	
    Loss 960.24267863752	
    Loss 959.88113908496	
    Loss 963.64203255736	
    Loss 965.54949548968	
    Loss 964.5577842771	
    Loss 966.3014852877	
    Loss 962.51023706926	
    Loss 962.12943644324	
    Loss 960.12494146335	
    Loss 961.25140347814	
    Loss 960.92143031973	
    Loss 960.09020310263	
    Loss 960.08185991344	
    Loss 961.62800982867	
    Loss 961.94865494235	

==> setup_56643429_26_out.txt <==
    Loss 1567.7694319233	
    Loss 1570.0351084861	
    Loss 1573.9758709138	
    Loss 1563.8859141314	
    Loss 1577.2282934518	
    Loss 1579.6228298552	
    Loss 1575.8780349303	
    Loss 1577.1091936605	
    Loss 1584.2100343245	
    Loss 1580.8450032666	
    Loss 1571.7217755116	
    Loss 1578.6472756456	
    Loss 1593.2457829725	
    Loss 1588.3562774957	
    Loss 1585.4989128281	
    Loss 1582.4555610023	
    Loss 1585.0410932414	
    Loss 1590.8290536172	
    Loss 1583.1961730229	
    Loss 1561.676659382	

==> setup_56643429_27_out.txt <==
    Loss 3828.2364632707	
    Loss 3846.033991286	
    Loss 3804.2681610526	
    Loss 3879.7740608758	
    Loss 3839.0495446253	
    Loss 3881.402842276	
    Loss 3833.4695978925	
    Loss 3868.8853787877	
    Loss 3897.6268484346	
    Loss 3932.8437987308	
    Loss 3869.2175358224	
    Loss 3867.0563900855	
    Loss 3839.4178325522	
    Loss 3864.7221636297	
    Loss 3932.9639018836	
    Loss 3998.4870063286	
    Loss 3789.35091363	
    Loss 3783.269433637	
    Loss 3798.0095472718	
    Loss 3839.4334539432	

==> setup_56643429_28_out.txt <==
    Loss 28014.982686757	
Epoch 6	
     0
   334
  1759
   117
  4963
     0
     0
 51358
 64372
     0
  5338
  1139
  2428
[torch.DoubleTensor of size 13]

Validation accuracy:	0.10429564214615	
Grad norm	9.5127530551449	
    Loss 28869.025048928	

==> setup_56643429_29_out.txt <==
    Loss 156421.39286223	
    Loss 155330.87222334	
    Loss 154248.06279056	
    Loss 153173.08022201	
    Loss 152105.23769584	
    Loss 151045.20257897	
    Loss 149992.39018995	
    Loss 148946.82002539	
    Loss 147908.53990591	
    Loss 146877.47918702	
    Loss 145853.91625131	
    Loss 144837.33988354	
    Loss 143827.91247811	
    Loss 142825.68122225	
    Loss 141830.4763532	
    Loss 140842.19979382	
    Loss 139860.84478403	
    Loss 138886.28948544	
    Loss 137918.70288272	
    Loss 136957.71268057	

==> setup_56643429_2_out.txt <==
    Loss 134639.2397431	
    Loss 134403.28981361	
    Loss 134167.9069771	
    Loss 133933.04241942	
    Loss 133698.51252232	
    Loss 133464.39661875	
    Loss 133230.756966	
    Loss 132997.31260064	
    Loss 132764.31863723	
    Loss 132531.69079868	
    Loss 132299.34656615	
    Loss 132067.48670751	
    Loss 131836.27665242	
    Loss 131605.34051499	
    Loss 131374.83087206	
    Loss 131144.71944994	
    Loss 130914.85300374	
    Loss 130685.72606129	
    Loss 130456.6806596	
    Loss 130228.10964573	

==> setup_56643429_30_out.txt <==
    Loss 759.96258371282	
    Loss 755.29935784903	
    Loss 750.86022808906	
    Loss 746.77369568422	
    Loss 742.4568703803	
    Loss 738.43370731877	
    Loss 734.47369621083	
    Loss 730.53702154155	
    Loss 726.58638162879	
    Loss 722.55584691454	
    Loss 719.057161266	
    Loss 714.79338639226	
    Loss 711.25467173694	
    Loss 707.91849675648	
    Loss 704.84793671397	
    Loss 701.55335147628	
    Loss 698.10141098497	
    Loss 695.0100932794	
    Loss 692.24573233403	
    Loss 689.14147954744	

==> setup_56643429_31_out.txt <==
    Loss 524.76182560064	
    Loss 523.9935859664	
    Loss 524.03077218248	
    Loss 524.35259970682	
    Loss 524.07821839355	
    Loss 524.24263552161	
    Loss 524.35904944458	
    Loss 524.5286867919	
    Loss 524.1369681829	
    Loss 523.54172626356	
    Loss 523.93998792654	
    Loss 522.5473873893	
    Loss 523.1251092117	
    Loss 523.58296186997	
    Loss 524.30880492688	
    Loss 524.43884658546	
    Loss 524.26484980703	
    Loss 524.89502009592	
    Loss 525.80740649039	
    Loss 525.83951195085	

==> setup_56643429_32_out.txt <==
    Loss 565.7624826683	
    Loss 564.00537617252	
    Loss 564.88608622084	
    Loss 565.71374388065	
    Loss 565.49953150795	
    Loss 565.80461109864	
    Loss 565.96879435589	
    Loss 565.86968800143	
    Loss 564.81427955579	
    Loss 563.60156910413	
    Loss 564.70035030779	
    Loss 561.93744820884	
    Loss 563.92235162569	
    Loss 564.79796335543	
    Loss 566.23406122631	
    Loss 566.02214423894	
    Loss 565.87896371581	
    Loss 567.51783290885	
    Loss 569.24057929212	
    Loss 568.81720716798	

==> setup_56643429_33_out.txt <==
    Loss 750.22138574351	
    Loss 753.85334485538	
    Loss 750.65407058438	
    Loss 745.53567456179	
    Loss 748.58445977018	
    Loss 753.03215924673	
    Loss 751.97381897752	
    Loss 752.74790973648	
    Loss 753.23757001408	
    Loss 748.34319754775	
    Loss 747.20697625221	
    Loss 743.07889466929	
    Loss 749.67114586089	
    Loss 742.611414268	
    Loss 748.93148903235	
    Loss 752.50143107842	
    Loss 754.84903532068	
    Loss 751.96332749277	
    Loss 752.63312715052	
    Loss 756.98209807022	

==> setup_56643429_34_out.txt <==
    Loss 1403.0722297405	
    Loss 1428.4760781542	
    Loss 1414.0617548473	
    Loss 1401.3723553659	
    Loss 1394.5956347718	
    Loss 1432.6639510057	
    Loss 1406.9735226472	
    Loss 1417.8867526172	
    Loss 1409.9999068083	
    Loss 1386.4687543047	
    Loss 1383.275703719	
    Loss 1377.0296525993	
    Loss 1418.444581916	
    Loss 1383.7735922082	
    Loss 1419.6495468449	
    Loss 1428.8350346706	
    Loss 1419.5899289	
    Loss 1406.3967226941	
    Loss 1419.7571694297	
    Loss 1424.1728282452	

==> setup_56643429_35_out.txt <==
Grad norm	10.225883307065	
    Loss 9615.6537755449	
    Loss 11654.492396351	
    Loss 9887.9263993378	
    Loss 10956.55213348	
    Loss 11593.6010089	
    Loss 8744.5210172063	
    Loss 8538.9906991419	
    Loss 17455.024432695	
    Loss 8460.5446480608	
    Loss 8999.102446807	
    Loss 9553.3973960277	
    Loss 8476.2890475594	
    Loss 8936.9394142695	
    Loss 12099.679697063	
    Loss 10727.629235568	
    Loss 9513.0445116934	
    Loss 9956.2493382454	
    Loss 11822.349205124	
    Loss 9851.7537787719	

==> setup_56643429_36_out.txt <==
    Loss 1675.7918782296	
    Loss 1657.1393877855	
    Loss 1638.8505762702	
    Loss 1620.6262761437	
    Loss 1602.5574889633	
    Loss 1584.9002213006	
    Loss 1567.485672864	
    Loss 1550.286595076	
    Loss 1533.4099403376	
    Loss 1516.6952599109	
    Loss 1500.1089571732	
    Loss 1483.8153589803	
    Loss 1467.8187596161	
    Loss 1452.0968956169	
    Loss 1436.5363338712	
    Loss 1421.026024531	
    Loss 1405.860258749	
    Loss 1390.8711178426	
    Loss 1376.1736745954	
    Loss 1361.6520522009	

==> setup_56643429_37_out.txt <==
      0
      0
      0
      0
  15656
[torch.DoubleTensor of size 9]

Validation accuracy:	0.096079145423647	
Grad norm	6.0985416010288	
    Loss 334.75537014591	
    Loss 334.76779346376	
    Loss 335.0077500195	
    Loss 335.01676287849	
    Loss 335.12399155577	
    Loss 335.36784138904	
    Loss 335.15832542721	
    Loss 334.73916304076	
    Loss 334.80925974352	
    Loss 335.01433738323	
    Loss 334.96286689812	

==> setup_56643429_38_out.txt <==
      0
      0
      0
      0
  23640
[torch.DoubleTensor of size 9]

Validation accuracy:	0.083849235251275	
Grad norm	6.1621969096755	
    Loss 338.82185355185	
    Loss 338.96590300606	
    Loss 339.39530847035	
    Loss 339.38030838626	
    Loss 339.54517965999	
    Loss 339.86860099891	
    Loss 339.47882206233	
    Loss 338.69253364299	
    Loss 338.99938653847	
    Loss 339.65588505087	
    Loss 339.40723208174	

==> setup_56643429_39_out.txt <==
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	

==> setup_56643429_3_out.txt <==
    Loss 6781.6659004284	
    Loss 6759.6795712633	
    Loss 6737.3878237872	
    Loss 6715.7920107559	
    Loss 6694.3658301721	
    Loss 6672.9960080971	
    Loss 6651.7716129876	
    Loss 6630.0837787068	
    Loss 6608.5309114272	
    Loss 6587.1543642628	
    Loss 6566.0603093491	
    Loss 6544.4628287346	
    Loss 6523.3445629708	
    Loss 6502.4638196609	
    Loss 6481.4983407521	
    Loss 6460.4388262496	
    Loss 6439.5576851233	
    Loss 6418.7298376152	
    Loss 6398.1427384187	
    Loss 6377.3746466779	

==> setup_56643429_40_out.txt <==
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	

==> setup_56643429_41_out.txt <==
    Loss 595.87117977707	
    Loss 598.62571456166	
    Loss 603.46284934272	
    Loss 595.32282980891	
    Loss 589.75570257481	
    Loss 605.66978043589	
    Loss 590.40582155468	
    Loss 597.07751381033	
    Loss 605.40021405675	
    Loss 599.95164708705	
    Loss 583.35468299229	
    Loss 604.27047918391	
    Loss 597.9306289702	
    Loss 599.00867831768	
    Loss 596.0114871564	
    Loss 606.38145706571	
    Loss 604.50934749816	
    Loss 606.60228205264	
    Loss 600.95465194337	
    Loss 591.51218974405	

==> setup_56643429_42_out.txt <==
    Loss 5291.805616926	
    Loss 6230.6942041884	
    Loss 4493.8470643929	
    Loss 3621.7032372217	
    Loss 4741.3612789759	
    Loss 5368.5215547493	
    Loss 4512.0667994913	
    Loss 2715.9861482025	
    Loss 2944.6371921146	
    Loss 4837.8197000121	
    Loss 3484.7931429916	
    Loss 3822.3971411451	
    Loss 2928.7109846086	
    Loss 4039.9607562783	
    Loss 4055.9367450582	
    Loss 4057.1331216058	
    Loss 4793.6746109632	
    Loss 4974.7002855202	
    Loss 3130.9200073102	
    Loss 3339.6100923402	

==> setup_56643429_43_out.txt <==
    Loss 266428.11192408	
    Loss 264572.97248391	
    Loss 262730.51993286	
    Loss 260900.40460496	
    Loss 259083.64537488	
    Loss 257279.47321624	
    Loss 255487.82240114	
    Loss 253709.06049293	
    Loss 251942.70645286	
    Loss 250188.44696818	
    Loss 248446.59207775	
    Loss 246716.98205747	
    Loss 244999.52485511	
    Loss 243294.51196995	
    Loss 241600.93839564	
    Loss 239919.34665463	
    Loss 238249.35403678	
    Loss 236591.22679438	
    Loss 234944.43767877	
    Loss 233308.83017354	

==> setup_56643429_44_out.txt <==
    Loss 1471.0603998767	
    Loss 1466.0484903205	
    Loss 1459.6097072563	
    Loss 1453.2031950862	
    Loss 1448.1857091538	
    Loss 1442.8303334055	
    Loss 1437.5180011178	
    Loss 1431.7812167392	
    Loss 1426.1831220299	
    Loss 1420.5511716557	
    Loss 1415.4954021367	
    Loss 1410.4343214375	
    Loss 1406.3750464517	
    Loss 1401.9444412113	
    Loss 1396.9394420197	
    Loss 1392.7194195863	
    Loss 1388.3885474574	
    Loss 1385.2107597411	
    Loss 1380.1762251929	
    Loss 1374.8274496318	

==> setup_56643429_45_out.txt <==
    Loss 1211.8226187325	
    Loss 1213.2952398741	
    Loss 1216.6597595785	
    Loss 1215.4707141711	
    Loss 1215.1894541156	
    Loss 1215.9162236731	
    Loss 1215.9010191553	
    Loss 1216.0708695291	
    Loss 1218.1577792792	
    Loss 1215.8874051684	
    Loss 1215.0475040234	
    Loss 1216.275891229	
    Loss 1217.0380249443	
    Loss 1217.2443716559	
    Loss 1216.0371365204	
    Loss 1214.8810675408	
    Loss 1213.9696554407	
    Loss 1213.649440714	
    Loss 1213.381436925	
    Loss 1214.4488463064	

==> setup_56643429_46_out.txt <==
    Loss 1377.5508183994	
    Loss 1382.7564063047	
    Loss 1389.314700407	
    Loss 1385.2258478051	
    Loss 1385.610379496	
    Loss 1386.8689836113	
    Loss 1387.783270205	
    Loss 1388.2918566591	
    Loss 1394.2142545748	
    Loss 1389.4115802594	
    Loss 1388.7597511184	
    Loss 1392.0916202124	
    Loss 1393.3132825076	
    Loss 1394.9904885939	
    Loss 1392.2773212924	
    Loss 1389.6575813878	
    Loss 1388.1119203383	
    Loss 1387.386355954	
    Loss 1387.1183541063	
    Loss 1387.1002109153	

==> setup_56643429_47_out.txt <==
    Loss 2080.4808551522	
    Loss 2082.3055522549	
    Loss 2083.0647678053	
    Loss 2106.175880311	
    Loss 2097.2951299973	
    Loss 2097.066720706	
    Loss 2097.9159760621	
    Loss 2097.2924397909	
    Loss 2114.2027335907	
    Loss 2099.4839765103	
    Loss 2093.9571584615	
    Loss 2078.6350402862	
    Loss 2075.0470195059	
    Loss 2077.7436015986	
    Loss 2086.2830496011	
    Loss 2084.3629615956	
    Loss 2085.9180205557	
    Loss 2104.5645175452	
    Loss 2098.7824214219	
    Loss 2106.5065954844	

==> setup_56643429_48_out.txt <==
    Loss 4668.081164077	
    Loss 4645.0155239903	
    Loss 4652.3304070322	
    Loss 4649.2100196412	
    Loss 4636.022968228	
    Loss 4690.7996094278	
    Loss 4617.4406677707	
    Loss 4708.6351369926	
    Loss 4721.3788840948	
    Loss 4658.665406083	
    Loss 4730.86082106	
    Loss 4548.0044711321	
    Loss 4580.2193060782	
    Loss 4579.9896671072	
    Loss 4717.42960259	
    Loss 4742.112489451	
    Loss 4681.0772923739	
    Loss 4784.7779203228	
    Loss 4637.8980359006	
    Loss 4722.5955464944	

==> setup_56643429_49_out.txt <==
    Loss 34387.352064088	
    Loss 36158.07343453	
    Loss 37998.482338954	
    Loss 31243.283021764	
    Loss 28753.073548632	
    Loss 30759.833664678	
    Loss 32983.114267678	
    Loss 33955.958789957	
    Loss 29366.556295381	
    Loss 33353.346619101	
    Loss 33782.106918191	
    Loss 32436.306945753	
    Loss 31867.494812714	
    Loss 34499.14372104	
    Loss 31919.030712999	
    Loss 33838.642842502	
    Loss 34607.800118564	
    Loss 30190.96666528	
    Loss 31287.360646486	
    Loss 30052.554821945	

==> setup_56643429_4_out.txt <==
    Loss 599.90620734737	
    Loss 599.75362623219	
    Loss 599.68455830858	
    Loss 599.78806104479	
    Loss 599.50167729225	
    Loss 599.42786804536	
    Loss 599.44315851279	
    Loss 599.87508201301	
    Loss 600.05692735783	
    Loss 599.66974785652	
    Loss 599.47494134832	
    Loss 599.10109219103	
    Loss 599.16493843332	
    Loss 598.82167558343	
    Loss 598.80977602431	
    Loss 599.04952247703	
    Loss 599.06785533386	
    Loss 599.46719613423	
    Loss 599.53899694801	
    Loss 599.9501071098	

==> setup_56643429_50_out.txt <==
    Loss 2079.9288306428	
    Loss 2061.2465908112	
    Loss 2042.5628926298	
    Loss 2024.4975793562	
    Loss 2006.761267512	
    Loss 1988.8984174969	
    Loss 1971.2822344711	
    Loss 1954.0724614444	
    Loss 1937.2311316874	
    Loss 1920.5588441289	
    Loss 1904.5120701276	
    Loss 1888.4824111574	
    Loss 1872.6917568666	
    Loss 1856.8528228629	
    Loss 1841.1695420785	
    Loss 1826.0144441639	
    Loss 1810.8444476179	
    Loss 1796.0112142563	
    Loss 1781.1945187683	
Epoch 6	

==> setup_56643429_51_out.txt <==
    Loss 752.57146573467	
    Loss 752.01393980488	
    Loss 752.65108061366	
    Loss 753.24287490637	
    Loss 752.89256228329	
    Loss 752.75618745252	
    Loss 752.79260783087	
    Loss 752.42606198499	
    Loss 753.02853089075	
    Loss 753.20134392361	
    Loss 753.82427895586	
    Loss 753.16221566441	
    Loss 752.56377700859	
    Loss 752.47026651962	
    Loss 752.09154708748	
    Loss 752.38246691303	
    Loss 752.53399687432	
    Loss 753.0450258023	
    Loss 753.44051625568	
    Loss 753.13326831696	

==> setup_56643429_52_out.txt <==
    Loss 776.17170918143	
Epoch 11	
 131804
      0
      0
      0
      0
      0
      0
      0
      4
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10010773245933	
Grad norm	6.1777381912047	
    Loss 775.02851171398	
    Loss 773.90343948125	
    Loss 775.5277080238	
    Loss 775.96751159215	
    Loss 775.44269491955	

==> setup_56643429_53_out.txt <==
    Loss 824.31564990489	
    Loss 822.58465831445	
    Loss 823.18415905224	
    Loss 823.75367839545	
    Loss 824.6648193826	
    Loss 825.53788510078	
    Loss 824.46054377084	
    Loss 827.22629718251	
    Loss 827.76978548306	
    Loss 829.67394502309	
    Loss 828.65185560304	
    Loss 829.06676512467	
    Loss 829.83556544567	
    Loss 823.49475856041	
    Loss 825.86973646077	
    Loss 824.20576467677	
    Loss 824.89004191587	
    Loss 827.15913161196	
    Loss 829.96937515711	
    Loss 834.02097299386	

==> setup_56643429_54_out.txt <==
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	

==> setup_56643429_55_out.txt <==
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	

==> setup_56643429_56_out.txt <==
    Loss 9949.4128785793	
    Loss 12596.615426182	
    Loss 9917.4880317802	
    Loss 10625.736960175	
    Loss 17986.174486233	
    Loss 11309.063586412	
    Loss 16838.266714381	
    Loss 11840.436134365	
    Loss 9572.3552953456	
    Loss 10153.268467115	
    Loss 9749.2413466815	
    Loss 14454.265507025	
    Loss 9437.1037897265	
    Loss 12606.994313311	
    Loss 10269.166295714	
    Loss 17036.518641825	
    Loss 16426.105891846	
    Loss 12871.181617595	
    Loss 11125.877134775	
    Loss 11735.577513652	

==> setup_56643429_57_out.txt <==
    Loss 485.91598017962	
    Loss 485.98817122057	
    Loss 485.63023410637	
    Loss 485.61756895943	
    Loss 485.51090208376	
    Loss 485.63970898443	
    Loss 485.62409158421	
    Loss 485.58493210716	
    Loss 485.50755593328	
    Loss 485.33159345126	
    Loss 485.2148716326	
    Loss 485.36965141151	
    Loss 485.64561302659	
    Loss 485.66376817666	
    Loss 485.81478088632	
    Loss 485.61473553126	
    Loss 485.60233710339	
    Loss 485.73550621404	
    Loss 485.69950253677	
    Loss 485.47360390766	

==> setup_56643429_58_out.txt <==
    Loss 488.02741143209	
    Loss 487.82690303684	
    Loss 488.27428775884	
    Loss 488.41624824656	
    Loss 488.3723368373	
    Loss 488.24194131156	
    Loss 487.82126867062	
    Loss 487.40637897083	
    Loss 487.84461671116	
    Loss 488.68687152893	
    Loss 488.58660732614	
    Loss 488.84817513755	
    Loss 488.34522166623	
    Loss 488.62664883664	
    Loss 488.95193741458	
    Loss 488.7456565273	
    Loss 488.11676077948	
    Loss 488.01525703543	
    Loss 487.64898407606	
    Loss 487.74513402448	

==> setup_56643429_59_out.txt <==
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.5608851816	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	

==> setup_56643429_5_out.txt <==
    Loss 900.30704738458	
    Loss 901.49783564557	
    Loss 902.00434188594	
    Loss 904.87193694754	
    Loss 903.88552153339	
    Loss 905.02497529499	
    Loss 902.37079977501	
    Loss 898.66332842111	
    Loss 897.87887652848	
    Loss 897.2016149527	
    Loss 895.66681761193	
    Loss 901.05167748569	
    Loss 902.27796110573	
    Loss 901.1444702948	
    Loss 903.3355317744	
    Loss 899.22618364231	
    Loss 899.45240352374	
    Loss 897.97260090453	
    Loss 899.41391803812	
    Loss 898.18479343352	

==> setup_56643429_60_out.txt <==
    Loss 509.08801083942	
    Loss 509.34081153666	
    Loss 510.04751505148	
    Loss 511.01314677399	
    Loss 510.51303936095	
    Loss 508.20823357388	
    Loss 511.50476627498	
    Loss 510.02533295947	
    Loss 511.20454616368	
    Loss 511.72920497363	
    Loss 511.17824228371	
    Loss 511.21680745632	
    Loss 508.8453920749	
    Loss 506.16203391523	
    Loss 509.87950958427	
    Loss 512.56068786173	
    Loss 511.60122853601	
    Loss 511.39020659708	
    Loss 509.73915059501	
    Loss 512.09001644404	

==> setup_56643429_61_out.txt <==
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	

==> setup_56643429_62_out.txt <==
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	

==> setup_56643429_63_out.txt <==
    Loss 4174.3058208962	
    Loss 6725.9504110283	
    Loss 5283.1132486817	
    Loss 7706.3079542536	
    Loss 5434.7625286163	
    Loss 5978.1194677189	
    Loss 11378.727953238	
    Loss 4209.776997072	
    Loss 5516.830889338	
    Loss 5178.6475291788	
    Loss 6818.8109786418	
    Loss 3302.2176758259	
    Loss 4101.2244290138	
    Loss 6535.8219640936	
    Loss 3131.3703327469	
    Loss 3516.2982513972	
    Loss 9824.7824269134	
    Loss 4916.7731152058	
    Loss 4321.0422114386	
    Loss 4677.9605657876	

==> setup_56643429_6_out.txt <==
    Loss 2512.5434849847	
    Loss 2524.1300821273	
    Loss 2521.8711681698	
    Loss 2541.6900325353	
    Loss 2528.1067385408	
    Loss 2536.140669184	
    Loss 2530.8013740729	
    Loss 2515.5011624909	
    Loss 2507.6818199771	
    Loss 2494.1604335786	
    Loss 2487.5393552868	
    Loss 2525.4478986213	
    Loss 2534.7613889371	
    Loss 2534.4113402156	
    Loss 2532.455346899	
    Loss 2517.9261001768	
    Loss 2516.904412319	
    Loss 2510.4929068199	
    Loss 2515.1020345127	
    Loss 2506.065742258	

==> setup_56643429_7_out.txt <==
    Loss 20872.988670652	
    Loss 20739.166926294	
    Loss 20791.15703581	
    Loss 20526.205324801	
    Loss 20258.852803619	
    Loss 19897.694436789	
    Loss 20662.799950121	
    Loss 21745.665980544	
    Loss 21243.308062954	
    Loss 21301.554915265	
    Loss 20792.004736244	
    Loss 20896.159330052	
    Loss 20435.815591517	
    Loss 20752.971328719	
    Loss 20701.039278311	
    Loss 21194.290269847	
    Loss 19989.985048209	
    Loss 20180.855730689	
    Loss 20361.582838535	
    Loss 20138.847892826	

==> setup_56643429_8_out.txt <==
    Loss 185350.64061402	
    Loss 185090.49185757	
    Loss 184830.80503639	
    Loss 184571.3813942	
    Loss 184312.43571629	
    Loss 184053.8302645	
    Loss 183795.52737128	
    Loss 183537.55771729	
    Loss 183280.01986093	
    Loss 183022.84128352	
    Loss 182766.01141406	
    Loss 182509.56142422	
    Loss 182253.41495754	
    Loss 181997.64008263	
    Loss 181742.1700777	
    Loss 181487.24764398	
    Loss 181232.58113608	
    Loss 180978.01948311	
    Loss 180724.07088433	
    Loss 180470.52773692	

==> setup_56643429_9_out.txt <==
    Loss 18127.610040889	
    Loss 18064.962248226	
    Loss 18002.444753235	
    Loss 17940.241003665	
    Loss 17878.233668878	
    Loss 17816.474580313	
    Loss 17754.885711787	
    Loss 17693.539933823	
    Loss 17632.402023736	
    Loss 17571.56294282	
    Loss 17510.874897007	
    Loss 17450.419101379	
    Loss 17390.182049539	
    Loss 17330.100437694	
    Loss 17270.087230811	
    Loss 17210.434625254	
    Loss 17150.909586564	
    Loss 17091.749763446	
    Loss 17032.778585279	
    Loss 16974.020327564	
