[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	0.1	Lambda:	10	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4619
 1632
 2961
 3236
 2261
 4625
 2353
 3814
 1822
 2150
 2226
 6081
 6906
 4130
 2065
 1030
 1827
 4672
 5502
 1364
  799
 1672
 1583
 1841
 2349
 1954
 2061
 2328
 3020
 1010
 2782
 4599
  668
 1944
 4063
 5012
 1500
 1728
 4562
 2178
 1970
 3031
 3144
 8005
 2729
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02543851663025	
Grad norm	0	
    Loss 22763792.696206	
    Loss 22446539.601553	
    Loss 22133743.550001	
    Loss 21825311.893211	
    Loss 21521184.810251	
    Loss 21221289.466834	
    Loss 20925575.870543	
    Loss 20633990.090079	
    Loss 20346461.789085	
    Loss 20062940.951753	
    Loss 19783372.18381	
    Loss 19507696.412983	
    Loss 19235863.049251	
    Loss 18967819.361548	
    Loss 18703510.898219	
    Loss 18442889.833701	
    Loss 18185895.62745	
    Loss 17932484.341619	
    Loss 17682607.939509	
    Loss 17436208.573273	
    Loss 17193240.204595	
    Loss 16953661.495614	
    Loss 16717419.901965	
    Loss 16484470.561375	
    Loss 16254771.256995	
    Loss 16028273.846029	
    Loss 15804933.256003	
    Loss 15584704.492476	
    Loss 15367543.582819	
    Loss 15153409.388852	
    Loss 14942256.048652	
    Loss 14734048.805507	
    Loss 14528740.754019	
    Loss 14326293.86223	
    Loss 14126670.306388	
    Loss 13929825.998425	
    Loss 13735725.281418	
    Loss 13544331.068755	
    Loss 13355600.441528	
    Loss 13169501.955436	
    Loss 12985998.103311	
    Loss 12805047.32301	
    Loss 12626620.002242	
    Loss 12450681.234628	
    Loss 12277189.592361	
    Loss 12106118.130018	
    Loss 11937430.714578	
    Loss 11771097.146094	
    Loss 11607080.564162	
    Loss 11445353.078787	
    Loss 11285873.13131	
    Loss 11128618.561561	
    Loss 10973554.755694	
    Loss 10820652.39	
    Loss 10669878.853339	
    Loss 10521207.834244	
    Loss 10374608.953696	
    Loss 10230053.004811	
    Loss 10087511.745156	
    Loss 9946955.5229726	
    Loss 9808359.9118228	
    Loss 9671694.7832168	
    Loss 9536934.3843624	
    Loss 9404049.8008946	
    Loss 9273018.6965039	
    Loss 9143812.7475046	
    Loss 9016406.6502995	
    Loss 8890775.4754395	
    Loss 8766897.1894376	
    Loss 8644744.8952351	
    Loss 8524295.7990621	
    Loss 8405523.7380245	
    Loss 8288407.2595219	
    Loss 8172922.2311782	
    Loss 8059047.546754	
    Loss 7946757.2599866	
    Loss 7836031.8128379	
    Loss 7726850.4228071	
    Loss 7619192.2943563	
    Loss 7513032.373529	
    Loss 7408352.8120422	
    Loss 7305132.2175591	
    Loss 7203350.604791	
    Loss 7102985.5591807	
    Loss 7004020.0143735	
    Loss 6906433.2644013	
    Loss 6810206.7966015	
    Loss 6715321.2395839	
    Loss 6621756.3291213	
    Loss 6529494.4634076	
    Loss 6438520.3591063	
    Loss 6348813.2179734	
    Loss 6260356.8465204	
    Loss 6173132.4373517	
    Loss 6087124.9652662	
    Loss 6002314.4063795	
    Loss 5918683.9177736	
    Loss 5836223.0219632	
    Loss 5754910.321174	
    Loss 5674728.035146	
    Loss 5595663.2134165	
    Loss 5517702.0100901	
    Loss 5440826.6205781	
    Loss 5365022.5436168	
    Loss 5290274.5630419	
    Loss 5216568.6598348	
    Loss 5143889.6130219	
    Loss 5072222.4653457	
    Loss 5001554.9646808	
    Loss 4931871.8173724	
    Loss 4863158.8388642	
    Loss 4795405.2261506	
    Loss 4728595.378225	
    Loss 4662717.2391023	
    Loss 4597755.0282818	
    Loss 4533698.5811681	
    Loss 4470534.8076942	
    Loss 4408250.9860475	
    Loss 4346835.0423542	
    Loss 4286274.9904629	
    Loss 4226560.3651734	
    Loss 4167676.4384873	
    Loss 4109612.6785055	
    Loss 4052358.8025821	
    Loss 3995902.323733	
    Loss 3940234.0329956	
    Loss 3885339.8747535	
    Loss 3831210.344381	
    Loss 3777835.4642327	
    Loss 3725204.9631121	
    Loss 3673309.0282535	
    Loss 3622135.8350577	
    Loss 3571675.9588193	
    Loss 3521918.964145	
    Loss 3472854.6012271	
    Loss 3424473.4979654	
    Loss 3376766.8303145	
    Loss 3329724.2641462	
    Loss 3283337.1998378	
    Loss 3237596.2120073	
    Loss 3192493.4534764	
    Loss 3148018.1045084	
    Loss 3104163.9526025	
Epoch 2	
 91275
  2488
   220
    29
  8623
     2
     0
  4786
 21865
  2250
   234
     3
     1
     3
     1
    26
     1
     1
[torch.DoubleTensor of size 18]

Validation accuracy:	0.10194373634377	
Grad norm	941.72634268258	
    Loss 3078146.2587581	
    Loss 3035264.9887836	
    Loss 2992982.3213727	
    Loss 2951287.4798996	
    Loss 2910175.1741559	
    Loss 2869634.3663322	
    Loss 2829658.8086703	
    Loss 2790241.3686388	
    Loss 2751372.044086	
    Loss 2713044.5357822	
    Loss 2675251.5320633	
    Loss 2637984.1814891	
    Loss 2601236.604236	
    Loss 2565001.4286825	
    Loss 2529271.5146519	
    Loss 2494039.5686899	
    Loss 2459297.569171	
    Loss 2425040.394442	
    Loss 2391261.1823893	
    Loss 2357951.1664789	
    Loss 2325104.2962811	
    Loss 2292716.2516235	
    Loss 2260779.8128525	
    Loss 2229288.4178685	
    Loss 2198236.527819	
    Loss 2167617.0087298	
    Loss 2137424.3070651	
    Loss 2107652.4454065	
    Loss 2078294.8967571	
    Loss 2049346.5888398	
    Loss 2020800.8560564	
    Loss 1992654.0346471	
    Loss 1964898.838947	
    Loss 1937530.2284997	
    Loss 1910543.5823911	
    Loss 1883932.0500207	
    Loss 1857691.5177414	
    Loss 1831816.9549818	
    Loss 1806302.3564979	
    Loss 1781143.6767942	
    Loss 1756335.7178172	
    Loss 1731872.7724528	
    Loss 1707750.9322061	
    Loss 1683965.6302585	
    Loss 1660510.6351651	
    Loss 1637382.5807538	
    Loss 1614577.549703	
    Loss 1592090.9535803	
    Loss 1569917.3639767	
    Loss 1548053.9454449	
    Loss 1526492.8997453	
    Loss 1505234.0041356	
    Loss 1484270.2642279	
    Loss 1463599.1984919	
    Loss 1443216.0232069	
    Loss 1423117.2847488	
    Loss 1403298.0650361	
    Loss 1383755.1668739	
    Loss 1364484.8476076	
    Loss 1345482.7320837	
    Loss 1326745.5991777	
    Loss 1308269.1507853	
    Loss 1290050.6920123	
    Loss 1272085.2836254	
    Loss 1254370.7720394	
    Loss 1236902.8486011	
    Loss 1219678.478813	
    Loss 1202693.5473256	
    Loss 1185946.1920428	
    Loss 1169431.8117074	
    Loss 1153148.0974436	
    Loss 1137090.6195722	
    Loss 1121256.7855428	
    Loss 1105643.7271519	
    Loss 1090248.2898732	
    Loss 1075067.2916627	
    Loss 1060097.4020564	
    Loss 1045336.8049102	
    Loss 1030782.0740347	
    Loss 1016429.8665565	
    Loss 1002277.3304969	
    Loss 988322.48884064	
    Loss 974561.79114024	
    Loss 960992.63850451	
    Loss 947613.01997883	
    Loss 934419.51433396	
    Loss 921409.64793469	
    Loss 908581.26340382	
    Loss 895931.38814096	
    Loss 883457.93323151	
    Loss 871159.09442184	
    Loss 859030.62145124	
    Loss 847071.99206488	
    Loss 835279.44547032	
    Loss 823651.26181286	
    Loss 812185.04274875	
    Loss 800878.03216109	
    Loss 789729.52813979	
    Loss 778736.33861665	
    Loss 767895.86218661	
    Loss 757206.52679313	
    Loss 746666.37159149	
    Loss 736272.9684357	
    Loss 726024.4831755	
    Loss 715918.52631014	
    Loss 705953.49986616	
    Loss 696127.4773702	
    Loss 686438.03974627	
    Loss 676883.87405098	
    Loss 667462.26079049	
    Loss 658172.28499702	
    Loss 649011.99812832	
    Loss 639979.29063841	
    Loss 631073.09449845	
    Loss 622290.76069946	
    Loss 613630.70716952	
    Loss 605091.07746775	
    Loss 596670.06883556	
    Loss 588366.47751637	
    Loss 580178.40322764	
    Loss 572105.42251017	
    Loss 564144.43809444	
    Loss 556294.05625962	
    Loss 548553.3407958	
    Loss 540920.16884548	
    Loss 533393.6841203	
    Loss 525971.66195481	
    Loss 518653.37325539	
    Loss 511437.08030466	
    Loss 504321.16209392	
    Loss 497304.73159877	
    Loss 490386.06559444	
    Loss 483563.92838066	
    Loss 476836.81163414	
    Loss 470203.45080586	
    Loss 463662.33429477	
    Loss 457212.52647623	
    Loss 450852.03195912	
    Loss 444580.29328577	
    Loss 438395.97134688	
    Loss 432297.84950281	
    Loss 426284.51022056	
    Loss 420355.22422759	
Epoch 3	
 128445
      2
      0
      0
    238
      0
      0
     40
   3083
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10114712308813	
Grad norm	349.50641824602	
    Loss 416837.43117439	
    Loss 411039.55052013	
    Loss 405323.08008757	
    Loss 399685.9704443	
    Loss 394127.64667133	
    Loss 388646.24332375	
    Loss 383241.38589964	
    Loss 377912.13394341	
    Loss 372656.98381134	
    Loss 367474.93543432	
    Loss 362365.59387817	
    Loss 357326.55352337	
    Loss 352357.83993354	
    Loss 347458.45606445	
    Loss 342627.53451706	
    Loss 337864.19237449	
    Loss 333166.82402539	
    Loss 328535.29704131	
    Loss 323968.3482124	
    Loss 319464.52585934	
    Loss 315023.25830236	
    Loss 310644.31427612	
    Loss 306326.60123866	
    Loss 302068.93961539	
    Loss 297870.65063305	
    Loss 293730.61075452	
    Loss 289648.29726584	
    Loss 285623.00544818	
    Loss 281653.68557385	
    Loss 277739.70226336	
    Loss 273880.20992717	
    Loss 270074.82542432	
    Loss 266322.25378499	
    Loss 262621.89587197	
    Loss 258973.25429427	
    Loss 255374.99662316	
    Loss 251826.93303119	
    Loss 248328.27053864	
    Loss 244878.67233663	
    Loss 241476.95257729	
    Loss 238122.65706631	
    Loss 234815.12186263	
    Loss 231553.50237691	
    Loss 228337.34708378	
    Loss 225165.9935133	
    Loss 222038.53241318	
    Loss 218955.14314167	
    Loss 215914.83214626	
    Loss 212916.62013221	
    Loss 209960.64027743	
    Loss 207044.99132776	
    Loss 204170.99456069	
    Loss 201336.03766744	
    Loss 198541.19001518	
    Loss 195785.38197202	
    Loss 193068.23611504	
    Loss 190388.53072013	
    Loss 187746.10187429	
    Loss 185140.68381453	
    Loss 182571.63597072	
    Loss 180038.18069701	
    Loss 177539.84653298	
    Loss 175076.56180875	
    Loss 172647.20124135	
    Loss 170252.14563062	
    Loss 167890.25538872	
    Loss 165561.54477144	
    Loss 163264.86669572	
    Loss 161000.65454101	
    Loss 158767.68469538	
    Loss 156565.99110606	
    Loss 154394.88057468	
    Loss 152253.72143592	
    Loss 150142.53591651	
    Loss 148060.77077117	
    Loss 146008.28589217	
    Loss 143984.05508391	
    Loss 141988.46197533	
    Loss 140020.64396485	
    Loss 138080.32063052	
    Loss 136166.5900579	
    Loss 134279.92208767	
    Loss 132419.13047643	
    Loss 130584.4489977	
    Loss 128775.44502227	
    Loss 126991.37464846	
    Loss 125231.82852614	
    Loss 123497.19250875	
    Loss 121786.58535429	
    Loss 120100.17131842	
    Loss 118437.74081261	
    Loss 116797.58974624	
    Loss 115181.01709624	
    Loss 113586.52736699	
    Loss 112013.87235595	
    Loss 110463.4101457	
    Loss 108934.50934888	
    Loss 107426.95904316	
    Loss 105940.62862258	
    Loss 104474.8939058	
    Loss 103029.65005405	
    Loss 101604.57216494	
    Loss 100199.30737102	
    Loss 98813.718759503	
    Loss 97447.224692931	
    Loss 96099.808555095	
    Loss 94771.298791455	
    Loss 93461.171277446	
    Loss 92169.40134389	
    Loss 90894.985703564	
    Loss 89638.867590902	
    Loss 88400.142468933	
    Loss 87178.801950836	
    Loss 85975.011672676	
    Loss 84788.105094281	
    Loss 83617.521406672	
    Loss 82462.889801366	
    Loss 81324.020692383	
    Loss 80201.222882087	
    Loss 79093.834552045	
    Loss 78002.58872825	
    Loss 76926.290734875	
    Loss 75864.699299757	
    Loss 74818.05890417	
    Loss 73785.767268577	
    Loss 72768.007332581	
    Loss 71764.168195883	
    Loss 70774.686003476	
    Loss 69799.030481958	
    Loss 68836.6344873	
    Loss 67887.742212595	
    Loss 66952.190650573	
    Loss 66029.782933278	
    Loss 65120.199297225	
    Loss 64223.56046278	
    Loss 63339.227921505	
    Loss 62467.286052251	
    Loss 61607.123199761	
    Loss 60758.958450069	
    Loss 59922.817748626	
    Loss 59098.169492687	
    Loss 58285.05373415	
    Loss 57483.207648128	
Epoch 4	
 131806
      0
      0
      0
      0
      0
      0
      0
      2
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10011531925225	
Grad norm	131.80327895089	
    Loss 57007.374589686	
    Loss 56223.258797688	
    Loss 55450.484067665	
    Loss 54688.498555782	
    Loss 53936.972921517	
    Loss 53195.712712316	
    Loss 52464.849598448	
    Loss 51744.262049494	
    Loss 51033.883052436	
    Loss 50333.260302985	
    Loss 49642.729321896	
    Loss 48961.116721186	
    Loss 48289.000379149	
    Loss 47626.325381464	
    Loss 46972.981817683	
    Loss 46329.01743991	
    Loss 45693.82156781	
    Loss 45067.682880826	
    Loss 44450.26993985	
    Loss 43841.179661388	
    Loss 43240.560778548	
    Loss 42648.588730919	
    Loss 42064.983066973	
    Loss 41489.399569793	
    Loss 40921.785957577	
    Loss 40361.880168037	
    Loss 39809.814630379	
    Loss 39265.508982946	
    Loss 38728.770921028	
    Loss 38199.542159359	
    Loss 37677.848464593	
    Loss 37163.441124495	
    Loss 36656.136145253	
    Loss 36155.844012538	
    Loss 35662.539180154	
    Loss 35175.793419912	
    Loss 34695.877888134	
    Loss 34222.546725243	
    Loss 33756.293769444	
    Loss 33296.21600133	
    Loss 32842.580131996	
    Loss 32395.461627694	
    Loss 31954.280475749	
    Loss 31519.187187231	
    Loss 31090.372221271	
    Loss 30667.232773805	
    Loss 30250.334169054	
    Loss 29839.237339734	
    Loss 29433.664135123	
    Loss 29033.937815975	
    Loss 28639.379080826	
    Loss 28251.036209242	
    Loss 27867.236310517	
    Loss 27489.318669185	
    Loss 27116.822693727	
    Loss 26749.719763969	
    Loss 26387.463439102	
    Loss 26030.072352001	
    Loss 25677.827162492	
    Loss 25330.699791013	
    Loss 24988.128301473	
    Loss 24650.203249725	
    Loss 24317.080258985	
    Loss 23988.329286651	
    Loss 23664.590945399	
    Loss 23345.182351174	
    Loss 23030.487857762	
    Loss 22719.863325203	
    Loss 22413.850225471	
    Loss 22111.817237565	
    Loss 21814.072348546	
    Loss 21520.584576608	
    Loss 21230.82588915	
    Loss 20945.185884176	
    Loss 20663.575100299	
    Loss 20386.214927296	
    Loss 20112.440166367	
    Loss 19842.795979204	
    Loss 19576.809453981	
    Loss 19314.701207457	
    Loss 19055.827598946	
    Loss 18800.876058551	
    Loss 18549.106044259	
    Loss 18301.100207186	
    Loss 18056.549549283	
    Loss 17815.160820182	
    Loss 17576.761614634	
    Loss 17342.12891371	
    Loss 17110.625858652	
    Loss 16882.751878333	
    Loss 16658.423779206	
    Loss 16436.440239239	
    Loss 16218.217738889	
    Loss 16002.635842845	
    Loss 15789.542571453	
    Loss 15579.778127734	
    Loss 15373.080856227	
    Loss 15169.056785368	
    Loss 14968.141471554	
    Loss 14769.980935671	
    Loss 14574.62766388	
    Loss 14382.016107134	
    Loss 14192.051150757	
    Loss 14004.839447482	
    Loss 13820.040837576	
    Loss 13637.854779812	
    Loss 13458.297399198	
    Loss 13281.197227151	
    Loss 13106.608408604	
    Loss 12933.804242467	
    Loss 12763.968610146	
    Loss 12596.297048759	
    Loss 12431.157512675	
    Loss 12268.791924104	
    Loss 12108.893666226	
    Loss 11950.965679334	
    Loss 11794.856324298	
    Loss 11640.615559945	
    Loss 11488.788112388	
    Loss 11338.816933101	
    Loss 11191.551318339	
    Loss 11046.140978647	
    Loss 10902.492863936	
    Loss 10760.971771746	
    Loss 10621.210790286	
    Loss 10483.520843161	
    Loss 10347.494434754	
    Loss 10213.727427694	
    Loss 10081.889375969	
    Loss 9951.5218458369	
    Loss 9822.9926192231	
    Loss 9696.4055208578	
    Loss 9571.7028296744	
    Loss 9448.6855493821	
    Loss 9327.7435285537	
    Loss 9208.2866165757	
    Loss 9090.5234967231	
    Loss 8974.1065328912	
    Loss 8859.267657142	
    Loss 8746.3097013759	
    Loss 8634.7162887237	
    Loss 8524.783986172	
    Loss 8416.2254060795	
Epoch 5	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	51.797755764773	
    Loss 8351.6937725231	
    Loss 8245.5229391318	
    Loss 8141.1718292038	
    Loss 8038.3910450154	
    Loss 7936.762064079	
    Loss 7836.4605468848	
    Loss 7737.5826535665	
    Loss 7640.0964063994	
    Loss 7544.259424275	
    Loss 7449.6222121868	
    Loss 7356.5485390086	
    Loss 7264.1580258754	
    Loss 7173.0134444084	
    Loss 7083.2193636139	
    Loss 6994.7392365568	
    Loss 6907.7547921132	
    Loss 6821.8433352969	
    Loss 6737.2644310889	
    Loss 6653.8707663311	
    Loss 6571.4185540209	
    Loss 6490.1555332273	
    Loss 6410.2575840887	
    Loss 6331.548301061	
    Loss 6253.8319694824	
    Loss 6177.1339301296	
    Loss 6101.3220395771	
    Loss 6026.6150783865	
    Loss 5952.9769479365	
    Loss 5880.3748636567	
    Loss 5808.8218060338	
    Loss 5738.4910071723	
    Loss 5669.0452051214	
    Loss 5600.5557534893	
    Loss 5532.970563012	
    Loss 5466.2915617771	
    Loss 5400.2754067211	
    Loss 5335.2284874959	
    Loss 5270.9712312445	
    Loss 5208.1279450813	
    Loss 5145.8013038876	
    Loss 5084.384800024	
    Loss 5024.0762923593	
    Loss 4964.2752040672	
    Loss 4905.2304052459	
    Loss 4847.2765455027	
    Loss 4789.8522357754	
    Loss 4733.5101382792	
    Loss 4677.9149824407	
    Loss 4622.916504873	
    Loss 4568.7864870974	
    Loss 4515.1637625152	
    Loss 4462.8900808752	
    Loss 4410.541818673	
    Loss 4359.4150149611	
    Loss 4309.1782019675	
    Loss 4259.8160861484	
    Loss 4210.9441809863	
    Loss 4162.5247365644	
    Loss 4114.9344279462	
    Loss 4068.269440857	
    Loss 4021.9542260109	
    Loss 3976.1728126205	
    Loss 3931.0671548997	
    Loss 3886.3620347285	
    Loss 3842.7061956071	
    Loss 3799.489372828	
    Loss 3757.1270944371	
    Loss 3715.0838805999	
    Loss 3673.841405819	
    Loss 3632.8937534933	
    Loss 3592.5817373232	
    Loss 3553.0098043804	
    Loss 3513.6057566313	
    Loss 3474.804870149	
    Loss 3436.6199502006	
    Loss 3399.3027004351	
    Loss 3362.2534078197	
    Loss 3325.9918375801	
    Loss 3290.1123642567	
    Loss 3254.9363537478	
    Loss 3219.8521225048	
    Loss 3185.5344357046	
    Loss 3151.3507243281	
    Loss 3117.9159252781	
    Loss 3084.9092253428	
    Loss 3052.1270692229	
    Loss 3019.4148421985	
    Loss 2987.6170091342	
    Loss 2956.1212015999	
    Loss 2925.4797885905	
    Loss 2895.5930314981	
    Loss 2865.3879480431	
    Loss 2836.2513234399	
    Loss 2807.1323068644	
    Loss 2777.8622492641	
    Loss 2749.3803435702	
    Loss 2721.5056369894	
    Loss 2693.7333518677	
    Loss 2666.6273867456	
    Loss 2639.8705967808	
    Loss 2613.5222238795	
    Loss 2587.5677752852	
    Loss 2561.9376011734	
    Loss 2536.7763906393	
    Loss 2511.7739288964	
    Loss 2487.1624278081	
    Loss 2462.9617343659	
    Loss 2439.0860602977	
    Loss 2415.5653033841	
    Loss 2391.7285183848	
    Loss 2368.7836186466	
    Loss 2345.9270291913	
    Loss 2323.6114226202	
    Loss 2302.0549623103	
    Loss 2281.0350970669	
    Loss 2260.034845032	
    Loss 2238.9396702528	
    Loss 2217.8380149797	
    Loss 2197.3190433734	
    Loss 2176.8172483988	
    Loss 2157.1862669782	
    Loss 2137.648657731	
    Loss 2118.1303406872	
    Loss 2098.9993797257	
    Loss 2079.9288306428	
    Loss 2061.2465908112	
    Loss 2042.5628926298	
    Loss 2024.4975793562	
    Loss 2006.761267512	
    Loss 1988.8984174969	
    Loss 1971.2822344711	
    Loss 1954.0724614444	
    Loss 1937.2311316874	
    Loss 1920.5588441289	
    Loss 1904.5120701276	
    Loss 1888.4824111574	
    Loss 1872.6917568666	
    Loss 1856.8528228629	
    Loss 1841.1695420785	
    Loss 1826.0144441639	
    Loss 1810.8444476179	
    Loss 1796.0112142563	
    Loss 1781.1945187683	
Epoch 6	
