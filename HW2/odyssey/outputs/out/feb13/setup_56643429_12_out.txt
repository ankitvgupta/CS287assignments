[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	3	Lambda:	1	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3880
 5335
 2315
 3512
 3795
 1977
  994
 7418
 4523
 1750
 1461
  790
 1909
 5390
 1138
 3983
 6885
 2349
 2004
 3208
 2424
 1871
 1043
 1283
 1158
 1534
 2339
 5772
 1508
 2627
 3159
 4510
 3036
  965
 4985
 3902
 5441
  679
 3544
 4324
 1366
  901
 1776
 4710
 2335
[torch.DoubleTensor of size 45]

Validation accuracy:	0.029019482884195	
Grad norm	0	
    Loss 2274645.1020263	
    Loss 2180466.8559023	
    Loss 2090238.5030325	
    Loss 2003756.9230061	
    Loss 1920864.335705	
    Loss 1841411.1340839	
    Loss 1765249.191621	
    Loss 1692239.2962311	
    Loss 1622259.6131708	
    Loss 1555180.8828249	
    Loss 1490870.2954104	
    Loss 1429234.2108333	
    Loss 1370153.8832607	
    Loss 1313523.0178077	
    Loss 1259228.7180312	
    Loss 1207171.8346009	
    Loss 1157278.1064798	
    Loss 1109446.9979969	
    Loss 1063593.9951278	
    Loss 1019642.6514344	
    Loss 977493.97063384	
    Loss 937092.80577788	
    Loss 898369.16825268	
    Loss 861241.76097491	
    Loss 825654.16289881	
    Loss 791542.95735453	
    Loss 758843.95161526	
    Loss 727494.38485838	
    Loss 697441.17565598	
    Loss 668632.3216818	
    Loss 641010.59101066	
    Loss 614528.5884635	
    Loss 589144.57906662	
    Loss 564803.24287541	
    Loss 541471.95240551	
    Loss 519108.72776274	
    Loss 497669.37144266	
    Loss 477121.67370025	
    Loss 457417.10649453	
    Loss 438529.45162635	
    Loss 420421.82195595	
    Loss 403056.67174286	
    Loss 386415.56436895	
    Loss 370464.18237847	
    Loss 355169.83034892	
    Loss 340507.322644	
    Loss 326451.33741589	
    Loss 312978.22078544	
    Loss 300064.55666704	
    Loss 287682.45590203	
    Loss 275813.51882189	
    Loss 264430.56933092	
    Loss 253520.14153884	
    Loss 243060.07711704	
    Loss 233031.43983076	
    Loss 223416.02481323	
    Loss 214200.263589	
    Loss 205363.96447455	
    Loss 196898.7463572	
    Loss 188779.14139506	
    Loss 180996.2372387	
    Loss 173534.31671915	
    Loss 166379.35311585	
    Loss 159522.15431738	
    Loss 152946.38963343	
    Loss 146642.20870216	
    Loss 140599.36947581	
    Loss 134805.24911792	
    Loss 129249.26293293	
    Loss 123923.50655916	
    Loss 118819.23352257	
    Loss 113926.2453777	
    Loss 109237.29383673	
    Loss 104740.04957912	
    Loss 100429.50015042	
    Loss 96296.717472256	
    Loss 92333.485570972	
    Loss 88534.606455658	
    Loss 84892.498703548	
    Loss 81402.169097213	
    Loss 78053.524450065	
    Loss 74843.85511572	
    Loss 71767.671092953	
    Loss 68819.506579733	
    Loss 65993.611562437	
    Loss 63279.841030396	
    Loss 60682.603914311	
    Loss 58191.90411529	
    Loss 55802.231853185	
    Loss 53512.980023368	
    Loss 51318.172041421	
    Loss 49213.745143248	
    Loss 47197.238178089	
    Loss 45263.872228643	
    Loss 43408.127109166	
    Loss 41630.624609979	
    Loss 39925.516832534	
    Loss 38291.16450295	
    Loss 36725.653862849	
    Loss 35223.418977216	
    Loss 33784.516217427	
    Loss 32405.698809791	
    Loss 31082.436197392	
    Loss 29814.089046108	
    Loss 28598.623285686	
    Loss 27432.944882799	
    Loss 26314.346564297	
    Loss 25243.251811794	
    Loss 24215.92421863	
    Loss 23229.367736031	
    Loss 22285.874658626	
    Loss 21379.854467675	
    Loss 20512.192783719	
    Loss 19680.727391876	
    Loss 18883.546946334	
    Loss 18120.660215028	
    Loss 17386.616326676	
    Loss 16684.80144072	
    Loss 16010.819916075	
    Loss 15364.289310963	
    Loss 14744.963939957	
    Loss 14151.956859088	
    Loss 13583.20588071	
    Loss 13037.857677076	
    Loss 12514.5530521	
    Loss 12012.598559559	
    Loss 11531.030482372	
    Loss 11069.119539514	
    Loss 10628.492341772	
    Loss 10204.414135571	
    Loss 9797.5989305712	
    Loss 9408.0890428283	
    Loss 9033.6642099949	
    Loss 8674.8349750343	
    Loss 8332.9014054724	
    Loss 8003.6722734537	
    Loss 7688.9095314123	
    Loss 7386.1225890357	
    Loss 7097.6278557968	
    Loss 6819.7854825059	
    Loss 6553.4109635564	
    Loss 6298.3674090442	
    Loss 6053.9510214449	
Epoch 2	
 117791
      8
      0
      0
   2189
      0
      0
    131
  11680
      9
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10235342316096	
Grad norm	17.979808404304	
    Loss 5910.5497741698	
    Loss 5683.0839756885	
    Loss 5466.0328226339	
    Loss 5255.6053387847	
    Loss 5054.6794286153	
    Loss 4860.8424812562	
    Loss 4676.1001447882	
    Loss 4498.1158704628	
    Loss 4329.5157026485	
    Loss 4166.3915866156	
    Loss 4010.676057343	
    Loss 3860.5992494643	
    Loss 3715.4886539572	
    Loss 3577.2586900616	
    Loss 3445.7740112105	
    Loss 3318.1014641148	
    Loss 3198.3619365937	
    Loss 3082.9740009578	
    Loss 2971.4361722923	
    Loss 2864.0622585157	
    Loss 2761.6218432441	
    Loss 2663.7548591416	
    Loss 2569.8184358328	
    Loss 2479.4347328645	
    Loss 2391.7019193362	
    Loss 2309.4675907849	
    Loss 2229.1237722338	
    Loss 2153.0969436187	
    Loss 2080.1076528442	
    Loss 2009.4424573608	
    Loss 1943.1477146122	
    Loss 1877.4067105022	
    Loss 1815.6923114172	
    Loss 1756.0662252375	
    Loss 1699.2736830806	
    Loss 1645.5241920976	
    Loss 1593.196658659	
    Loss 1543.5691963343	
    Loss 1495.4112195218	
    Loss 1449.0923648828	
    Loss 1405.7681466355	
    Loss 1362.4942331661	
    Loss 1320.3535026106	
    Loss 1281.9840138653	
    Loss 1245.8205064109	
    Loss 1210.1554048977	
    Loss 1175.6144065697	
    Loss 1142.8348447241	
    Loss 1110.7395138965	
    Loss 1079.822439344	
    Loss 1050.3264506979	
    Loss 1023.3697757771	
    Loss 995.3306564636	
    Loss 970.80967768355	
    Loss 946.63847301683	
    Loss 924.14619063233	
    Loss 901.31898255883	
    Loss 880.02385685732	
    Loss 860.82090272537	
    Loss 842.2881436914	
    Loss 822.61429692415	
    Loss 805.08805301355	
    Loss 788.41248476122	
    Loss 770.41365712855	
    Loss 754.3398022653	
    Loss 738.71664103078	
    Loss 723.92039817846	
    Loss 709.9097671842	
    Loss 694.57416579179	
    Loss 682.22470841055	
    Loss 670.06643803949	
    Loss 657.95260183472	
    Loss 646.57711400404	
    Loss 635.75694473826	
    Loss 625.71825956602	
    Loss 615.28439263473	
    Loss 605.18844591017	
    Loss 595.19093734215	
    Loss 586.07766726876	
    Loss 579.59378023336	
    Loss 570.0414931637	
    Loss 562.39886599145	
    Loss 553.6824025454	
    Loss 547.04469887457	
    Loss 542.71931242833	
    Loss 533.29635539129	
    Loss 526.75051909933	
    Loss 521.21890488383	
    Loss 514.38133145424	
    Loss 509.2298643136	
    Loss 504.80536758229	
    Loss 499.39047746277	
    Loss 495.2478547801	
    Loss 490.45753699192	
    Loss 484.5742345819	
    Loss 479.76072176114	
    Loss 475.21339443679	
    Loss 469.74412830354	
    Loss 466.65131664068	
    Loss 462.81138156702	
    Loss 459.86431230321	
    Loss 457.24756199758	
    Loss 453.83711853263	
    Loss 451.04835554667	
    Loss 449.50180018369	
    Loss 447.15903184887	
    Loss 443.56966350196	
    Loss 441.121865036	
    Loss 438.63747610283	
    Loss 433.62809678611	
    Loss 432.11829322562	
    Loss 429.64552433226	
    Loss 427.12439089352	
    Loss 425.74667392616	
    Loss 424.20708425756	
    Loss 424.76762091812	
    Loss 421.91880767443	
    Loss 421.23944322796	
    Loss 419.60108898066	
    Loss 417.28447722514	
    Loss 415.5312839795	
    Loss 414.60345000277	
    Loss 413.20577206563	
    Loss 412.12690351078	
    Loss 410.52420704308	
    Loss 409.08183899658	
    Loss 407.21368805532	
    Loss 405.54562462642	
    Loss 405.66229331109	
    Loss 404.35814267774	
    Loss 402.79129707341	
    Loss 401.74096452419	
    Loss 399.88039207293	
    Loss 397.86186493506	
    Loss 398.04517161732	
    Loss 396.9127699864	
    Loss 396.61879823068	
    Loss 395.68390493416	
    Loss 396.40063072483	
    Loss 395.71729770176	
    Loss 394.9661030105	
    Loss 394.66096229098	
    Loss 394.50471607219	
Epoch 3	
 118199
     38
      0
      0
   2105
      0
      0
    121
  11334
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10220168730274	
Grad norm	6.4653590308685	
    Loss 392.85584882904	
    Loss 393.20258445286	
    Loss 394.32857787193	
    Loss 393.32492711293	
    Loss 393.13253271628	
    Loss 391.90808459074	
    Loss 391.7196342985	
    Loss 390.80666833722	
    Loss 391.65427626866	
    Loss 391.18437026879	
    Loss 391.53471574489	
    Loss 390.88539253699	
    Loss 388.84152580646	
    Loss 387.76812629442	
    Loss 387.91175648625	
    Loss 386.54646053322	
    Loss 387.710287524	
    Loss 388.26718310525	
    Loss 388.05529901132	
    Loss 387.31461935097	
    Loss 387.14371855946	
    Loss 387.45882657469	
    Loss 387.38868307392	
    Loss 387.23543781269	
    Loss 385.88201771742	
    Loss 386.42240491972	
    Loss 385.44104811347	
    Loss 385.53314772367	
    Loss 385.53980370637	
    Loss 384.81611360181	
    Loss 385.59934317924	
    Loss 384.08781352678	
    Loss 383.98669261016	
    Loss 383.63877708974	
    Loss 383.57651414453	
    Loss 384.08388530306	
    Loss 383.86126842387	
    Loss 384.09019185603	
    Loss 383.77454475642	
    Loss 383.31956328894	
    Loss 384.02197246926	
    Loss 383.01935746173	
    Loss 381.27742492445	
    Loss 381.68423826862	
    Loss 382.68073941422	
    Loss 382.70996086212	
    Loss 382.34999668206	
    Loss 382.31941948415	
    Loss 381.4856129522	
    Loss 380.67672541621	
    Loss 379.97070018007	
    Loss 380.71835550665	
    Loss 379.27049173361	
    Loss 380.22786047925	
    Loss 380.52053245338	
    Loss 381.42786669591	
    Loss 381.00875122195	
    Loss 381.27095290737	
    Loss 382.57130027642	
    Loss 383.82315396569	
    Loss 383.06404721189	
    Loss 383.71810672215	
    Loss 384.50990225792	
    Loss 383.14978582961	
    Loss 383.08090740522	
    Loss 382.82005346913	
    Loss 382.6865217711	
    Loss 382.80940557712	
    Loss 380.98330228034	
    Loss 381.62176830217	
    Loss 381.93706098054	
    Loss 381.72363052397	
    Loss 381.72851652312	
    Loss 381.88474352308	
    Loss 382.33483821656	
    Loss 381.92501128164	
    Loss 381.50838303352	
    Loss 380.75583987244	
    Loss 380.49977719233	
    Loss 382.53008748561	
    Loss 381.12336413292	
    Loss 381.3163332218	
    Loss 380.06298679618	
    Loss 380.57559809057	
    Loss 383.17825840817	
    Loss 380.394989821	
    Loss 380.14903745059	
    Loss 380.69825480735	
    Loss 379.69619908106	
    Loss 380.10148910686	
    Loss 381.02298596335	
    Loss 380.72717962285	
    Loss 381.51086166061	
    Loss 381.4208445312	
    Loss 380.06876531193	
    Loss 379.57069699683	
    Loss 379.18866362354	
    Loss 377.68466734058	
    Loss 378.40462958233	
    Loss 378.24348934405	
    Loss 378.78281480165	
    Loss 379.50266928697	
    Loss 379.30849034895	
    Loss 379.62145988983	
    Loss 381.06406381298	
    Loss 381.58581855575	
    Loss 380.72658903319	
    Loss 380.88211553313	
    Loss 380.90054242473	
    Loss 378.27383458785	
    Loss 379.07053220754	
    Loss 378.83415246359	
    Loss 378.41269786768	
    Loss 379.07127763719	
    Loss 379.48081458896	
    Loss 381.92741872206	
    Loss 380.86957301784	
    Loss 381.9029869942	
    Loss 381.9226152089	
    Loss 381.17950847084	
    Loss 380.92872794527	
    Loss 381.44603816357	
    Loss 381.42302027372	
    Loss 381.67326686268	
    Loss 381.34029382646	
    Loss 381.13174353929	
    Loss 380.43993665871	
    Loss 379.91176764638	
    Loss 381.09958549181	
    Loss 380.83329348682	
    Loss 380.26353486156	
    Loss 380.16431579809	
    Loss 379.22476835555	
    Loss 378.07084292696	
    Loss 379.08517847876	
    Loss 378.75055277271	
    Loss 379.219883797	
    Loss 379.03296637185	
    Loss 380.45715359666	
    Loss 380.44897407662	
    Loss 380.34306712865	
    Loss 380.66160905369	
    Loss 381.10431468264	
Epoch 4	
 118185
     38
      0
      0
   2105
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10212581937363	
Grad norm	5.9947281686251	
    Loss 379.80630166189	
    Loss 380.6929312885	
    Loss 382.32347801174	
    Loss 381.81498667954	
    Loss 382.09749422617	
    Loss 381.3356252964	
    Loss 381.58916465289	
    Loss 381.09980369202	
    Loss 382.34305783774	
    Loss 382.2611256545	
    Loss 382.9903966182	
    Loss 382.69720213079	
    Loss 380.98684252266	
    Loss 380.23109410158	
    Loss 380.68404675978	
    Loss 379.61901930237	
    Loss 381.06255483364	
    Loss 381.88908387924	
    Loss 381.94397598708	
    Loss 381.45582722021	
    Loss 381.5263085859	
    Loss 382.07866634858	
    Loss 382.22460742613	
    Loss 382.28969459123	
    Loss 381.14035445572	
    Loss 381.87506777218	
    Loss 381.08039190843	
    Loss 381.353056767	
    Loss 381.53422891304	
    Loss 380.9761504616	
    Loss 381.91695243535	
    Loss 380.55386817865	
    Loss 380.59801616376	
    Loss 380.39705173874	
    Loss 380.47180465736	
    Loss 381.10483573035	
    Loss 381.00616260868	
    Loss 381.3506090024	
    Loss 381.14635283446	
    Loss 380.79906938783	
    Loss 381.6069414172	
    Loss 380.70732824116	
    Loss 379.05899089034	
    Loss 379.55727840668	
    Loss 380.64132438612	
    Loss 380.75745858371	
    Loss 380.47883650369	
    Loss 380.52566597418	
    Loss 379.76034865154	
    Loss 379.02312494551	
    Loss 378.38302828526	
    Loss 379.19729080197	
    Loss 377.81527184052	
    Loss 378.83513271753	
    Loss 379.18953440719	
    Loss 380.15252372741	
    Loss 379.78647411763	
    Loss 380.10249227422	
    Loss 381.44792303145	
    Loss 382.74793376645	
    Loss 382.03276854834	
    Loss 382.73066322425	
    Loss 383.56632953982	
    Loss 382.24422895031	
    Loss 382.21363248106	
    Loss 381.9902484387	
    Loss 381.88957969627	
    Loss 382.04732682509	
    Loss 380.25296080644	
    Loss 380.92339780567	
    Loss 381.27005426755	
    Loss 381.084158219	
    Loss 381.11486070815	
    Loss 381.29859332875	
    Loss 381.77335006335	
    Loss 381.38577016858	
    Loss 380.99372483746	
    Loss 380.26319840126	
    Loss 380.02767787329	
    Loss 382.0784612964	
    Loss 380.69066698631	
    Loss 380.90318726727	
    Loss 379.66647023517	
    Loss 380.19460345495	
    Loss 382.81521685177	
    Loss 380.04934802204	
    Loss 379.81732886456	
    Loss 380.38168741877	
    Loss 379.39424701012	
    Loss 379.81175119406	
    Loss 380.74579595029	
    Loss 380.46175733531	
    Loss 381.25766761985	
    Loss 381.17849704516	
    Loss 379.83765142981	
    Loss 379.34927424404	
    Loss 378.97758180194	
    Loss 377.48235400744	
    Loss 378.21127455251	
    Loss 378.05974344405	
    Loss 378.60643692105	
    Loss 379.33310908472	
    Loss 379.14631884329	
    Loss 379.46706646881	
    Loss 380.91769595248	
    Loss 381.44730993989	
    Loss 380.59481154912	
    Loss 380.7562501325	
    Loss 380.78051857641	
    Loss 378.15887434404	
    Loss 378.9612947547	
    Loss 378.73151993207	
    Loss 378.31442366283	
    Loss 378.97827048468	
    Loss 379.39269662393	
    Loss 381.84461666091	
    Loss 380.79125286069	
    Loss 381.82865922437	
    Loss 381.85293529627	
    Loss 381.11358773719	
    Loss 380.86601626432	
    Loss 381.38664037551	
    Loss 381.36634998439	
    Loss 381.61963266199	
    Loss 381.28943792973	
    Loss 381.08427305192	
    Loss 380.39542258405	
    Loss 379.87060299228	
    Loss 381.0606656726	
    Loss 380.7969679286	
    Loss 380.22983100526	
    Loss 380.1329108157	
    Loss 379.19595589394	
    Loss 378.04374901191	
    Loss 379.05978968012	
    Loss 378.72681259399	
    Loss 379.19766845454	
    Loss 379.01298106636	
    Loss 380.4388118966	
    Loss 380.43207655679	
    Loss 380.32744892108	
    Loss 380.64758299357	
    Loss 381.09176013362	
Epoch 5	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9781786549395	
    Loss 379.79473080378	
    Loss 380.68194065413	
    Loss 382.31239190697	
    Loss 381.80437546613	
    Loss 382.08733846162	
    Loss 381.32619802023	
    Loss 381.58045852873	
    Loss 381.09170558543	
    Loss 382.33512452995	
    Loss 382.25369490474	
    Loss 382.98377060624	
    Loss 382.69108356212	
    Loss 380.98084802554	
    Loss 380.22506350445	
    Loss 380.6781903031	
    Loss 379.61347869265	
    Loss 381.05697595496	
    Loss 381.88349353927	
    Loss 381.93878295487	
    Loss 381.45088035687	
    Loss 381.52153749457	
    Loss 382.0743062055	
    Loss 382.22012269032	
    Loss 382.28562502848	
    Loss 381.13643620274	
    Loss 381.87123725523	
    Loss 381.07667135433	
    Loss 381.34950650984	
    Loss 381.53093857977	
    Loss 380.9730262393	
    Loss 381.91389563893	
    Loss 380.55077944149	
    Loss 380.59506147122	
    Loss 380.39453004311	
    Loss 380.46953922606	
    Loss 381.10256981082	
    Loss 381.00401553054	
    Loss 381.34845678485	
    Loss 381.14420580125	
    Loss 380.79698735473	
    Loss 381.60501580456	
    Loss 380.70561539063	
    Loss 379.05725804844	
    Loss 379.55559638879	
    Loss 380.63970805791	
    Loss 380.75603358323	
    Loss 380.47750193759	
    Loss 380.52439026357	
    Loss 379.75888597327	
    Loss 379.02174547028	
    Loss 378.38162319954	
    Loss 379.19598923066	
    Loss 377.81416368523	
    Loss 378.83419085892	
    Loss 379.18883214379	
    Loss 380.15187360495	
    Loss 379.78587716457	
    Loss 380.10206796272	
    Loss 381.44738933668	
    Loss 382.74750102067	
    Loss 382.03233293067	
    Loss 382.73029216642	
    Loss 383.56611248228	
    Loss 382.24399278326	
    Loss 382.21345397676	
    Loss 381.99015581737	
    Loss 381.88942610272	
    Loss 382.04726427275	
    Loss 380.25291797799	
    Loss 380.92344378199	
    Loss 381.27020148623	
    Loss 381.08428809954	
    Loss 381.11496687967	
    Loss 381.29879843078	
    Loss 381.77356906163	
    Loss 381.38594183028	
    Loss 380.9939991843	
    Loss 380.26350045145	
    Loss 380.02797641391	
    Loss 382.07878584153	
    Loss 380.69098579595	
    Loss 380.9035725928	
    Loss 379.66682266399	
    Loss 380.19490016727	
    Loss 382.81559414323	
    Loss 380.04981931688	
    Loss 379.81776973837	
    Loss 380.38217745366	
    Loss 379.39478550673	
    Loss 379.81225282794	
    Loss 380.74630325442	
    Loss 380.46225486125	
    Loss 381.25820022042	
    Loss 381.17902811058	
    Loss 379.83821360754	
    Loss 379.34982060059	
    Loss 378.97815771972	
    Loss 377.48290864641	
    Loss 378.21183638294	
    Loss 378.06035499689	
    Loss 378.60701535649	
    Loss 379.33364169264	
    Loss 379.14685084083	
    Loss 379.46762277343	
    Loss 380.91829948968	
    Loss 381.44797284053	
    Loss 380.59549124563	
    Loss 380.75692614288	
    Loss 380.78119362509	
    Loss 378.15952940149	
    Loss 378.96196752177	
    Loss 378.73226056972	
    Loss 378.31513900427	
    Loss 378.97901367669	
    Loss 379.39345830129	
    Loss 381.8454194626	
    Loss 380.79207224362	
    Loss 381.82947746462	
    Loss 381.85379359345	
    Loss 381.11445225591	
    Loss 380.8668660213	
    Loss 381.38748674569	
    Loss 381.36717345435	
    Loss 381.62045169808	
    Loss 381.29024874704	
    Loss 381.08510783738	
    Loss 380.3962677994	
    Loss 379.87148112603	
    Loss 381.06153077053	
    Loss 380.7978396169	
    Loss 380.23071653938	
    Loss 380.13380016031	
    Loss 379.19686521694	
    Loss 378.04464501502	
    Loss 379.06067340731	
    Loss 378.72768466317	
    Loss 379.19852731081	
    Loss 379.01386378229	
    Loss 380.43969239511	
    Loss 380.43294971296	
    Loss 380.32830907763	
    Loss 380.64845284757	
    Loss 381.09263284627	
Epoch 6	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777934829023	
    Loss 379.79561012405	
    Loss 380.68278932115	
    Loss 382.31318007511	
    Loss 381.80513343839	
    Loss 382.08806642663	
    Loss 381.32690949203	
    Loss 381.58115787558	
    Loss 381.09238774813	
    Loss 382.33577295612	
    Loss 382.25432500662	
    Loss 382.98439781135	
    Loss 382.69169802623	
    Loss 380.98143344089	
    Loss 380.22561182131	
    Loss 380.67871260309	
    Loss 379.61398237486	
    Loss 381.0574472483	
    Loss 381.88393319451	
    Loss 381.93921233897	
    Loss 381.45129373049	
    Loss 381.52193197857	
    Loss 382.07469294552	
    Loss 382.22047789714	
    Loss 382.28597599359	
    Loss 381.13677134167	
    Loss 381.8715547386	
    Loss 381.07697345983	
    Loss 381.34979624927	
    Loss 381.53122233808	
    Loss 380.973299982	
    Loss 381.9141549054	
    Loss 380.5510209625	
    Loss 380.59529452089	
    Loss 380.39476736037	
    Loss 380.46977400967	
    Loss 381.10279137981	
    Loss 381.00422833257	
    Loss 381.34865662656	
    Loss 381.14439338869	
    Loss 380.79716632254	
    Loss 381.60519081483	
    Loss 380.7057885709	
    Loss 379.05741970539	
    Loss 379.55574997927	
    Loss 380.63985531335	
    Loss 380.75618053655	
    Loss 380.47764431469	
    Loss 380.52452671445	
    Loss 379.75900618719	
    Loss 379.02186180696	
    Loss 378.3817316732	
    Loss 379.19609527333	
    Loss 377.81427176856	
    Loss 378.83430032071	
    Loss 379.18894637916	
    Loss 380.1519838612	
    Loss 379.78598422322	
    Loss 380.1021772022	
    Loss 381.44748873427	
    Loss 382.74759982461	
    Loss 382.03242666852	
    Loss 382.73038384659	
    Loss 383.56620661775	
    Loss 382.244082163	
    Loss 382.21354212684	
    Loss 381.99024405906	
    Loss 381.88950763246	
    Loss 382.04734617026	
    Loss 380.25299752194	
    Loss 380.92352423274	
    Loss 381.27028298738	
    Loss 381.08436532966	
    Loss 381.11504031037	
    Loss 381.29887369186	
    Loss 381.77364226832	
    Loss 381.38601038592	
    Loss 380.99407002319	
    Loss 380.26357020788	
    Loss 380.02804369215	
    Loss 382.07885187696	
    Loss 380.69104929444	
    Loss 380.90363722175	
    Loss 379.66688391536	
    Loss 380.19495687487	
    Loss 382.81565253252	
    Loss 380.04988035222	
    Loss 379.81782792578	
    Loss 380.38223631013	
    Loss 379.39484502529	
    Loss 379.81230907175	
    Loss 380.74635840962	
    Loss 380.46230812432	
    Loss 381.25825374248	
    Loss 381.17908043412	
    Loss 379.83826616283	
    Loss 379.34987131107	
    Loss 378.97820868493	
    Loss 377.48295768833	
    Loss 378.21188489946	
    Loss 378.06040478195	
    Loss 378.60706282526	
    Loss 379.33368611889	
    Loss 379.1468945645	
    Loss 379.46766667345	
    Loss 380.91834464677	
    Loss 381.44802007669	
    Loss 380.59553849097	
    Loss 380.75697269195	
    Loss 380.78123940581	
    Loss 378.15957374141	
    Loss 378.96201209635	
    Loss 378.73230772681	
    Loss 378.31518443245	
    Loss 378.9790599496	
    Loss 379.39350498922	
    Loss 381.84546740311	
    Loss 380.79212054038	
    Loss 381.82952520211	
    Loss 381.85384284425	
    Loss 381.11450142106	
    Loss 380.86691409525	
    Loss 381.38753424683	
    Loss 381.3672195182	
    Loss 381.62049716515	
    Loss 381.29029350674	
    Loss 381.08515333256	
    Loss 380.3963134761	
    Loss 379.87152801343	
    Loss 381.06157671702	
    Loss 380.79788551422	
    Loss 380.23076280418	
    Loss 380.13384635327	
    Loss 379.19691203714	
    Loss 378.04469103237	
    Loss 379.06071857888	
    Loss 378.72772902666	
    Loss 379.19857082662	
    Loss 379.01390822978	
    Loss 380.43973646429	
    Loss 380.43299322839	
    Loss 380.32835172979	
    Loss 380.64849590499	
    Loss 381.09267585523	
Epoch 7	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777915615329	
    Loss 379.79565329184	
    Loss 380.68283098324	
    Loss 382.31321884078	
    Loss 381.80517077877	
    Loss 382.08810229493	
    Loss 381.3269444854	
    Loss 381.58119228237	
    Loss 381.09242128778	
    Loss 382.33580494262	
    Loss 382.25435608381	
    Loss 382.9844286872	
    Loss 382.69172832239	
    Loss 380.98146239358	
    Loss 380.22563897517	
    Loss 380.67873847875	
    Loss 379.61400729925	
    Loss 381.05747063495	
    Loss 381.88395500981	
    Loss 381.93923362723	
    Loss 381.45131421724	
    Loss 381.52195150768	
    Loss 382.07471201047	
    Loss 382.22049538401	
    Loss 382.28599323865	
    Loss 381.13678779102	
    Loss 381.87157031872	
    Loss 381.0769882918	
    Loss 381.34981044523	
    Loss 381.53123625472	
    Loss 380.97331340633	
    Loss 381.91416762386	
    Loss 380.55103284183	
    Loss 380.59530602517	
    Loss 380.3947790179	
    Loss 380.46978553518	
    Loss 381.1028022998	
    Loss 381.00423878899	
    Loss 381.34866645661	
    Loss 381.14440262392	
    Loss 380.79717513871	
    Loss 381.60519943528	
    Loss 380.7057970557	
    Loss 379.05742763157	
    Loss 379.55575750134	
    Loss 380.63986252912	
    Loss 380.75618772634	
    Loss 380.47765128131	
    Loss 380.52453338194	
    Loss 379.75901210177	
    Loss 379.02186752898	
    Loss 378.38173704187	
    Loss 379.19610051672	
    Loss 377.81427709325	
    Loss 378.83430570574	
    Loss 379.18895196969	
    Loss 380.15198924198	
    Loss 379.78598944581	
    Loss 380.1021825069	
    Loss 381.44749359214	
    Loss 382.74760463953	
    Loss 382.0324312378	
    Loss 382.73038830136	
    Loss 383.56621117148	
    Loss 382.24408650245	
    Loss 382.21354641128	
    Loss 381.9902483454	
    Loss 381.88951159597	
    Loss 382.04735013778	
    Loss 380.25300138498	
    Loss 380.92352813871	
    Loss 381.27028692251	
    Loss 381.08436904712	
    Loss 381.11504385458	
    Loss 381.29887731852	
    Loss 381.77364579488	
    Loss 381.38601369556	
    Loss 380.99407343414	
    Loss 380.26357356303	
    Loss 380.02804692727	
    Loss 382.07885504083	
    Loss 380.69105233147	
    Loss 380.90364030966	
    Loss 379.66688684515	
    Loss 380.19495958485	
    Loss 382.81565530986	
    Loss 380.04988325068	
    Loss 379.81783069747	
    Loss 380.38223910687	
    Loss 379.39484784768	
    Loss 379.81231173347	
    Loss 380.74636102019	
    Loss 380.46231064041	
    Loss 381.25825626505	
    Loss 381.17908290213	
    Loss 379.8382686398	
    Loss 379.34987369981	
    Loss 378.97821108358	
    Loss 377.48296000094	
    Loss 378.21188719115	
    Loss 378.06040712656	
    Loss 378.60706506541	
    Loss 379.33368821225	
    Loss 379.14689663008	
    Loss 379.46766873913	
    Loss 380.91834676445	
    Loss 381.44802229279	
    Loss 380.59554070386	
    Loss 380.7569748776	
    Loss 380.78124155178	
    Loss 378.15957582235	
    Loss 378.96201418741	
    Loss 378.73230993804	
    Loss 378.31518656112	
    Loss 378.97906211952	
    Loss 379.39350718036	
    Loss 381.84546964462	
    Loss 380.79212279963	
    Loss 381.82952743199	
    Loss 381.85384514752	
    Loss 381.11450371996	
    Loss 380.86691634263	
    Loss 381.38753646538	
    Loss 381.36722166808	
    Loss 381.62049928552	
    Loss 381.29029559333	
    Loss 381.08515544973	
    Loss 380.39631560172	
    Loss 379.87153019441	
    Loss 381.06157885098	
    Loss 380.79788764167	
    Loss 380.23076494803	
    Loss 380.1338484935	
    Loss 379.19691420415	
    Loss 378.0446931632	
    Loss 379.06072066716	
    Loss 378.72773107479	
    Loss 379.19857283373	
    Loss 379.01391028118	
    Loss 380.4397384938	
    Loss 380.4329952306	
    Loss 380.32835368776	
    Loss 380.64849788611	
    Loss 381.09267783328	
Epoch 8	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777915068613	
    Loss 379.79565527506	
    Loss 380.68283289745	
    Loss 382.31322062166	
    Loss 381.80517249719	
    Loss 382.08810394578	
    Loss 381.32694609579	
    Loss 381.58119386818	
    Loss 381.09242283436	
    Loss 382.33580642133	
    Loss 382.25435752162	
    Loss 382.98443011677	
    Loss 382.69172972866	
    Loss 380.98146374029	
    Loss 380.22564023786	
    Loss 380.67873968182	
    Loss 379.61400845713	
    Loss 381.0574717225	
    Loss 381.88395602263	
    Loss 381.93923461604	
    Loss 381.45131516837	
    Loss 381.52195241299	
    Loss 382.07471289219	
    Loss 382.22049618936	
    Loss 382.28599403319	
    Loss 381.13678854807	
    Loss 381.87157103529	
    Loss 381.07698897393	
    Loss 381.34981109703	
    Loss 381.53123689513	
    Loss 380.973314024	
    Loss 381.91416820906	
    Loss 380.55103338887	
    Loss 380.59530655678	
    Loss 380.39477955619	
    Loss 380.46978606815	
    Loss 381.10280280614	
    Loss 381.00423927272	
    Loss 381.34866691104	
    Loss 381.14440305074	
    Loss 380.79717554615	
    Loss 381.60519983418	
    Loss 380.70579744725	
    Loss 379.05742799704	
    Loss 379.55575784777	
    Loss 380.63986286151	
    Loss 380.75618805798	
    Loss 380.47765160292	
    Loss 380.52453368936	
    Loss 379.75901237464	
    Loss 379.02186779304	
    Loss 378.3817372905	
    Loss 379.19610075976	
    Loss 377.81427734017	
    Loss 378.8343059558	
    Loss 379.18895222916	
    Loss 380.1519894912	
    Loss 379.78598968759	
    Loss 380.10218275214	
    Loss 381.44749381751	
    Loss 382.74760486279	
    Loss 382.03243144964	
    Loss 382.73038850751	
    Loss 383.56621138201	
    Loss 382.24408670359	
    Loss 382.21354661037	
    Loss 381.99024854476	
    Loss 381.88951178027	
    Loss 382.04735032208	
    Loss 380.25300156504	
    Loss 380.92352832106	
    Loss 381.27028710581	
    Loss 381.08436921981	
    Loss 381.11504401954	
    Loss 381.2988774875	
    Loss 381.77364595915	
    Loss 381.38601384981	
    Loss 380.99407359314	
    Loss 380.26357371938	
    Loss 380.02804707802	
    Loss 382.07885518793	
    Loss 380.69105247255	
    Loss 380.90364045313	
    Loss 379.66688698135	
    Loss 380.19495971063	
    Loss 382.81565543851	
    Loss 380.0498833851	
    Loss 379.81783082638	
    Loss 380.38223923678	
    Loss 379.39484797872	
    Loss 379.81231185683	
    Loss 380.74636114129	
    Loss 380.46231075692	
    Loss 381.25825638182	
    Loss 381.17908301643	
    Loss 379.83826875465	
    Loss 379.34987381063	
    Loss 378.97821119481	
    Loss 377.48296010847	
    Loss 378.21188729787	
    Loss 378.06040723554	
    Loss 378.60706516981	
    Loss 379.33368830953	
    Loss 379.14689672643	
    Loss 379.46766883513	
    Loss 380.91834686276	
    Loss 381.4480223959	
    Loss 380.59554080666	
    Loss 380.75697497945	
    Loss 380.78124165167	
    Loss 378.15957591927	
    Loss 378.96201428482	
    Loss 378.73231004125	
    Loss 378.3151866603	
    Loss 378.97906222073	
    Loss 379.39350728276	
    Loss 381.8454697491	
    Loss 380.79212290499	
    Loss 381.82952753589	
    Loss 381.85384525496	
    Loss 381.11450382721	
    Loss 380.86691644753	
    Loss 381.38753656882	
    Loss 381.36722176836	
    Loss 381.62049938436	
    Loss 381.29029569056	
    Loss 381.08515554824	
    Loss 380.39631570058	
    Loss 379.87153029598	
    Loss 381.0615789502	
    Loss 380.79788774041	
    Loss 380.23076504746	
    Loss 380.13384859285	
    Loss 379.19691430476	
    Loss 378.04469326213	
    Loss 379.06072076399	
    Loss 378.72773116969	
    Loss 379.19857292662	
    Loss 379.0139103763	
    Loss 380.43973858776	
    Loss 380.43299532325	
    Loss 380.32835377817	
    Loss 380.64849797773	
    Loss 381.09267792473	
Epoch 9	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777915041247	
    Loss 379.79565536669	
    Loss 380.68283298595	
    Loss 382.31322070397	
    Loss 381.80517257675	
    Loss 382.08810402216	
    Loss 381.32694617031	
    Loss 381.58119394165	
    Loss 381.0924229061	
    Loss 382.33580649003	
    Loss 382.25435758852	
    Loss 382.98443018334	
    Loss 382.69172979427	
    Loss 380.98146380328	
    Loss 380.22564029681	
    Loss 380.67873973806	
    Loss 379.61400851115	
    Loss 381.05747177332	
    Loss 381.88395606995	
    Loss 381.93923466211	
    Loss 381.4513152128	
    Loss 381.52195245513	
    Loss 382.07471293318	
    Loss 382.22049622666	
    Loss 382.28599406994	
    Loss 381.13678858307	
    Loss 381.87157106846	
    Loss 381.07698900545	
    Loss 381.34981112713	
    Loss 381.53123692475	
    Loss 380.97331405254	
    Loss 381.91416823608	
    Loss 380.5510334142	
    Loss 380.59530658157	
    Loss 380.39477958119	
    Loss 380.46978609286	
    Loss 381.10280282978	
    Loss 381.00423929522	
    Loss 381.34866693217	
    Loss 381.14440307054	
    Loss 380.79717556507	
    Loss 381.60519985276	
    Loss 380.70579746541	
    Loss 379.05742801398	
    Loss 379.55575786381	
    Loss 380.63986287688	
    Loss 380.75618807331	
    Loss 380.47765161786	
    Loss 380.52453370368	
    Loss 379.75901238729	
    Loss 379.02186780532	
    Loss 378.38173730211	
    Loss 379.19610077113	
    Loss 377.81427735165	
    Loss 378.83430596753	
    Loss 379.18895224128	
    Loss 380.15198950283	
    Loss 379.78598969883	
    Loss 380.1021827635	
    Loss 381.447493828	
    Loss 382.74760487323	
    Loss 382.03243145954	
    Loss 382.73038851708	
    Loss 383.56621139181	
    Loss 382.24408671302	
    Loss 382.21354661965	
    Loss 381.99024855403	
    Loss 381.88951178894	
    Loss 382.04735033071	
    Loss 380.25300157346	
    Loss 380.92352832964	
    Loss 381.27028711441	
    Loss 381.08436922789	
    Loss 381.11504402726	
    Loss 381.29887749543	
    Loss 381.77364596688	
    Loss 381.38601385706	
    Loss 380.99407360059	
    Loss 380.26357372667	
    Loss 380.02804708513	
    Loss 382.07885519479	
    Loss 380.69105247912	
    Loss 380.90364045986	
    Loss 379.66688698772	
    Loss 380.19495971645	
    Loss 382.81565544453	
    Loss 380.04988339136	
    Loss 379.81783083242	
    Loss 380.38223924286	
    Loss 379.39484798483	
    Loss 379.81231186253	
    Loss 380.746361147	
    Loss 380.46231076236	
    Loss 381.25825638725	
    Loss 381.17908302173	
    Loss 379.83826876002	
    Loss 379.3498738158	
    Loss 378.97821120002	
    Loss 377.48296011346	
    Loss 378.21188730286	
    Loss 378.06040724065	
    Loss 378.60706517464	
    Loss 379.33368831406	
    Loss 379.14689673092	
    Loss 379.46766883964	
    Loss 380.91834686737	
    Loss 381.44802240075	
    Loss 380.59554081146	
    Loss 380.75697498423	
    Loss 380.78124165633	
    Loss 378.15957592379	
    Loss 378.96201428942	
    Loss 378.73231004608	
    Loss 378.31518666496	
    Loss 378.97906222553	
    Loss 379.39350728761	
    Loss 381.84546975399	
    Loss 380.79212290994	
    Loss 381.82952754077	
    Loss 381.85384526	
    Loss 381.11450383225	
    Loss 380.86691645245	
    Loss 381.38753657363	
    Loss 381.36722177306	
    Loss 381.620499389	
    Loss 381.29029569514	
    Loss 381.08515555285	
    Loss 380.39631570522	
    Loss 379.87153030074	
    Loss 381.06157895485	
    Loss 380.79788774505	
    Loss 380.23076505212	
    Loss 380.1338485975	
    Loss 379.19691430945	
    Loss 378.04469326674	
    Loss 379.06072076852	
    Loss 378.72773117411	
    Loss 379.19857293095	
    Loss 379.01391038076	
    Loss 380.43973859212	
    Loss 380.43299532755	
    Loss 380.32835378238	
    Loss 380.64849798201	
    Loss 381.09267792899	
Epoch 10	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777915039825	
    Loss 379.79565537099	
    Loss 380.68283299006	
    Loss 382.31322070778	
    Loss 381.80517258048	
    Loss 382.08810402572	
    Loss 381.32694617374	
    Loss 381.58119394509	
    Loss 381.09242290947	
    Loss 382.33580649328	
    Loss 382.25435759165	
    Loss 382.98443018644	
    Loss 382.69172979736	
    Loss 380.98146380625	
    Loss 380.22564029958	
    Loss 380.67873974072	
    Loss 379.61400851369	
    Loss 381.05747177573	
    Loss 381.88395607215	
    Loss 381.93923466426	
    Loss 381.45131521488	
    Loss 381.52195245712	
    Loss 382.07471293505	
    Loss 382.22049622839	
    Loss 382.28599407165	
    Loss 381.13678858471	
    Loss 381.87157106998	
    Loss 381.07698900695	
    Loss 381.34981112853	
    Loss 381.53123692617	
    Loss 380.97331405389	
    Loss 381.91416823736	
    Loss 380.55103341538	
    Loss 380.59530658273	
    Loss 380.39477958236	
    Loss 380.46978609402	
    Loss 381.10280283089	
    Loss 381.00423929625	
    Loss 381.34866693318	
    Loss 381.14440307147	
    Loss 380.79717556594	
    Loss 381.6051998536	
    Loss 380.70579746628	
    Loss 379.05742801479	
    Loss 379.55575786452	
    Loss 380.63986287762	
    Loss 380.75618807402	
    Loss 380.47765161856	
    Loss 380.52453370434	
    Loss 379.75901238787	
    Loss 379.0218678059	
    Loss 378.38173730266	
    Loss 379.19610077169	
    Loss 377.81427735217	
    Loss 378.83430596805	
    Loss 379.18895224186	
    Loss 380.15198950338	
    Loss 379.78598969935	
    Loss 380.10218276402	
    Loss 381.44749382848	
    Loss 382.74760487373	
    Loss 382.03243146	
    Loss 382.73038851753	
    Loss 383.56621139231	
    Loss 382.24408671344	
    Loss 382.21354662009	
    Loss 381.99024855445	
    Loss 381.88951178937	
    Loss 382.04735033109	
    Loss 380.25300157386	
    Loss 380.92352833006	
    Loss 381.27028711481	
    Loss 381.08436922827	
    Loss 381.11504402761	
    Loss 381.29887749582	
    Loss 381.77364596725	
    Loss 381.3860138574	
    Loss 380.99407360095	
    Loss 380.26357372703	
    Loss 380.02804708544	
    Loss 382.07885519511	
    Loss 380.69105247944	
    Loss 380.9036404602	
    Loss 379.66688698802	
    Loss 380.19495971675	
    Loss 382.81565544483	
    Loss 380.04988339166	
    Loss 379.81783083269	
    Loss 380.38223924312	
    Loss 379.39484798511	
    Loss 379.81231186282	
    Loss 380.74636114727	
    Loss 380.4623107626	
    Loss 381.25825638749	
    Loss 381.17908302197	
    Loss 379.83826876028	
    Loss 379.34987381605	
    Loss 378.97821120026	
    Loss 377.48296011369	
    Loss 378.21188730307	
    Loss 378.06040724087	
    Loss 378.60706517488	
    Loss 379.33368831427	
    Loss 379.14689673115	
    Loss 379.46766883986	
    Loss 380.91834686757	
    Loss 381.44802240096	
    Loss 380.59554081168	
    Loss 380.75697498444	
    Loss 380.78124165654	
    Loss 378.159575924	
    Loss 378.96201428964	
    Loss 378.73231004629	
    Loss 378.31518666516	
    Loss 378.97906222573	
    Loss 379.39350728784	
    Loss 381.84546975421	
    Loss 380.79212291018	
    Loss 381.82952754101	
    Loss 381.8538452602	
    Loss 381.11450383247	
    Loss 380.86691645266	
    Loss 381.38753657388	
    Loss 381.36722177328	
    Loss 381.62049938922	
    Loss 381.29029569536	
    Loss 381.08515555306	
    Loss 380.39631570543	
    Loss 379.87153030095	
    Loss 381.06157895507	
    Loss 380.79788774526	
    Loss 380.23076505233	
    Loss 380.1338485977	
    Loss 379.19691430967	
    Loss 378.04469326696	
    Loss 379.06072076873	
    Loss 378.72773117432	
    Loss 379.19857293114	
    Loss 379.01391038098	
    Loss 380.43973859235	
    Loss 380.43299532775	
    Loss 380.32835378258	
    Loss 380.6484979822	
    Loss 381.09267792919	
Epoch 11	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.9777915039803	
    Loss 379.79565537121	
    Loss 380.68283299025	
    Loss 382.31322070795	
    Loss 381.80517258065	
    Loss 382.08810402589	
    Loss 381.3269461739	
    Loss 381.58119394524	
    Loss 381.09242290963	
    Loss 382.33580649343	
    Loss 382.2543575918	
    Loss 382.98443018658	
    Loss 382.6917297975	
    Loss 380.98146380638	
    Loss 380.22564029971	
    Loss 380.67873974084	
    Loss 379.6140085138	
    Loss 381.05747177584	
    Loss 381.88395607227	
    Loss 381.93923466437	
    Loss 381.45131521497	
    Loss 381.5219524572	
    Loss 382.07471293516	
    Loss 382.22049622849	
    Loss 382.28599407172	
    Loss 381.13678858477	
    Loss 381.87157107005	
    Loss 381.07698900703	
    Loss 381.34981112862	
    Loss 381.53123692624	
    Loss 380.97331405394	
    Loss 381.91416823741	
    Loss 380.55103341545	
    Loss 380.59530658279	
    Loss 380.3947795824	
    Loss 380.46978609407	
    Loss 381.10280283093	
    Loss 381.0042392963	
    Loss 381.34866693323	
    Loss 381.14440307152	
    Loss 380.797175566	
    Loss 381.60519985364	
    Loss 380.70579746632	
    Loss 379.05742801482	
    Loss 379.55575786456	
    Loss 380.63986287766	
    Loss 380.75618807405	
    Loss 380.4776516186	
    Loss 380.52453370436	
    Loss 379.7590123879	
    Loss 379.02186780593	
    Loss 378.38173730268	
    Loss 379.19610077173	
    Loss 377.8142773522	
    Loss 378.83430596807	
    Loss 379.18895224189	
    Loss 380.1519895034	
    Loss 379.78598969936	
    Loss 380.10218276404	
    Loss 381.44749382852	
    Loss 382.74760487376	
    Loss 382.03243146003	
    Loss 382.73038851756	
    Loss 383.56621139232	
    Loss 382.24408671347	
    Loss 382.21354662012	
    Loss 381.99024855447	
    Loss 381.88951178939	
    Loss 382.04735033112	
    Loss 380.25300157387	
    Loss 380.92352833008	
    Loss 381.27028711484	
    Loss 381.08436922829	
    Loss 381.11504402762	
    Loss 381.29887749584	
    Loss 381.77364596727	
    Loss 381.38601385743	
    Loss 380.99407360097	
    Loss 380.26357372704	
    Loss 380.02804708546	
    Loss 382.07885519513	
    Loss 380.69105247946	
    Loss 380.90364046022	
    Loss 379.66688698804	
    Loss 380.19495971676	
    Loss 382.81565544483	
    Loss 380.04988339167	
    Loss 379.81783083271	
    Loss 380.38223924313	
    Loss 379.39484798513	
    Loss 379.81231186283	
    Loss 380.74636114728	
    Loss 380.46231076261	
    Loss 381.2582563875	
    Loss 381.17908302197	
    Loss 379.83826876029	
    Loss 379.34987381606	
    Loss 378.97821120028	
    Loss 377.48296011371	
    Loss 378.21188730309	
    Loss 378.06040724089	
    Loss 378.60706517488	
    Loss 379.33368831428	
    Loss 379.14689673116	
    Loss 379.46766883988	
    Loss 380.91834686758	
    Loss 381.44802240098	
    Loss 380.59554081169	
    Loss 380.75697498445	
    Loss 380.78124165656	
    Loss 378.15957592401	
    Loss 378.96201428965	
    Loss 378.7323100463	
    Loss 378.31518666517	
    Loss 378.97906222574	
    Loss 379.39350728786	
    Loss 381.84546975421	
    Loss 380.79212291019	
    Loss 381.82952754102	
    Loss 381.85384526021	
    Loss 381.11450383248	
    Loss 380.86691645267	
    Loss 381.38753657389	
    Loss 381.36722177329	
    Loss 381.62049938923	
    Loss 381.29029569537	
    Loss 381.08515555307	
    Loss 380.39631570544	
    Loss 379.87153030096	
    Loss 381.06157895508	
    Loss 380.79788774527	
    Loss 380.23076505235	
    Loss 380.13384859771	
    Loss 379.19691430968	
    Loss 378.04469326697	
    Loss 379.06072076874	
    Loss 378.72773117434	
    Loss 379.19857293114	
    Loss 379.01391038099	
    Loss 380.43973859236	
    Loss 380.43299532775	
    Loss 380.32835378258	
    Loss 380.64849798221	
    Loss 381.0926779292	
Epoch 12	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.97779150398	
    Loss 379.79565537122	
    Loss 380.68283299026	
    Loss 382.31322070796	
    Loss 381.80517258066	
    Loss 382.08810402591	
    Loss 381.3269461739	
    Loss 381.58119394525	
    Loss 381.09242290964	
    Loss 382.33580649344	
    Loss 382.25435759181	
    Loss 382.98443018659	
    Loss 382.69172979751	
    Loss 380.98146380638	
    Loss 380.22564029972	
    Loss 380.67873974084	
    Loss 379.61400851381	
    Loss 381.05747177585	
    Loss 381.88395607227	
    Loss 381.93923466437	
    Loss 381.45131521498	
    Loss 381.5219524572	
    Loss 382.07471293516	
    Loss 382.22049622849	
    Loss 382.28599407172	
    Loss 381.13678858478	
    Loss 381.87157107005	
    Loss 381.07698900704	
    Loss 381.34981112862	
    Loss 381.53123692624	
    Loss 380.97331405394	
    Loss 381.91416823741	
    Loss 380.55103341545	
    Loss 380.5953065828	
    Loss 380.39477958241	
    Loss 380.46978609407	
    Loss 381.10280283093	
    Loss 381.0042392963	
    Loss 381.34866693323	
    Loss 381.14440307153	
    Loss 380.797175566	
    Loss 381.60519985364	
    Loss 380.70579746632	
    Loss 379.05742801482	
    Loss 379.55575786456	
    Loss 380.63986287766	
    Loss 380.75618807405	
    Loss 380.47765161861	
    Loss 380.52453370436	
    Loss 379.7590123879	
    Loss 379.02186780593	
    Loss 378.38173730268	
    Loss 379.19610077173	
    Loss 377.8142773522	
    Loss 378.83430596807	
    Loss 379.18895224189	
    Loss 380.15198950341	
    Loss 379.78598969937	
    Loss 380.10218276404	
    Loss 381.44749382853	
    Loss 382.74760487376	
    Loss 382.03243146003	
    Loss 382.73038851756	
    Loss 383.56621139233	
    Loss 382.24408671347	
    Loss 382.21354662012	
    Loss 381.99024855447	
    Loss 381.8895117894	
    Loss 382.04735033112	
    Loss 380.25300157387	
    Loss 380.92352833008	
    Loss 381.27028711484	
    Loss 381.08436922829	
    Loss 381.11504402762	
    Loss 381.29887749584	
    Loss 381.77364596727	
    Loss 381.38601385743	
    Loss 380.99407360097	
    Loss 380.26357372703	
    Loss 380.02804708546	
    Loss 382.07885519513	
    Loss 380.69105247946	
    Loss 380.90364046022	
    Loss 379.66688698804	
    Loss 380.19495971676	
    Loss 382.81565544483	
    Loss 380.04988339166	
    Loss 379.81783083271	
    Loss 380.38223924313	
    Loss 379.39484798513	
    Loss 379.81231186283	
    Loss 380.74636114728	
    Loss 380.46231076261	
    Loss 381.2582563875	
    Loss 381.17908302197	
    Loss 379.83826876029	
    Loss 379.34987381606	
    Loss 378.97821120028	
    Loss 377.48296011371	
    Loss 378.21188730309	
    Loss 378.06040724089	
    Loss 378.60706517488	
    Loss 379.33368831428	
    Loss 379.14689673116	
    Loss 379.46766883988	
    Loss 380.91834686758	
    Loss 381.44802240098	
    Loss 380.59554081169	
    Loss 380.75697498445	
    Loss 380.78124165656	
    Loss 378.15957592401	
    Loss 378.96201428965	
    Loss 378.7323100463	
    Loss 378.31518666518	
    Loss 378.97906222574	
    Loss 379.39350728786	
    Loss 381.84546975421	
    Loss 380.7921229102	
    Loss 381.82952754103	
    Loss 381.85384526021	
    Loss 381.11450383248	
    Loss 380.86691645267	
    Loss 381.38753657389	
    Loss 381.36722177329	
    Loss 381.62049938923	
    Loss 381.29029569537	
    Loss 381.08515555307	
    Loss 380.39631570544	
    Loss 379.87153030096	
    Loss 381.06157895508	
    Loss 380.79788774527	
    Loss 380.23076505235	
    Loss 380.13384859771	
    Loss 379.19691430968	
    Loss 378.04469326697	
    Loss 379.06072076874	
    Loss 378.72773117434	
    Loss 379.19857293114	
    Loss 379.01391038099	
    Loss 380.43973859236	
    Loss 380.43299532776	
    Loss 380.32835378259	
    Loss 380.64849798222	
    Loss 381.0926779292	
Epoch 13	
 118183
     38
      0
      0
   2107
      0
      0
    124
  11345
     11
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10211823258072	
Grad norm	5.97779150398	
    Loss 379.79565537122	
    Loss 380.68283299026	
    Loss 382.31322070796	
    Loss 381.80517258066	
    Loss 382.08810402591	
    Loss 381.3269461739	
    Loss 381.58119394525	
    Loss 381.09242290964	
    Loss 382.33580649344	
    Loss 382.25435759181	
    Loss 382.98443018659	
    Loss 382.69172979751	
    Loss 380.98146380639	
    Loss 380.22564029972	
    Loss 380.67873974084	
    Loss 379.61400851381	
    Loss 381.05747177585	
    Loss 381.88395607228	
    Loss 381.93923466437	
    Loss 381.45131521498	
    Loss 381.5219524572	
    Loss 382.07471293517	
    Loss 382.22049622849	
    Loss 382.28599407172	
    Loss 381.13678858478	
    Loss 381.87157107005	
    Loss 381.07698900704	
    Loss 381.34981112862	
    Loss 381.53123692624	
    Loss 380.97331405394	
    Loss 381.91416823741	
    Loss 380.55103341545	
    Loss 380.5953065828	
    Loss 380.39477958241	
    Loss 380.46978609407	
    Loss 381.10280283093	
    Loss 381.0042392963	
    Loss 381.34866693323	
    Loss 381.14440307153	
    Loss 380.797175566	
    Loss 381.60519985364	
    Loss 380.70579746632	
    Loss 379.05742801482	
    Loss 379.55575786456	
    Loss 380.63986287766	
    Loss 380.75618807405	
    Loss 380.47765161861	
    Loss 380.52453370436	
    Loss 379.7590123879	
    Loss 379.02186780593	
    Loss 378.38173730268	
    Loss 379.19610077173	
    Loss 377.8142773522	
    Loss 378.83430596807	
    Loss 379.18895224189	
    Loss 380.1519895034	
    Loss 379.78598969937	
    Loss 380.10218276404	
    Loss 381.44749382853	
    Loss 382.74760487376	
    Loss 382.03243146003	
    Loss 382.73038851756	
    Loss 383.56621139233	
