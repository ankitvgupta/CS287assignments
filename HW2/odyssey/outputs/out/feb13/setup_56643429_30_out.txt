[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	0.25	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1797
 3376
 1368
 2096
 1369
 2231
 3991
 6142
 3559
 1715
 2218
 1689
 1084
 2968
 4384
 3064
 3583
 1855
 4230
 2285
 4887
 1468
 2839
 3405
 1087
  466
  730
 3833
 1384
 4052
 3296
 2785
 5886
 5048
 1047
 2493
 2567
 4141
 3842
 1180
  643
 5094
 2785
 2399
 9447
[torch.DoubleTensor of size 45]

Validation accuracy:	0.023632859917456	
Grad norm	0	
    Loss 11388493.816629	
    Loss 11190326.475037	
    Loss 10995646.590761	
    Loss 10804349.936273	
    Loss 10616383.419766	
    Loss 10431686.716992	
    Loss 10250212.278008	
    Loss 10071893.670912	
    Loss 9896675.9974063	
    Loss 9724508.2925841	
    Loss 9555335.3633986	
    Loss 9389107.4855307	
    Loss 9225778.0322738	
    Loss 9065291.3506851	
    Loss 8907597.0907286	
    Loss 8752644.1367379	
    Loss 8600384.5877007	
    Loss 8450773.3355305	
    Loss 8303768.405551	
    Loss 8159322.5144635	
    Loss 8017391.5942976	
    Loss 7877928.398865	
    Loss 7740886.9707368	
    Loss 7606233.001245	
    Loss 7473924.7586968	
    Loss 7343920.7819188	
    Loss 7216178.6532252	
    Loss 7090657.9998	
    Loss 6967320.6197682	
    Loss 6846130.3912714	
    Loss 6727043.5269774	
    Loss 6610034.3019851	
    Loss 6495057.527623	
    Loss 6382078.4137883	
    Loss 6271064.9482092	
    Loss 6161986.0368438	
    Loss 6054803.7115719	
    Loss 5949488.0702045	
    Loss 5846004.965024	
    Loss 5744318.1172091	
    Loss 5644405.4991568	
    Loss 5546225.9340127	
    Loss 5449756.3310566	
    Loss 5354964.5940506	
    Loss 5261818.050812	
    Loss 5170296.9968721	
    Loss 5080367.6899638	
    Loss 4992003.8128661	
    Loss 4905174.3379514	
    Loss 4819860.4511873	
    Loss 4736028.4810454	
    Loss 4653652.8847205	
    Loss 4572712.3558199	
    Loss 4493177.6134861	
    Loss 4415023.3077387	
    Loss 4338232.5695317	
    Loss 4262779.3399085	
    Loss 4188636.1295583	
    Loss 4115785.7698496	
    Loss 4044202.2264976	
    Loss 3973868.4592671	
    Loss 3904755.4632486	
    Loss 3836842.8033192	
    Loss 3770113.5927288	
    Loss 3704543.5969266	
    Loss 3640113.4333747	
    Loss 3576803.5748534	
    Loss 3514595.6986537	
    Loss 3453467.1323818	
    Loss 3393400.4016853	
    Loss 3334383.947652	
    Loss 3276392.6144096	
    Loss 3219407.0409549	
    Loss 3163419.0556468	
    Loss 3108404.683374	
    Loss 3054342.0595751	
    Loss 3001224.0736948	
    Loss 2949029.0885876	
    Loss 2897739.9280359	
    Loss 2847345.5400196	
    Loss 2797828.7162592	
    Loss 2749169.7157148	
    Loss 2701360.8614721	
    Loss 2654381.9719122	
    Loss 2608220.6136707	
    Loss 2562863.1953191	
    Loss 2518295.0569705	
    Loss 2474501.0190519	
    Loss 2431466.9626263	
    Loss 2389180.6576973	
    Loss 2347631.9172613	
    Loss 2306805.9498296	
    Loss 2266690.4414353	
    Loss 2227274.0411048	
    Loss 2188541.1702922	
    Loss 2150482.5554719	
    Loss 2113086.2060589	
    Loss 2076341.6566765	
    Loss 2040233.2414383	
    Loss 2004751.3264856	
    Loss 1969889.0773373	
    Loss 1935633.5349815	
    Loss 1901974.151111	
    Loss 1868900.5887181	
    Loss 1836402.3258805	
    Loss 1804470.9438183	
    Loss 1773093.3149418	
    Loss 1742261.2930066	
    Loss 1711965.7540764	
    Loss 1682197.8252275	
    Loss 1652946.6155163	
    Loss 1624205.1275948	
    Loss 1595962.0756906	
    Loss 1568212.0985642	
    Loss 1540942.1214201	
    Loss 1514148.0850983	
    Loss 1487819.6714502	
    Loss 1461949.3545853	
    Loss 1436530.2735964	
    Loss 1411552.0504096	
    Loss 1387008.3656295	
    Loss 1362891.4489924	
    Loss 1339196.2763571	
    Loss 1315911.8637843	
    Loss 1293033.1946453	
    Loss 1270552.185247	
    Loss 1248461.2115178	
    Loss 1226754.249596	
    Loss 1205424.8236469	
    Loss 1184466.6841066	
    Loss 1163874.0427391	
    Loss 1143639.9725808	
    Loss 1123759.2517876	
    Loss 1104222.0119566	
    Loss 1085025.0625699	
    Loss 1066162.1686634	
    Loss 1047626.9805804	
    Loss 1029414.2376114	
    Loss 1011517.3215467	
    Loss 993932.04789551	
    Loss 976654.02400533	
    Loss 959676.40512058	
    Loss 942993.47014867	
Epoch 2	
 106810
    791
     27
      1
   4954
      0
      0
   1962
  16404
    830
     28
      0
      0
      0
      0
      1
[torch.DoubleTensor of size 16]

Validation accuracy:	0.10285415149308	
Grad norm	369.80618342664	
    Loss 933123.58749717	
    Loss 916902.68504005	
    Loss 900964.56272435	
    Loss 885301.80426967	
    Loss 869912.04470282	
    Loss 854790.11889378	
    Loss 839932.29901423	
    Loss 825331.43136371	
    Loss 810985.22459121	
    Loss 796888.62709201	
    Loss 783036.94506325	
    Loss 769425.96632714	
    Loss 756052.48376508	
    Loss 742911.9067589	
    Loss 729999.29145052	
    Loss 717311.02451791	
    Loss 704843.50353654	
    Loss 692592.8211295	
    Loss 680555.32087539	
    Loss 668727.50457976	
    Loss 657105.86871584	
    Loss 645685.96701317	
    Loss 634463.97399515	
    Loss 623437.54792475	
    Loss 612603.22320854	
    Loss 601957.43449089	
    Loss 591497.19782674	
    Loss 581218.39534909	
    Loss 571118.6804644	
    Loss 561194.69908467	
    Loss 551442.66631338	
    Loss 541861.1463652	
    Loss 532445.74323222	
    Loss 523193.39424121	
    Loss 514102.13558931	
    Loss 505169.66987719	
    Loss 496391.79780336	
    Loss 487766.80298116	
    Loss 479292.93093406	
    Loss 470964.85356856	
    Loss 462782.92444178	
    Loss 454742.44812523	
    Loss 446841.75456636	
    Loss 439078.62829214	
    Loss 431450.29247947	
    Loss 423955.08341088	
    Loss 416590.45183475	
    Loss 409353.86126676	
    Loss 402242.44624832	
    Loss 395255.70700412	
    Loss 388389.9893038	
    Loss 381644.20329712	
    Loss 375014.45912624	
    Loss 368500.65464197	
    Loss 362099.76218638	
    Loss 355811.05135837	
    Loss 349631.65178528	
    Loss 343559.48654348	
    Loss 337593.57070084	
    Loss 331731.68359624	
    Loss 325972.01681474	
    Loss 320311.83385761	
    Loss 314749.92761646	
    Loss 309284.71323301	
    Loss 303914.76107844	
    Loss 298637.9791772	
    Loss 293452.94478814	
    Loss 288358.27508336	
    Loss 283351.5590663	
    Loss 278431.46270255	
    Loss 273597.87694847	
    Loss 268848.16948555	
    Loss 264180.00888267	
    Loss 259594.64254764	
    Loss 255089.11276676	
    Loss 250661.13729326	
    Loss 246310.80225646	
    Loss 242036.08856684	
    Loss 237834.99486521	
    Loss 233708.07488798	
    Loss 229652.42987956	
    Loss 225667.08601308	
    Loss 221751.31076824	
    Loss 217903.6213153	
    Loss 214122.80939968	
    Loss 210407.58959479	
    Loss 206756.81354823	
    Loss 203169.78652749	
    Loss 199644.81937329	
    Loss 196181.24383952	
    Loss 192778.5463705	
    Loss 189434.29654664	
    Loss 186148.82976882	
    Loss 182920.52202653	
    Loss 179747.2820527	
    Loss 176629.85718932	
    Loss 173566.95015628	
    Loss 170557.11305292	
    Loss 167599.3162259	
    Loss 164692.69835388	
    Loss 161837.29963254	
    Loss 159031.42329586	
    Loss 156274.6259707	
    Loss 153565.76750715	
    Loss 150904.06436215	
    Loss 148288.86203581	
    Loss 145718.80759619	
    Loss 143193.36590351	
    Loss 140711.9099257	
    Loss 138272.94424909	
    Loss 135876.95920418	
    Loss 133522.66257723	
    Loss 131209.15464028	
    Loss 128936.63322257	
    Loss 126703.10527834	
    Loss 124508.76146277	
    Loss 122351.96995664	
    Loss 120232.65338136	
    Loss 118150.60511458	
    Loss 116104.08312958	
    Loss 114093.8194088	
    Loss 112118.45544255	
    Loss 110177.66280343	
    Loss 108270.40090199	
    Loss 106396.30584037	
    Loss 104554.70830822	
    Loss 102744.82006701	
    Loss 100966.56626175	
    Loss 99219.547337369	
    Loss 97502.50775989	
    Loss 95815.399239253	
    Loss 94157.913945121	
    Loss 92529.602205842	
    Loss 90929.103386253	
    Loss 89356.673079235	
    Loss 87811.690176103	
    Loss 86293.325517228	
    Loss 84801.270798065	
    Loss 83335.099911441	
    Loss 81894.759715141	
    Loss 80479.376509833	
    Loss 79088.938634058	
    Loss 77722.353475912	
Epoch 3	
 131600
      0
      0
      0
      0
      0
      0
      0
    208
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099872541879097	
Grad norm	109.545045308	
    Loss 76913.683980144	
    Loss 75584.729279246	
    Loss 74279.515590483	
    Loss 72996.565509281	
    Loss 71735.722336365	
    Loss 70496.856743215	
    Loss 69279.835363605	
    Loss 68083.598967747	
    Loss 66908.631559993	
    Loss 65753.887418109	
    Loss 64619.340945273	
    Loss 63504.024634755	
    Loss 62408.340647782	
    Loss 61331.818972544	
    Loss 60273.770966508	
    Loss 59234.372069273	
    Loss 58213.107383117	
    Loss 57209.717996105	
    Loss 56223.709435349	
    Loss 55254.687682578	
    Loss 54302.911323798	
    Loss 53367.663198634	
    Loss 52448.435119457	
    Loss 51545.260572848	
    Loss 50657.606250612	
    Loss 49785.337703244	
    Loss 48928.454360787	
    Loss 48086.262944501	
    Loss 47258.961655546	
    Loss 46446.058982265	
    Loss 45647.407317321	
    Loss 44862.649703152	
    Loss 44091.320800908	
    Loss 43333.206613517	
    Loss 42588.387023156	
    Loss 41856.542867662	
    Loss 41137.182678916	
    Loss 40430.251781029	
    Loss 39736.360313121	
    Loss 39053.784097154	
    Loss 38383.527932167	
    Loss 37724.890518559	
    Loss 37077.317184295	
    Loss 36441.134298751	
    Loss 35816.280257869	
    Loss 35202.071330197	
    Loss 34598.809814572	
    Loss 34005.98999182	
    Loss 33423.252416515	
    Loss 32850.814758515	
    Loss 32288.120119849	
    Loss 31735.750435336	
    Loss 31191.937928054	
    Loss 30658.290942674	
    Loss 30133.971482348	
    Loss 29619.128957927	
    Loss 29112.977144017	
    Loss 28615.459634864	
    Loss 28126.943489017	
    Loss 27647.202977431	
    Loss 27175.532448018	
    Loss 26711.865633328	
    Loss 26256.270503039	
    Loss 25808.301868538	
    Loss 25368.519811733	
    Loss 24936.289369607	
    Loss 24511.645425743	
    Loss 24094.419681955	
    Loss 23684.263667337	
    Loss 23280.92393417	
    Loss 22884.891849571	
    Loss 22495.802625562	
    Loss 22112.882469208	
    Loss 21737.137836338	
    Loss 21368.06845677	
    Loss 21005.508406602	
    Loss 20649.110159053	
    Loss 20299.055588076	
    Loss 19954.797388778	
    Loss 19617.051678979	
    Loss 19284.677517003	
    Loss 18958.307488295	
    Loss 18637.459312778	
    Loss 18322.276066578	
    Loss 18012.65104229	
    Loss 17707.981219322	
    Loss 17408.422531079	
    Loss 17114.554298436	
    Loss 16825.538478077	
    Loss 16541.814623196	
    Loss 16263.408106151	
    Loss 15989.13089708	
    Loss 15720.308138328	
    Loss 15455.971539523	
    Loss 15195.470312674	
    Loss 14939.941481581	
    Loss 14689.131109968	
    Loss 14442.382288883	
    Loss 14200.050098533	
    Loss 13961.815528377	
    Loss 13727.911696679	
    Loss 13498.039626089	
    Loss 13272.258458789	
    Loss 13050.441297924	
    Loss 12832.505438867	
    Loss 12618.393313229	
    Loss 12407.858083711	
    Loss 12201.02620529	
    Loss 11997.754668262	
    Loss 11797.265701048	
    Loss 11600.975561798	
    Loss 11407.89113219	
    Loss 11218.496578392	
    Loss 11032.758774703	
    Loss 10850.098947152	
    Loss 10670.675459124	
    Loss 10493.858066986	
    Loss 10320.029415939	
    Loss 10149.48356077	
    Loss 9981.4172391285	
    Loss 9816.9850681187	
    Loss 9655.3561102226	
    Loss 9496.3580243061	
    Loss 9340.126509627	
    Loss 9186.5141315627	
    Loss 9035.6023127145	
    Loss 8886.9912557499	
    Loss 8741.1091385639	
    Loss 8598.2120414913	
    Loss 8457.3602092437	
    Loss 8318.8644668587	
    Loss 8182.9955910096	
    Loss 8049.6746821286	
    Loss 7918.4885118559	
    Loss 7789.8443960974	
    Loss 7663.3762552438	
    Loss 7538.9061248889	
    Loss 7416.4374693053	
    Loss 7296.1462496829	
    Loss 7178.3604035926	
    Loss 7062.3526889942	
    Loss 6948.6246962658	
    Loss 6836.7114176847	
Epoch 4	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	35.134944900236	
    Loss 6770.336514669	
    Loss 6661.2440911037	
    Loss 6554.589632659	
    Loss 6449.6811122813	
    Loss 6346.2317493447	
    Loss 6244.629656847	
    Loss 6144.9309559393	
    Loss 6046.8390456335	
    Loss 5950.7748938361	
    Loss 5856.2203909577	
    Loss 5763.4177911141	
    Loss 5671.7755065132	
    Loss 5581.7686345379	
    Loss 5493.5148862946	
    Loss 5406.6256989246	
    Loss 5321.5157326972	
    Loss 5237.9184041863	
    Loss 5155.9146287286	
    Loss 5075.2803008832	
    Loss 4995.7721627322	
    Loss 4917.9149546571	
    Loss 4841.566780908	
    Loss 4766.3873658233	
    Loss 4692.4989599434	
    Loss 4619.6752052525	
    Loss 4548.0147471355	
    Loss 4477.7817498627	
    Loss 4408.6177344399	
    Loss 4340.8521810485	
    Loss 4274.3003544321	
    Loss 4209.1560743573	
    Loss 4144.942250768	
    Loss 4081.7003415195	
    Loss 4019.5069892003	
    Loss 3958.4574509396	
    Loss 3898.3353060455	
    Loss 3839.1813431154	
    Loss 3780.9501943912	
    Loss 3724.3042315306	
    Loss 3668.1514808215	
    Loss 3613.1894418772	
    Loss 3559.3393296427	
    Loss 3505.9818492264	
    Loss 3453.6916094355	
    Loss 3402.5918327654	
    Loss 3352.0870988456	
    Loss 3302.6954246778	
    Loss 3254.1435336497	
    Loss 3206.3496959207	
    Loss 3159.3553201759	
    Loss 3113.0180635249	
    Loss 3067.9691030172	
    Loss 3022.82360313	
    Loss 2979.1293554726	
    Loss 2936.3194309048	
    Loss 2894.4931027975	
    Loss 2853.1163964604	
    Loss 2812.2585526595	
    Loss 2772.4186841941	
    Loss 2733.5471668352	
    Loss 2694.9853842307	
    Loss 2657.0247891302	
    Loss 2619.7287294269	
    Loss 2582.7642275707	
    Loss 2546.8617671739	
    Loss 2511.5215847759	
    Loss 2476.8727274689	
    Loss 2442.8192239131	
    Loss 2409.2941495549	
    Loss 2376.0606662605	
    Loss 2343.5924732789	
    Loss 2311.7894170995	
    Loss 2280.0787274384	
    Loss 2249.1354191035	
    Loss 2218.8828169404	
    Loss 2189.4729861436	
    Loss 2160.2374326663	
    Loss 2131.7094414201	
    Loss 2103.4970037881	
    Loss 2076.1247412993	
    Loss 2048.7634480276	
    Loss 2022.179105714	
    Loss 1995.852828782	
    Loss 1970.0606679024	
    Loss 1944.837155223	
    Loss 1919.581225969	
    Loss 1894.5751992287	
    Loss 1870.5137878115	
    Loss 1846.6082028058	
    Loss 1823.4286681387	
    Loss 1800.9706880346	
    Loss 1778.220604125	
    Loss 1756.5802785263	
    Loss 1735.0778778815	
    Loss 1713.2748117978	
    Loss 1692.2249068153	
    Loss 1671.8004214798	
    Loss 1651.4354762516	
    Loss 1631.6262783679	
    Loss 1612.0746831976	
    Loss 1592.9276435789	
    Loss 1574.1169157975	
    Loss 1555.6643206552	
    Loss 1537.5923393719	
    Loss 1519.8472180806	
    Loss 1502.4189461118	
    Loss 1485.183361848	
    Loss 1468.3366410623	
    Loss 1451.7046993139	
    Loss 1434.6209156964	
    Loss 1418.5578661622	
    Loss 1402.5062199881	
    Loss 1387.2049985681	
    Loss 1372.4183446795	
    Loss 1357.8349484077	
    Loss 1343.4829079571	
    Loss 1328.9123544962	
    Loss 1314.4860878399	
    Loss 1300.5439983385	
    Loss 1286.4059645312	
    Loss 1273.232534283	
    Loss 1260.2315171572	
    Loss 1247.1894382652	
    Loss 1234.4198070427	
    Loss 1221.7552848608	
    Loss 1209.3851568648	
    Loss 1196.8890852998	
    Loss 1184.7406543672	
    Loss 1173.2917169899	
    Loss 1161.6045862972	
    Loss 1149.9925211958	
    Loss 1138.7756094349	
    Loss 1127.9083899927	
    Loss 1117.1199606925	
    Loss 1106.7969129259	
    Loss 1096.5321936167	
    Loss 1086.2601543208	
    Loss 1075.9834435806	
    Loss 1065.9491413461	
    Loss 1056.5373122486	
    Loss 1046.9883752472	
    Loss 1037.8316511971	
    Loss 1028.7287931317	
Epoch 5	
 131808
[torch.DoubleTensor of size 1]

Validation accuracy:	0.10011531925225	
Grad norm	13.958239587722	
    Loss 1023.1834163036	
    Loss 1014.0325224513	
    Loss 1005.5514060876	
    Loss 997.19096583943	
    Loss 988.57148620883	
    Loss 980.15076625104	
    Loss 971.97829203614	
    Loss 963.87956803897	
    Loss 956.20085875952	
    Loss 948.52541265615	
    Loss 941.0795721793	
    Loss 933.33029157397	
    Loss 925.69692573402	
    Loss 918.42171800654	
    Loss 911.12009703383	
    Loss 904.20792389894	
    Loss 897.43858826751	
    Loss 890.93657453933	
    Loss 884.49804336802	
    Loss 877.86415097337	
    Loss 871.5700249177	
    Loss 865.60010730142	
    Loss 859.59328080365	
    Loss 853.65128061656	
    Loss 847.59314730347	
    Loss 841.52023033294	
    Loss 835.72619547472	
    Loss 829.89518280683	
    Loss 824.34660605686	
    Loss 818.93955980513	
    Loss 813.90679953735	
    Loss 808.70439109124	
    Loss 803.4689917112	
    Loss 798.31597701072	
    Loss 793.29820786335	
    Loss 788.19235134035	
    Loss 783.14295159908	
    Loss 778.07498982924	
    Loss 773.61612195852	
    Loss 768.81488489216	
    Loss 764.24506448575	
    Loss 759.96258371282	
    Loss 755.29935784903	
    Loss 750.86022808906	
    Loss 746.77369568422	
    Loss 742.4568703803	
    Loss 738.43370731877	
    Loss 734.47369621083	
    Loss 730.53702154155	
    Loss 726.58638162879	
    Loss 722.55584691454	
    Loss 719.057161266	
    Loss 714.79338639226	
    Loss 711.25467173694	
    Loss 707.91849675648	
    Loss 704.84793671397	
    Loss 701.55335147628	
    Loss 698.10141098497	
    Loss 695.0100932794	
    Loss 692.24573233403	
    Loss 689.14147954744	
