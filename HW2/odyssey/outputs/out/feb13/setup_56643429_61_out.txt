[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	3	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1428
 4842
 4160
 1066
 3175
 2866
 2388
 3895
 3006
 1043
 1376
 1490
 3108
  853
 5279
 1033
 1590
 5657
 1064
 3827
 4930
 1835
 5385
 3119
 1421
 1193
 3807
 1522
 2398
 2484
 2037
 3123
 2891
 6275
 3123
 1285
 1113
 5726
 7711
 5048
  951
 3589
 2152
 3953
 1591
[torch.DoubleTensor of size 45]

Validation accuracy:	0.02737314882253	
Grad norm	0	
    Loss 22760295.080682	
    Loss 9791531.4347958	
    Loss 4212577.5411185	
    Loss 1812554.0880222	
    Loss 780095.84799865	
    Loss 335930.2014137	
    Loss 144838.89970835	
    Loss 62635.463663204	
    Loss 27273.442954211	
    Loss 12063.524342329	
    Loss 5512.0427798394	
    Loss 2699.4473023118	
    Loss 1487.9061989994	
    Loss 961.88037609849	
    Loss 738.90464820597	
    Loss 643.44378883437	
    Loss 604.15861643797	
    Loss 586.06219699224	
    Loss 574.2786058229	
    Loss 577.72001361276	
    Loss 571.93161249095	
    Loss 570.8173399177	
    Loss 573.31515422394	
    Loss 571.76641011464	
    Loss 573.82810291616	
    Loss 568.36370842681	
    Loss 560.02511915367	
    Loss 573.14330024367	
    Loss 574.70160873328	
    Loss 573.47191958058	
    Loss 569.68794514111	
    Loss 569.28165594456	
    Loss 575.48770453015	
    Loss 570.80426560583	
    Loss 569.19935673239	
    Loss 569.2830242546	
    Loss 568.89363462289	
    Loss 568.00453897992	
    Loss 569.22354582811	
    Loss 574.19547227798	
    Loss 571.10124208753	
    Loss 565.22524285374	
    Loss 574.36895877759	
    Loss 568.27553112328	
    Loss 567.33056464869	
    Loss 575.04931305027	
    Loss 572.28130526941	
    Loss 562.36677990961	
    Loss 569.31754640575	
    Loss 570.12940530474	
    Loss 569.34295807819	
    Loss 569.28958888027	
    Loss 572.24045231756	
    Loss 575.14359356525	
    Loss 575.91087083849	
    Loss 570.53868748172	
    Loss 567.36719635055	
    Loss 577.68214741142	
    Loss 574.36998098355	
    Loss 573.81725121383	
    Loss 576.66621603766	
    Loss 571.68614641769	
    Loss 570.86387863768	
    Loss 570.04721575148	
    Loss 573.90966865991	
    Loss 569.58873743083	
    Loss 566.21222323776	
    Loss 570.74006212299	
    Loss 570.31807731102	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 2	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.9638231943544	
    Loss 563.42067684209	
    Loss 570.92733508722	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597107	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 3	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 4	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 5	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736117	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 6	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233435	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 7	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736117	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 8	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 9	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 10	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
    Loss 571.68614641793	
    Loss 570.86387863767	
    Loss 570.04721575158	
    Loss 573.90966865987	
    Loss 569.5887374308	
    Loss 566.21222323777	
    Loss 570.74006212298	
    Loss 570.31807731103	
    Loss 570.81016988856	
    Loss 566.4177442393	
    Loss 569.50162529039	
Epoch 11	
 82613
   549
     0
   239
     1
     0
     0
   365
 46188
  1853
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11776978635591	
Grad norm	6.963823194338	
    Loss 563.42067684209	
    Loss 570.92733508723	
    Loss 573.8450186713	
    Loss 571.08004774682	
    Loss 573.25004239577	
    Loss 575.8218584752	
    Loss 568.28405762215	
    Loss 566.62195505971	
    Loss 570.52133599713	
    Loss 575.85567600484	
    Loss 570.23075274033	
    Loss 573.71298890048	
    Loss 573.08738450403	
    Loss 568.18652920217	
    Loss 569.47343597108	
    Loss 570.5052010889	
    Loss 572.80070070302	
    Loss 572.57174562485	
    Loss 568.44807986514	
    Loss 575.20839999154	
    Loss 570.85462377527	
    Loss 570.34367640975	
    Loss 573.11482583917	
    Loss 571.67783600729	
    Loss 573.79109881352	
    Loss 568.34789666451	
    Loss 560.01861440154	
    Loss 573.14026507298	
    Loss 574.70038298638	
    Loss 573.4713529056	
    Loss 569.68768617987	
    Loss 569.28141474905	
    Loss 575.48758868092	
    Loss 570.80422736116	
    Loss 569.19934984496	
    Loss 569.28301083282	
    Loss 568.89362303376	
    Loss 568.004531306	
    Loss 569.22354418942	
    Loss 574.19547160771	
    Loss 571.10124177642	
    Loss 565.22524260733	
    Loss 574.36895915602	
    Loss 568.27553086989	
    Loss 567.33056461037	
    Loss 575.0493129685	
    Loss 572.28130535813	
    Loss 562.36677985929	
    Loss 569.31754639042	
    Loss 570.1294053166	
    Loss 569.34295810002	
    Loss 569.28958890005	
    Loss 572.24045233436	
    Loss 575.14359356805	
    Loss 575.91087084378	
    Loss 570.53868748384	
    Loss 567.36719635012	
    Loss 577.68214741028	
    Loss 574.36998098288	
    Loss 573.81725121387	
    Loss 576.66621603777	
