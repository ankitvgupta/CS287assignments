[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	0.5	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4715
 1221
 1498
 1762
 5529
 3916
 2244
 1798
 1238
 5267
 4617
 7208
 3457
 1907
 1808
  696
 1958
 1181
 4678
  721
  351
 3805
 2897
 2588
 2107
 2339
 2907
 1183
  904
 1011
 2535
 4492
 1865
 4713
 4294
 4867
 3929
 1290
 3992
 3019
 2156
 6954
 3665
 2315
 4211
[torch.DoubleTensor of size 45]

Validation accuracy:	0.021887897547949	
Grad norm	0	
    Loss 2277897.0029179	
    Loss 2246069.782374	
    Loss 2214696.7725605	
    Loss 2183763.4822597	
    Loss 2153264.9087738	
    Loss 2123192.95017	
    Loss 2093543.8654008	
    Loss 2064311.7390164	
    Loss 2035487.8062753	
    Loss 2007066.1545928	
    Loss 1979042.2124325	
    Loss 1951409.9210796	
    Loss 1924165.5016587	
    Loss 1897302.7048565	
    Loss 1870814.1886452	
    Loss 1844694.5816992	
    Loss 1818940.9671509	
    Loss 1793547.7610134	
    Loss 1768507.9078243	
    Loss 1743820.2816825	
    Loss 1719476.6603817	
    Loss 1695473.0798552	
    Loss 1671805.3598984	
    Loss 1648467.6127563	
    Loss 1625457.465292	
    Loss 1602768.4274614	
    Loss 1580397.1323157	
    Loss 1558337.1598196	
    Loss 1536584.495899	
    Loss 1515135.8190476	
    Loss 1493987.7117417	
    Loss 1473135.1502974	
    Loss 1452573.135765	
    Loss 1432299.0096042	
    Loss 1412305.921411	
    Loss 1392593.9676832	
    Loss 1373158.6895763	
    Loss 1353993.8947612	
    Loss 1335095.7655153	
    Loss 1316461.6351394	
    Loss 1298087.59585	
    Loss 1279971.8402648	
    Loss 1262108.1414865	
    Loss 1244494.5890988	
    Loss 1227126.3023901	
    Loss 1209998.6367542	
    Loss 1193112.3524657	
    Loss 1176461.8492086	
    Loss 1160042.9461708	
    Loss 1143854.1715668	
    Loss 1127888.9482702	
    Loss 1112147.6592581	
    Loss 1096625.9230399	
    Loss 1081321.318734	
    Loss 1066231.0084561	
    Loss 1051351.8726803	
    Loss 1036679.3654006	
    Loss 1022211.8388836	
    Loss 1007946.0780137	
    Loss 993879.39225809	
    Loss 980009.70499875	
    Loss 966333.46600824	
    Loss 952849.05652598	
    Loss 939553.02856042	
    Loss 926440.86449636	
    Loss 913513.16162399	
    Loss 900766.34709555	
    Loss 888197.11640721	
    Loss 875803.03211279	
    Loss 863581.18316219	
    Loss 851530.81181518	
    Loss 839648.20118668	
Epoch 2	
 37501
 11984
   553
 13470
 10639
    10
    85
  9239
 31631
 12897
  1206
   737
    78
     6
   238
   611
   315
   259
   312
    10
    12
     9
     6
[torch.DoubleTensor of size 23]

Validation accuracy:	0.09088219227968	
Grad norm	314.03722194208	
    Loss 836116.92109238	
    Loss 824449.96467696	
    Loss 812945.38365993	
    Loss 801601.91840167	
    Loss 790416.37747755	
    Loss 779387.12793704	
    Loss 768512.38917022	
    Loss 757790.16205058	
    Loss 747216.77075141	
    Loss 736790.60604366	
    Loss 726509.93176189	
    Loss 716372.78895742	
    Loss 706377.65299954	
    Loss 696522.60459699	
    Loss 686804.10792228	
    Loss 677220.86307495	
    Loss 667771.8146122	
    Loss 658454.71974371	
    Loss 649266.79238522	
    Loss 640208.57466795	
    Loss 631276.25952854	
    Loss 622468.7050863	
    Loss 613784.37087745	
    Loss 605220.64008419	
    Loss 596777.69862513	
    Loss 588452.16981277	
    Loss 580243.09701989	
    Loss 572147.95878404	
    Loss 564165.37895982	
    Loss 556294.29731781	
    Loss 548533.54966918	
    Loss 540881.1328788	
    Loss 533335.20030705	
    Loss 525894.82288871	
    Loss 518557.29682056	
    Loss 511323.24884443	
    Loss 504190.70841128	
    Loss 497157.24122175	
    Loss 490221.65767801	
    Loss 483382.84896359	
    Loss 476639.29889137	
    Loss 469990.73598293	
    Loss 463434.47012952	
    Loss 456970.15141519	
    Loss 450595.67955581	
    Loss 444309.34455955	
    Loss 438111.87598379	
    Loss 432000.66862406	
    Loss 425974.45365364	
    Loss 420032.72497748	
    Loss 414172.9311073	
    Loss 408395.29121196	
    Loss 402698.03672129	
    Loss 397080.52768879	
    Loss 391541.74190363	
    Loss 386080.53034096	
    Loss 380694.90703943	
    Loss 375384.76810766	
    Loss 370148.61210666	
    Loss 364985.21117792	
    Loss 359894.33207728	
    Loss 354874.4856668	
    Loss 349924.95372787	
    Loss 345044.52056923	
    Loss 340231.53462982	
    Loss 335486.22071131	
    Loss 330807.32832284	
    Loss 326193.55440768	
    Loss 321644.15441999	
    Loss 317157.84549274	
    Loss 312734.5456835	
    Loss 308372.79352749	
Epoch 3	
 52785
  7902
    27
 10371
  6626
     0
     3
  4983
 38985
  9882
   129
    49
     4
     0
    14
    32
    10
     3
     3
[torch.DoubleTensor of size 19]

Validation accuracy:	0.097755826656956	
Grad norm	192.27094345889	
    Loss 307076.61487218	
    Loss 302793.93402033	
    Loss 298570.9170012	
    Loss 294407.04669559	
    Loss 290301.07978403	
    Loss 286252.60828373	
    Loss 282260.65950538	
    Loss 278324.70550032	
    Loss 274443.34341613	
    Loss 270616.10746272	
    Loss 266842.16113876	
    Loss 263120.97279747	
    Loss 259451.87589622	
    Loss 255834.150566	
    Loss 252266.50799227	
    Loss 248748.46186563	
    Loss 245279.77597261	
    Loss 241859.5442285	
    Loss 238486.45837118	
    Loss 235161.32919266	
    Loss 231882.20866143	
    Loss 228648.99055273	
    Loss 225461.08038612	
    Loss 222317.24638602	
    Loss 219217.99012875	
    Loss 216161.61886246	
    Loss 213148.08028201	
    Loss 210176.365773	
    Loss 207245.9022096	
    Loss 204356.2936417	
    Loss 201507.19403801	
    Loss 198697.88573113	
    Loss 195927.61801442	
    Loss 193196.11991437	
    Loss 190502.24711201	
    Loss 187846.46490825	
    Loss 185228.00490311	
    Loss 182645.80848835	
    Loss 180099.57264243	
    Loss 177588.83423948	
    Loss 175113.05525112	
    Loss 172672.23819979	
    Loss 170265.21624631	
    Loss 167891.8932554	
    Loss 165551.6738535	
    Loss 163243.7718168	
    Loss 160968.5221025	
    Loss 158724.78826251	
    Loss 156512.35004625	
    Loss 154330.90074773	
    Loss 152179.50990002	
    Loss 150058.28308787	
    Loss 147966.49252694	
    Loss 145903.9939308	
    Loss 143870.50392305	
    Loss 141865.43060528	
    Loss 139888.04306526	
    Loss 137938.56774579	
    Loss 136016.17895232	
    Loss 134120.2955954	
    Loss 132251.22108261	
    Loss 130408.19411112	
    Loss 128590.98402214	
    Loss 126799.13133273	
    Loss 125032.04117092	
    Loss 123289.72367179	
    Loss 121571.80597011	
    Loss 119877.8384049	
    Loss 118207.55661269	
    Loss 116560.35346746	
    Loss 114936.32576557	
    Loss 113334.88301593	
Epoch 4	
 68792
  3344
     0
  5581
  2653
     0
     0
  1622
 44229
  5587
[torch.DoubleTensor of size 10]

Validation accuracy:	0.1032183175528	
Grad norm	118.55332194558	
    Loss 112858.98517423	
    Loss 111286.55697105	
    Loss 109736.11511529	
    Loss 108207.31041192	
    Loss 106699.79883536	
    Loss 105213.39766856	
    Loss 103747.66166012	
    Loss 102302.45362994	
    Loss 100877.35597735	
    Loss 99472.201608128	
    Loss 98086.512199564	
    Loss 96720.258848344	
    Loss 95373.087746162	
    Loss 94044.708232061	
    Loss 92734.748198049	
    Loss 91442.965706644	
    Loss 90169.388268069	
    Loss 88913.595566679	
    Loss 87674.945469094	
    Loss 86454.149018619	
    Loss 85250.10916751	
    Loss 84062.992884959	
    Loss 82892.560291387	
    Loss 81738.19178338	
    Loss 80600.316000848	
    Loss 79478.039193515	
    Loss 78371.526789687	
    Loss 77280.435976752	
    Loss 76204.471773757	
    Loss 75143.416447062	
    Loss 74097.258788265	
    Loss 73065.73561715	
    Loss 72048.548173361	
    Loss 71045.604782558	
    Loss 70056.382629093	
    Loss 69081.177406907	
    Loss 68119.718883896	
    Loss 67171.516683946	
    Loss 66236.544714989	
    Loss 65314.599658186	
    Loss 64405.499340446	
    Loss 63509.276781324	
    Loss 62625.43402269	
    Loss 61753.856927561	
    Loss 60894.57868617	
    Loss 60047.174894944	
    Loss 59211.73464809	
    Loss 58387.750925993	
    Loss 57575.353907714	
    Loss 56774.295069978	
    Loss 55984.278888653	
    Loss 55205.348329408	
    Loss 54437.19025705	
    Loss 53679.78881487	
    Loss 52933.125903008	
    Loss 52196.798135677	
    Loss 51470.634204289	
    Loss 50754.888574158	
    Loss 50049.00690921	
    Loss 49352.711962599	
    Loss 48666.404500303	
    Loss 47989.616211812	
    Loss 47322.329407487	
    Loss 46664.347243211	
    Loss 46015.464347363	
    Loss 45375.616584879	
    Loss 44744.727514341	
    Loss 44122.683256618	
    Loss 43509.373899402	
    Loss 42904.477295925	
    Loss 42308.134683123	
    Loss 41720.078101291	
Epoch 5	
 81225
   647
     0
  1618
   526
     0
     0
   215
 45818
  1759
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10400734401554	
Grad norm	73.916029218389	
    Loss 41545.303583654	
    Loss 40967.897377624	
    Loss 40398.631902357	
    Loss 39837.220084431	
    Loss 39283.685370241	
    Loss 38737.875352472	
    Loss 38199.593346579	
    Loss 37668.803098907	
    Loss 37145.499287786	
    Loss 36629.578772852	
    Loss 36120.708953268	
    Loss 35619.034381805	
    Loss 35124.323909055	
    Loss 34636.443884328	
    Loss 34155.381835678	
    Loss 33680.981247925	
    Loss 33213.321742673	
    Loss 32752.169593268	
    Loss 32297.199617306	
    Loss 31848.979539676	
    Loss 31406.811693867	
    Loss 30970.908791832	
    Loss 30541.17128613	
    Loss 30117.241034843	
    Loss 29699.442765812	
    Loss 29287.266981849	
    Loss 28880.88429629	
    Loss 28480.256857861	
    Loss 28085.18072999	
    Loss 27695.488138399	
    Loss 27311.296918537	
    Loss 26932.498610024	
    Loss 26558.991408656	
    Loss 26190.710282613	
    Loss 25827.400160717	
    Loss 25469.22627065	
    Loss 25116.136644369	
    Loss 24767.877651458	
    Loss 24424.505770212	
    Loss 24085.925007084	
    Loss 23752.07080445	
    Loss 23422.952190594	
    Loss 23098.388593528	
    Loss 22778.202745708	
    Loss 22462.673049563	
    Loss 22151.534244855	
    Loss 21844.748189539	
    Loss 21542.068365752	
    Loss 21243.731582635	
    Loss 20949.515229891	
    Loss 20659.373008426	
    Loss 20373.308390869	
    Loss 20091.184759851	
    Loss 19813.009201292	
    Loss 19538.842458968	
    Loss 19268.361999213	
    Loss 19001.640243553	
    Loss 18738.894785211	
    Loss 18479.688997719	
    Loss 18223.889377528	
    Loss 17971.87629631	
    Loss 17723.308997319	
    Loss 17478.256582226	
    Loss 17236.613958116	
    Loss 16998.322389674	
    Loss 16763.296855476	
    Loss 16531.554483566	
    Loss 16303.105806417	
    Loss 16077.893249646	
    Loss 15855.72348364	
    Loss 15636.743544879	
    Loss 15420.798571018	
Epoch 6	
 88568
    32
     0
   228
    27
     0
     0
     9
 42690
   254
[torch.DoubleTensor of size 10]

Validation accuracy:	0.10170854576353	
Grad norm	46.889385591714	
    Loss 15356.582221891	
    Loss 15144.546013487	
    Loss 14935.555663424	
    Loss 14729.358508871	
    Loss 14526.118500469	
    Loss 14325.682055455	
    Loss 14127.959606014	
    Loss 13932.931963517	
    Loss 13740.770211233	
    Loss 13551.375601394	
    Loss 13364.472370367	
    Loss 13180.265606221	
    Loss 12998.574663318	
    Loss 12819.322721515	
    Loss 12642.640134982	
    Loss 12468.402809956	
    Loss 12296.67918596	
    Loss 12127.310109241	
    Loss 11960.130494522	
    Loss 11795.592544257	
    Loss 11633.193837232	
    Loss 11473.136219077	
    Loss 11315.378433584	
    Loss 11159.673993973	
    Loss 11006.26997792	
    Loss 10854.850578449	
    Loss 10705.542076594	
    Loss 10558.453064779	
    Loss 10413.405706785	
    Loss 10270.247484203	
    Loss 10129.141994358	
    Loss 9990.0309378622	
    Loss 9852.9039764238	
    Loss 9717.6822354129	
    Loss 9584.2352484098	
    Loss 9452.6384019151	
    Loss 9322.9421925614	
    Loss 9194.9928186443	
    Loss 9068.8692338495	
    Loss 8944.5210430424	
    Loss 8821.9170763659	
    Loss 8701.0380796372	
    Loss 8581.8638102378	
    Loss 8464.1616875667	
    Loss 8348.3035248657	
    Loss 8234.0992669834	
    Loss 8121.4480845972	
    Loss 8010.2113386497	
    Loss 7900.6519536895	
    Loss 7792.5611177472	
    Loss 7685.9927954105	
    Loss 7580.9251763965	
    Loss 7477.3025287278	
    Loss 7375.1272378994	
    Loss 7274.4742656322	
    Loss 7175.0629238648	
    Loss 7077.072930582	
    Loss 6980.6817484795	
    Loss 6885.5084921866	
    Loss 6791.4893211679	
    Loss 6698.9655194508	
    Loss 6607.6551517797	
    Loss 6517.6618615949	
    Loss 6428.9124957527	
    Loss 6341.4048730052	
    Loss 6255.0497945479	
    Loss 6169.8914486015	
    Loss 6085.9837975332	
    Loss 6003.2872477941	
    Loss 5921.6704525025	
    Loss 5841.2772641099	
    Loss 5761.9888597949	
Epoch 7	
 93161
     0
     0
     2
     0
     0
     0
     0
 38617
    28
[torch.DoubleTensor of size 10]

Validation accuracy:	0.097740653071134	
Grad norm	30.532845736027	
    Loss 5738.3671712072	
    Loss 5660.5113753237	
    Loss 5583.8233232669	
    Loss 5508.0724205426	
    Loss 5433.4733310118	
    Loss 5359.869334707	
    Loss 5287.2164033105	
    Loss 5215.4898329055	
    Loss 5144.9399777	
    Loss 5075.4600287655	
    Loss 5006.7957500497	
    Loss 4939.17371256	
    Loss 4872.4348205543	
    Loss 4806.5225707203	
    Loss 4741.6257806983	
    Loss 4677.6335472707	
    Loss 4614.5916603402	
    Loss 4552.3716255853	
    Loss 4490.8937810222	
    Loss 4430.5300332485	
    Loss 4370.8782491984	
    Loss 4312.1210581575	
    Loss 4254.2428200293	
    Loss 4197.0456287918	
    Loss 4140.7260951416	
    Loss 4085.0706742611	
    Loss 4030.1615354975	
    Loss 3976.1808169651	
    Loss 3922.9559772084	
    Loss 3870.3398289577	
    Loss 3818.5102087941	
    Loss 3767.4232895281	
    Loss 3717.1143400659	
    Loss 3667.4852440415	
    Loss 3618.4673574977	
    Loss 3570.0779775441	
    Loss 3522.4187124865	
    Loss 3475.3790542786	
    Loss 3429.0426596092	
    Loss 3383.3790729687	
    Loss 3338.3632620415	
    Loss 3293.9543793058	
    Loss 3250.2173873445	
    Loss 3206.8802788204	
    Loss 3164.350260961	
    Loss 3122.4748030112	
    Loss 3081.1224752667	
    Loss 3040.199996339	
    Loss 2999.9723560579	
    Loss 2960.2382190214	
    Loss 2921.0939095411	
    Loss 2882.5015634468	
    Loss 2844.4450573703	
    Loss 2806.9174958938	
    Loss 2769.989300575	
    Loss 2733.4039865673	
    Loss 2697.3897205256	
    Loss 2662.0937963479	
    Loss 2627.1637731698	
    Loss 2592.5708096837	
    Loss 2558.6238400632	
    Loss 2525.0693599749	
    Loss 2492.0247111306	
    Loss 2459.4277921507	
    Loss 2427.2987889549	
    Loss 2395.5486975435	
    Loss 2364.2283115684	
    Loss 2333.405345034	
    Loss 2303.046275405	
    Loss 2273.0513106782	
    Loss 2243.5602349844	
    Loss 2214.4634996086	
Epoch 8	
 95423
     0
     0
     0
     0
     0
     0
     0
 36380
     5
[torch.DoubleTensor of size 10]

Validation accuracy:	0.09610949259529	
Grad norm	20.64708733195	
    Loss 2205.7473227252	
    Loss 2177.1732807798	
    Loss 2149.0727951284	
    Loss 2121.2297033789	
    Loss 2093.8755927858	
    Loss 2066.8522072808	
    Loss 2040.1352367196	
    Loss 2013.6908126893	
    Loss 1987.8070545396	
    Loss 1962.3687332213	
    Loss 1937.1295934998	
    Loss 1912.3240931628	
    Loss 1887.8025361744	
    Loss 1863.515676487	
    Loss 1839.6772094174	
    Loss 1816.1821424801	
    Loss 1793.0567652478	
    Loss 1770.1855518961	
    Loss 1747.5363350124	
    Loss 1725.4287641453	
    Loss 1703.514578041	
    Loss 1681.9606118658	
    Loss 1660.7632372833	
    Loss 1639.746250369	
    Loss 1619.0751810629	
    Loss 1598.5925257128	
    Loss 1578.347706742	
    Loss 1558.5612870306	
    Loss 1539.0602269531	
    Loss 1519.6995605563	
    Loss 1500.660315479	
    Loss 1481.9015457978	
    Loss 1463.482822233	
    Loss 1445.2904591091	
    Loss 1427.2880597676	
    Loss 1409.456729405	
    Loss 1391.9248390037	
    Loss 1374.6014591837	
    Loss 1357.5702493081	
    Loss 1340.8097411369	
    Loss 1324.2935246755	
    Loss 1307.9662293958	
    Loss 1291.9399202503	
    Loss 1275.9165265275	
    Loss 1260.3168008698	
    Loss 1245.0077724764	
    Loss 1229.8429991035	
    Loss 1214.7486735787	
    Loss 1199.9862263437	
    Loss 1185.3582821982	
    Loss 1170.9803587864	
    Loss 1156.8044997151	
    Loss 1142.835044708	
    Loss 1129.0572729117	
    Loss 1115.5338149429	
    Loss 1102.0229257966	
    Loss 1088.775351578	
    Loss 1075.9183060778	
    Loss 1063.1146712396	
    Loss 1050.3538755784	
    Loss 1037.9226349869	
    Loss 1025.5817246324	
    Loss 1013.4539940519	
    Loss 1001.4811402831	
    Loss 989.69241634391	
    Loss 977.99975509857	
    Loss 966.45490466916	
    Loss 955.12897454011	
    Loss 943.99038414426	
    Loss 932.95621512643	
    Loss 922.16306284666	
    Loss 911.50220553889	
Epoch 9	
 96631
     0
     0
     0
     0
     0
     0
     0
 35173
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.095631524641903	
Grad norm	14.688175113615	
    Loss 908.25853825511	
    Loss 897.78583654869	
    Loss 887.5290249346	
    Loss 877.28124742752	
    Loss 867.27892665587	
    Loss 857.36337862752	
    Loss 847.51911367336	
    Loss 837.70461850083	
    Loss 828.22707044089	
    Loss 818.96491543425	
    Loss 809.67479028912	
    Loss 800.594263179	
    Loss 791.5777716641	
    Loss 782.57871953193	
    Loss 773.82186306877	
    Loss 765.20520241261	
    Loss 756.74112187858	
    Loss 748.31975318438	
    Loss 739.93608070695	
    Loss 731.87692879698	
    Loss 723.82400110041	
    Loss 715.93321688194	
    Loss 708.20698507921	
    Loss 700.47906877785	
    Loss 692.89632707469	
    Loss 685.33282070336	
    Loss 677.81615382783	
    Loss 670.58763528859	
    Loss 663.4726526931	
    Loss 656.32732852125	
    Loss 649.33247441601	
    Loss 642.44668921474	
    Loss 635.74319308457	
    Loss 629.09746282996	
    Loss 622.49055918284	
    Loss 615.88153940142	
    Loss 609.41329757832	
    Loss 603.00422011712	
    Loss 596.73709476132	
    Loss 590.5946293019	
    Loss 584.54779144357	
    Loss 578.53195594983	
    Loss 572.68598250158	
    Loss 566.69516433352	
    Loss 560.98558740483	
    Loss 555.43483714213	
    Loss 549.88903816966	
    Loss 544.28292934996	
    Loss 538.87415073341	
    Loss 533.46736807601	
    Loss 528.18753208144	
    Loss 522.98039810216	
    Loss 517.86130406047	
    Loss 512.81006236679	
    Loss 507.8826819955	
    Loss 502.84656378317	
    Loss 497.96333188991	
    Loss 493.34757011196	
    Loss 488.67071712492	
    Loss 483.93206989553	
    Loss 479.40421044218	
    Loss 474.85530495927	
    Loss 470.41049851676	
    Loss 466.01278524151	
    Loss 461.69517499846	
    Loss 457.37039355827	
    Loss 453.08992383389	
    Loss 448.92496164595	
    Loss 444.84447706017	
    Loss 440.77493440789	
    Loss 436.85072959212	
    Loss 432.96182898901	
Epoch 10	
 97003
     0
     0
     0
     0
     0
     0
     0
 34801
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094440398154892	
Grad norm	11.113452548611	
    Loss 431.72695484638	
    Loss 427.90314223352	
    Loss 424.19894532273	
    Loss 420.41340502301	
    Loss 416.78376682283	
    Loss 413.15139568991	
    Loss 409.50489791063	
    Loss 405.79741513545	
    Loss 402.34623323354	
    Loss 399.02552533373	
    Loss 395.59293744232	
    Loss 392.28742603384	
    Loss 388.96523333832	
    Loss 385.58079073135	
    Loss 382.36400258604	
    Loss 379.21457143145	
    Loss 376.13570192525	
    Loss 373.02007499855	
    Loss 369.87847463244	
    Loss 366.97752860934	
    Loss 364.01602835863	
    Loss 361.14285595052	
    Loss 358.36366101883	
    Loss 355.51690133218	
    Loss 352.73835542753	
    Loss 349.92020557882	
    Loss 347.07625644136	
    Loss 344.45930939856	
    Loss 341.89338226079	
    Loss 339.23518791685	
    Loss 336.66456824804	
    Loss 334.13901319254	
    Loss 331.73969759856	
    Loss 329.33502949408	
    Loss 326.9157736121	
    Loss 324.42773924399	
    Loss 322.02190014288	
    Loss 319.6213187375	
    Loss 317.30800964879	
    Loss 315.06667802236	
    Loss 312.86610359986	
    Loss 310.63591638588	
    Loss 308.53052273293	
    Loss 306.22479153435	
    Loss 304.14701979257	
    Loss 302.18064461782	
    Loss 300.16807709842	
    Loss 298.04787924655	
    Loss 296.07486545333	
    Loss 294.05472594171	
    Loss 292.11742236449	
    Loss 290.20482237517	
    Loss 288.33833643944	
    Loss 286.49424709555	
    Loss 284.72384484461	
    Loss 282.80022636988	
    Loss 280.99042452228	
    Loss 279.40131297491	
    Loss 277.70921937862	
    Loss 275.91891860809	
    Loss 274.29433631647	
    Loss 272.60754978059	
    Loss 270.98472426517	
    Loss 269.36929346272	
    Loss 267.79584980301	
    Loss 266.17765327201	
    Loss 264.5658559998	
    Loss 263.0308978362	
    Loss 261.5418604734	
    Loss 260.03057698044	
    Loss 258.63000160069	
    Loss 257.22862305387	
Epoch 11	
 97263
     0
     0
     0
     0
     0
     0
     0
 34541
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094569373634377	
Grad norm	8.9853287356946	
    Loss 256.73082936337	
    Loss 255.34923364127	
    Loss 254.05090877746	
    Loss 252.63857876758	
    Loss 251.34916964985	
    Loss 250.02423737891	
    Loss 248.65435845579	
    Loss 247.18921925985	
    Loss 245.95167830175	
    Loss 244.81315685814	
    Loss 243.53162453746	
    Loss 242.34676228048	
    Loss 241.11557875089	
    Loss 239.7929495025	
    Loss 238.61128407669	
    Loss 237.47133450553	
    Loss 236.37047338258	
    Loss 235.20256700714	
    Loss 233.98769402595	
    Loss 232.98021081792	
    Loss 231.88881852507	
    Loss 230.85801135142	
    Loss 229.89527502433	
    Loss 228.84132497276	
    Loss 227.82540169133	
    Loss 226.75022664495	
    Loss 225.62111728847	
    Loss 224.69736455551	
    Loss 223.80203065174	
    Loss 222.79216278339	
    Loss 221.84669217578	
    Loss 220.92211629129	
    Loss 220.1043565187	
    Loss 219.2573579087	
    Loss 218.37738017867	
    Loss 217.40233801836	
    Loss 216.48777595552	
    Loss 215.55922373212	
    Loss 214.69817171957	
    Loss 213.89030639877	
    Loss 213.10289720078	
    Loss 212.2620558088	
    Loss 211.53132777176	
    Loss 210.5791193652	
    Loss 209.83471618911	
    Loss 209.18486405517	
    Loss 208.47010736215	
    Loss 207.63074163084	
    Loss 206.91971224018	
    Loss 206.14321027435	
    Loss 205.4340298451	
    Loss 204.7316510522	
    Loss 204.06089603055	
    Loss 203.39585768979	
    Loss 202.7847097519	
    Loss 202.0040886058	
    Loss 201.32379319563	
    Loss 200.84606711154	
    Loss 200.25007202843	
    Loss 199.54368550116	
    Loss 198.98563832531	
    Loss 198.35011001755	
    Loss 197.76376089575	
    Loss 197.17017313774	
    Loss 196.60461002826	
    Loss 195.98066475909	
    Loss 195.34929834593	
    Loss 194.78014909947	
    Loss 194.24229131202	
    Loss 193.67068985088	
    Loss 193.19737761016	
    Loss 192.7096908712	
Epoch 12	
 97280
     0
     0
     0
     0
     0
     0
     0
 34524
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094501092498179	
Grad norm	7.7306694754147	
    Loss 192.4821715086	
    Loss 191.99757488921	
    Loss 191.58231041988	
    Loss 191.04137843531	
    Loss 190.61119188928	
    Loss 190.133503058	
    Loss 189.59986415881	
    Loss 188.95781898378	
    Loss 188.53338452613	
    Loss 188.19625350605	
    Loss 187.70447614426	
    Loss 187.29816783873	
    Loss 186.8346426845	
    Loss 186.26899737499	
    Loss 185.83491074208	
    Loss 185.4338088753	
    Loss 185.05943594324	
    Loss 184.60622541965	
    Loss 184.09975977741	
    Loss 183.78699723028	
    Loss 183.38244913836	
    Loss 183.02792016927	
    Loss 182.73197789239	
    Loss 182.33644032288	
    Loss 181.96669911733	
    Loss 181.5316582906	
    Loss 181.03149692619	
    Loss 180.72922184463	
    Loss 180.44728473786	
    Loss 180.04292408624	
    Loss 179.69439680038	
    Loss 179.35747292818	
    Loss 179.12094584686	
    Loss 178.84599153203	
    Loss 178.53202161722	
    Loss 178.11226927923	
    Loss 177.74489339513	
    Loss 177.35678145202	
    Loss 177.02910144983	
    Loss 176.74803386209	
    Loss 176.47988589044	
    Loss 176.14862981944	
    Loss 175.92322968354	
    Loss 175.46811126018	
    Loss 175.21304401221	
    Loss 175.04667180219	
    Loss 174.808598187	
    Loss 174.43990898138	
    Loss 174.19237916223	
    Loss 173.87243741587	
    Loss 173.61454862881	
    Loss 173.35673472953	
    Loss 173.1257554533	
    Loss 172.89437506368	
    Loss 172.70878781269	
    Loss 172.3478210546	
    Loss 172.08272228647	
    Loss 172.01298002781	
    Loss 171.81941343455	
    Loss 171.51168283994	
    Loss 171.34543639084	
    Loss 171.09599529084	
    Loss 170.89028447249	
    Loss 170.67193678777	
    Loss 170.47649713331	
    Loss 170.21776069202	
    Loss 169.94661338198	
    Loss 169.73205611754	
    Loss 169.54313550439	
    Loss 169.31666624204	
    Loss 169.18410060853	
    Loss 169.03197958695	
Epoch 13	
 97283
     0
     0
     0
     0
     0
     0
     0
 34521
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094485918912357	
Grad norm	6.9970295054731	
    Loss 168.90345550177	
    Loss 168.74826786798	
    Loss 168.65694029376	
    Loss 168.4358697112	
    Loss 168.32103164675	
    Loss 168.15433768755	
    Loss 167.92783326786	
    Loss 167.58776548534	
    Loss 167.46197549576	
    Loss 167.41908569327	
    Loss 167.21714607607	
    Loss 167.09655617253	
    Loss 166.91473531239	
    Loss 166.62690568107	
    Loss 166.46741146741	
    Loss 166.33811792204	
    Loss 166.23053891614	
    Loss 166.03938622332	
    Loss 165.79351376393	
    Loss 165.73544137175	
    Loss 165.5831108523	
    Loss 165.47669312158	
    Loss 165.42540971442	
    Loss 165.27162232845	
    Loss 165.13845787826	
    Loss 164.93846805881	
    Loss 164.66874920824	
    Loss 164.59445704388	
    Loss 164.53766962821	
    Loss 164.35573423152	
    Loss 164.22645407567	
    Loss 164.1051071741	
    Loss 164.08223815628	
    Loss 164.0173214249	
    Loss 163.9115927708	
    Loss 163.69551688603	
    Loss 163.52875870296	
    Loss 163.33897763487	
    Loss 163.20715200255	
    Loss 163.11971611417	
    Loss 163.04238905574	
    Loss 162.89784692775	
    Loss 162.85828388101	
    Loss 162.58568451564	
    Loss 162.51007675964	
    Loss 162.52120465245	
    Loss 162.45819817582	
    Loss 162.2625014582	
    Loss 162.18517695902	
    Loss 162.03276166181	
    Loss 161.94073167684	
    Loss 161.84622242931	
    Loss 161.77710120091	
    Loss 161.70533632685	
    Loss 161.67589247927	
    Loss 161.46894726706	
    Loss 161.35652166748	
    Loss 161.43647285727	
    Loss 161.39059474774	
    Loss 161.22958222545	
    Loss 161.20726565862	
    Loss 161.09959012667	
    Loss 161.03363442866	
    Loss 160.95304940699	
    Loss 160.8934940117	
    Loss 160.76888499718	
    Loss 160.63009720887	
    Loss 160.54567551468	
    Loss 160.48464411017	
    Loss 160.38491871106	
    Loss 160.37758900937	
    Loss 160.34867724464	
Epoch 14	
 97259
     0
     0
     0
     0
     0
     0
     0
 34545
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094539026462734	
Grad norm	6.569569958816	
    Loss 160.25634738702	
    Loss 160.22209560117	
    Loss 160.2494835169	
    Loss 160.14576046903	
    Loss 160.14658582392	
    Loss 160.09398817007	
    Loss 159.9802652566	
    Loss 159.75089105989	
    Loss 159.73477323234	
    Loss 159.7998772919	
    Loss 159.70423060803	
    Loss 159.68842572392	
    Loss 159.60990454358	
    Loss 159.42396697018	
    Loss 159.36531553071	
    Loss 159.33610174506	
    Loss 159.32647834323	
    Loss 159.23129662194	
    Loss 159.08135925137	
    Loss 159.11653148309	
    Loss 159.05678946044	
    Loss 159.04131641474	
    Loss 159.07973515911	
    Loss 159.01467657083	
    Loss 158.96793424258	
    Loss 158.85422023878	
    Loss 158.66879097476	
    Loss 158.6780537122	
    Loss 158.70386302537	
    Loss 158.60362728201	
    Loss 158.55485900017	
    Loss 158.51251635929	
    Loss 158.56821604515	
    Loss 158.58039246132	
    Loss 158.55134386819	
    Loss 158.40989996582	
    Loss 158.3165974074	
    Loss 158.19954509114	
    Loss 158.13961179615	
    Loss 158.12336463345	
    Loss 158.11617151202	
    Loss 158.03992464875	
    Loss 158.06875051757	
    Loss 157.86314097268	
    Loss 157.85327190991	
    Loss 157.92952983813	
    Loss 157.93080767503	
    Loss 157.79870875277	
    Loss 157.78386828055	
    Loss 157.69286972061	
    Loss 157.66180970888	
    Loss 157.62728670896	
    Loss 157.6178034488	
    Loss 157.60485506996	
    Loss 157.6326524242	
    Loss 157.48220199527	
    Loss 157.42594424905	
    Loss 157.56077271882	
    Loss 157.56905656889	
    Loss 157.46209512543	
    Loss 157.49265047176	
    Loss 157.43700968571	
    Loss 157.42234649855	
    Loss 157.39231876584	
    Loss 157.38262773555	
    Loss 157.30726349436	
    Loss 157.21711712485	
    Loss 157.18042371595	
    Loss 157.16619367569	
    Loss 157.11300098354	
    Loss 157.15171034216	
    Loss 157.16801553876	
Epoch 15	
 97211
     0
     0
     0
     0
     0
     0
     0
 34593
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094523852876912	
Grad norm	6.3224414718303	
    Loss 157.08887998605	
    Loss 157.09900471047	
    Loss 157.16982802533	
    Loss 157.10911762301	
    Loss 157.15232003368	
    Loss 157.14154536522	
    Loss 157.06922097642	
    Loss 156.88036423256	
    Loss 156.90451116905	
    Loss 157.00922651725	
    Loss 156.95251272072	
    Loss 156.9750946752	
    Loss 156.93440620479	
    Loss 156.78579748346	
    Loss 156.76416804766	
    Loss 156.77185497902	
    Loss 156.7981839103	
    Loss 156.73807733568	
    Loss 156.62350022333	
    Loss 156.69273311237	
    Loss 156.66696192203	
    Loss 156.68477422908	
    Loss 156.75604102435	
    Loss 156.72352627204	
    Loss 156.70823777964	
    Loss 156.62617041482	
    Loss 156.47148564735	
    Loss 156.5113233608	
    Loss 156.56740608069	
    Loss 156.49717597661	
    Loss 156.47796441873	
    Loss 156.46452618276	
    Loss 156.54913532821	
    Loss 156.58959250169	
    Loss 156.5888176891	
    Loss 156.47467546772	
    Loss 156.40820863797	
    Loss 156.31779226219	
    Loss 156.28423219399	
    Loss 156.29416612389	
    Loss 156.31275512812	
    Loss 156.26141773363	
    Loss 156.31543899395	
    Loss 156.13439990126	
    Loss 156.14856502972	
    Loss 156.24869809291	
    Loss 156.27357458946	
    Loss 156.16486557138	
    Loss 156.17295213282	
    Loss 156.10443565224	
    Loss 156.09579598227	
    Loss 156.08330730234	
    Loss 156.09583768432	
    Loss 156.10460217745	
    Loss 156.15335323295	
    Loss 156.02360438403	
    Loss 155.98802645525	
    Loss 156.14294427269	
    Loss 156.17106724007	
    Loss 156.08404947167	
    Loss 156.13402629671	
    Loss 156.09747244067	
    Loss 156.10162225049	
    Loss 156.09013679193	
    Loss 156.09872968985	
    Loss 156.04143555971	
    Loss 155.96916920333	
    Loss 155.94996141459	
    Loss 155.95281534494	
    Loss 155.91669950123	
    Loss 155.97233964141	
    Loss 156.00522504743	
Epoch 16	
 97212
     0
     0
     0
     0
     0
     0
     0
 34592
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094523852876912	
Grad norm	6.1816104963071	
    Loss 155.93087633368	
    Loss 155.95727029386	
    Loss 156.04394230122	
    Loss 155.99897445352	
    Loss 156.05767511745	
    Loss 156.06220944451	
    Loss 156.00507116398	
    Loss 155.83100962528	
    Loss 155.86993290955	
    Loss 155.98916218124	
    Loss 155.94667875388	
    Loss 155.98329791755	
    Loss 155.95643691525	
    Loss 155.82147891792	
    Loss 155.81343342315	
    Loss 155.83475690423	
    Loss 155.8742737437	
    Loss 155.82694329814	
    Loss 155.72542426764	
    Loss 155.80705223623	
    Loss 155.79373381721	
    Loss 155.82369779004	
    Loss 155.90696915847	
    Loss 155.88637720652	
    Loss 155.88246607679	
    Loss 155.81199370945	
    Loss 155.66847097122	
    Loss 155.71946819048	
    Loss 155.78663037364	
    Loss 155.72741807875	
    Loss 155.71905173011	
    Loss 155.71615834964	
    Loss 155.81141528429	
    Loss 155.8622375863	
    Loss 155.87191037413	
    Loss 155.76772845257	
    Loss 155.7110291519	
    Loss 155.63034696668	
    Loss 155.60645151338	
    Loss 155.6260185031	
    Loss 155.65408822257	
    Loss 155.61179300474	
    Loss 155.67511360459	
    Loss 155.50307609264	
    Loss 155.52599959412	
    Loss 155.63487015709	
    Loss 155.66840610178	
    Loss 155.56830484138	
    Loss 155.58479696317	
    Loss 155.52448922813	
    Loss 155.52409864771	
    Loss 155.51970401647	
    Loss 155.54038368827	
    Loss 155.55718732375	
    Loss 155.61359005895	
    Loss 155.49141468829	
    Loss 155.46345980448	
    Loss 155.62571462281	
    Loss 155.661090081	
    Loss 155.58144920992	
    Loss 155.63855987813	
    Loss 155.60899969806	
    Loss 155.62004233868	
    Loss 155.61535063681	
    Loss 155.6306389317	
    Loss 155.57996906831	
    Loss 155.51427823908	
    Loss 155.50146497805	
    Loss 155.51052915842	
    Loss 155.48067604572	
    Loss 155.54254591037	
    Loss 155.58150195387	
Epoch 17	
 97195
     0
     0
     0
     0
     0
     0
     0
 34609
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094516266084001	
Grad norm	6.1025613752977	
    Loss 155.50887621716	
    Loss 155.54122576721	
    Loss 155.63365427241	
    Loss 155.59443418804	
    Loss 155.65878599349	
    Loss 155.66891126096	
    Loss 155.61733735473	
    Loss 155.44865696426	
    Loss 155.49299910511	
    Loss 155.61753733934	
    Loss 155.58023676323	
    Loss 155.62197383467	
    Loss 155.60014939205	
    Loss 155.4701682749	
    Loss 155.46710201716	
    Loss 155.49348334364	
    Loss 155.53783335625	
    Loss 155.49513072621	
    Loss 155.39844839434	
    Loss 155.48455883699	
    Loss 155.47579849279	
    Loss 155.51018013127	
    Loss 155.59782440241	
    Loss 155.58159220199	
    Loss 155.5817537301	
    Loss 155.51552141092	
    Loss 155.3760198108	
    Loss 155.43107285391	
    Loss 155.50228017752	
    Loss 155.44711216467	
    Loss 155.4427220378	
    Loss 155.44365718615	
    Loss 155.54284231622	
    Loss 155.59745806838	
    Loss 155.61100461017	
    Loss 155.51044022029	
    Loss 155.45727388518	
    Loss 155.38013634858	
    Loss 155.35977620697	
    Loss 155.38288989969	
    Loss 155.41444754485	
    Loss 155.37540854105	
    Loss 155.44217182115	
    Loss 155.2734257981	
    Loss 155.29952357958	
    Loss 155.41158265098	
    Loss 155.44829420451	
    Loss 155.35136396992	
    Loss 155.37093356379	
    Loss 155.31361008427	
    Loss 155.31625727909	
    Loss 155.31483644508	
    Loss 155.33854702815	
    Loss 155.35834115297	
    Loss 155.41752675215	
    Loss 155.29811477638	
    Loss 155.27297526711	
    Loss 155.43789929298	
    Loss 155.47591729066	
    Loss 155.39901558429	
    Loss 155.45874663159	
    Loss 155.43174433689	
    Loss 155.44530799972	
    Loss 155.44310158609	
    Loss 155.4608362896	
    Loss 155.41259071584	
    Loss 155.349320139	
    Loss 155.3388385217	
    Loss 155.35014459911	
    Loss 155.32258580021	
    Loss 155.38674994815	
    Loss 155.42792297981	
Epoch 18	
 97198
     0
     0
     0
     0
     0
     0
     0
 34606
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094470745326536	
Grad norm	6.0585929858258	
    Loss 155.35590926695	
    Loss 155.39043335223	
    Loss 155.48493667661	
    Loss 155.44780679949	
    Loss 155.51420896247	
    Loss 155.52636849672	
    Loss 155.47682950454	
    Loss 155.31009336487	
    Loss 155.35642054052	
    Loss 155.48289532498	
    Loss 155.44747098919	
    Loss 155.49106494995	
    Loss 155.47106461252	
    Loss 155.34288896607	
    Loss 155.34164480321	
    Loss 155.36991322981	
    Loss 155.4160320238	
    Loss 155.37499008353	
    Loss 155.28010776771	
    Loss 155.36782210232	
    Loss 155.36072606799	
    Loss 155.39670241491	
    Loss 155.48593114492	
    Loss 155.47128810503	
    Loss 155.47288144118	
    Loss 155.4081945202	
    Loss 155.27012258839	
    Loss 155.32663928951	
    Loss 155.39931773751	
    Loss 155.3456334563	
    Loss 155.34269926274	
    Loss 155.34501314541	
    Loss 155.44565145047	
    Loss 155.50165236869	
    Loss 155.51664302148	
    Loss 155.41738279163	
    Loss 155.36548082645	
    Loss 155.28962628392	
    Loss 155.27055544658	
    Loss 155.29497617644	
    Loss 155.32781796691	
    Loss 155.28993569359	
    Loss 155.35797982609	
    Loss 155.19043352404	
    Loss 155.2176711612	
    Loss 155.33088810259	
    Loss 155.36876318316	
    Loss 155.273003104	
    Loss 155.29369692181	
    Loss 155.23745037312	
    Loss 155.24121787431	
    Loss 155.24089016962	
    Loss 155.26573644596	
    Loss 155.28665133803	
    Loss 155.34684218913	
    Loss 155.22843418441	
    Loss 155.20433759481	
    Loss 155.37022641504	
    Loss 155.40920199652	
    Loss 155.33332353723	
    Loss 155.39401719147	
    Loss 155.36794756138	
    Loss 155.38243063449	
    Loss 155.38113115011	
    Loss 155.39975646689	
    Loss 155.35239556245	
    Loss 155.29001717332	
    Loss 155.28038165521	
    Loss 155.29248763207	
    Loss 155.26576791097	
    Loss 155.33077821239	
    Loss 155.37275729954	
Epoch 19	
 97184
     0
     0
     0
     0
     0
     0
     0
 34620
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094478332119446	
Grad norm	6.0343151057858	
    Loss 155.30095609319	
    Loss 155.33627069101	
    Loss 155.43151206406	
    Loss 155.3951373079	
    Loss 155.4622772014	
    Loss 155.47517241704	
    Loss 155.42637514951	
    Loss 155.26033366419	
    Loss 155.30738672379	
    Loss 155.43456471294	
    Loss 155.39981254511	
    Loss 155.44407478669	
    Loss 155.42472877154	
    Loss 155.29720265867	
    Loss 155.29662338059	
    Loss 155.32560256877	
    Loss 155.37236724639	
    Loss 155.33191178075	
    Loss 155.23770433586	
    Loss 155.3259821294	
    Loss 155.31949134073	
    Loss 155.35603646151	
    Loss 155.44583426586	
    Loss 155.43176741822	
    Loss 155.43384808586	
    Loss 155.36972127509	
    Loss 155.23214608997	
    Loss 155.28918471097	
    Loss 155.36239474581	
    Loss 155.30925434372	
    Loss 155.30685215004	
    Loss 155.30965560091	
    Loss 155.4108339409	
    Loss 155.46733880968	
    Loss 155.48287242513	
    Loss 155.38407652587	
    Loss 155.33261875623	
    Loss 155.2572238532	
    Loss 155.23862080764	
    Loss 155.26352383684	
    Loss 155.29683896646	
    Loss 155.25935787776	
    Loss 155.32788241618	
    Loss 155.16077107674	
    Loss 155.18841154412	
    Loss 155.3020454125	
    Loss 155.34034625811	
    Loss 155.24501918098	
    Loss 155.26612218422	
    Loss 155.21025933959	
    Loss 155.21444098606	
    Loss 155.21451555574	
    Loss 155.2397923964	
    Loss 155.26113226204	
    Loss 155.32168207124	
    Loss 155.2036362418	
    Loss 155.17992782833	
    Loss 155.34616159315	
    Loss 155.38548099953	
    Loss 155.3099883651	
    Loss 155.37103575806	
    Loss 155.34530444803	
    Loss 155.36012124561	
    Loss 155.35915143233	
    Loss 155.37809898515	
    Loss 155.3310593262	
    Loss 155.26901072994	
    Loss 155.25967979288	
    Loss 155.27206544522	
    Loss 155.24565176611	
    Loss 155.31097486279	
    Loss 155.35324476574	
Epoch 20	
 97190
     0
     0
     0
     0
     0
     0
     0
 34614
     4
[torch.DoubleTensor of size 10]

Validation accuracy:	0.094470745326536	
Grad norm	6.0210180891888	
    Loss 155.28151436709	
    Loss 155.31711419598	
    Loss 155.41261209794	
    Loss 155.37650714816	
    Loss 155.44390872574	
    Loss 155.45706733132	
    Loss 155.40853889839	
    Loss 155.24274084242	
    Loss 155.29005860478	
    Loss 155.41749005056	
    Loss 155.38297431296	
    Loss 155.42747378938	
    Loss 155.40835863647	
    Loss 155.28106291088	
    Loss 155.28072508636	
    Loss 155.30997596389	
    Loss 155.35697567273	
    Loss 155.31672153292	
    Loss 155.22277001879	
    Loss 155.31123933231	
    Loss 155.30496717421	
    Loss 155.34171092926	
    Loss 155.43171011085	
    Loss 155.41785030863	
    Loss 155.42008678342	
    Loss 155.35616102804	
    Loss 155.21875130103	
    Loss 155.27597220808	
    Loss 155.34937228608	
    Loss 155.29643098313	
    Loss 155.29422253605	
    Loss 155.29719552522	
    Loss 155.39857606664	
    Loss 155.45526321463	
    Loss 155.47100368578	
    Loss 155.37236957589	
    Loss 155.32106271861	
    Loss 155.24582957424	
    Loss 155.22739476962	
    Loss 155.2524760894	
    Loss 155.28596602289	
    Loss 155.24861803128	
    Loss 155.31732509241	
    Loss 155.15037003693	
    Loss 155.1781488653	
    Loss 155.29193065346	
    Loss 155.33038701584	
    Loss 155.23522089103	
    Loss 155.2564719259	
    Loss 155.20074273185	
    Loss 155.20507807274	
    Loss 155.20530102974	
    Loss 155.23074407196	
    Loss 155.25224798371	
    Loss 155.3129234224	
    Loss 155.19500670244	
    Loss 155.17144384854	
    Loss 155.33779864391	
    Loss 155.37723957477	
    Loss 155.30189449287	
    Loss 155.36307198738	
    Loss 155.33746232819	
    Loss 155.35239930969	
    Loss 155.35154859351	
    Loss 155.37061152014	
    Loss 155.32368752678	
    Loss 155.26176142594	
    Loss 155.25253865987	
    Loss 155.26501857902	
    Loss 155.23871604774	
    Loss 155.30415520878	
    Loss 155.34652861963	
Done	
