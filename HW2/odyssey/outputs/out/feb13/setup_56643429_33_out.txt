[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	3	Lambda:	5	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4129
 2288
 6090
 2471
 4701
 1645
 2068
 3210
 1028
 4443
 2604
 2465
 4535
 3429
 8708
 3228
 1977
 1084
 3335
  995
 1387
 2137
 1212
 5189
 2281
 3136
 4746
 2915
 1064
 3271
 2550
 1567
 4933
 3981
 2612
 1147
 2447
 2227
 5631
 3463
 1267
 1724
 1214
 1563
 3711
[torch.DoubleTensor of size 45]

Validation accuracy:	0.024945375091042	
Grad norm	0	
    Loss 11388950.020646	
    Loss 9225349.8723172	
    Loss 7472901.7448372	
    Loss 6053434.8172953	
    Loss 4903639.3473844	
    Loss 3972301.8463283	
    Loss 3217879.0128468	
    Loss 2606766.6691677	
    Loss 2111749.9304121	
    Loss 1710770.2350844	
    Loss 1385953.1902953	
    Loss 1122860.0752246	
    Loss 909718.7625938	
    Loss 737072.1713071	
    Loss 597219.6080052	
    Loss 483923.60814922	
    Loss 392152.85507133	
    Loss 317813.23809642	
    Loss 257590.46272435	
    Loss 208814.01299761	
    Loss 169291.81917456	
    Loss 137285.01426418	
    Loss 111350.78466249	
    Loss 90343.40246732	
    Loss 73326.270197694	
    Loss 59548.019829088	
    Loss 48378.123304316	
    Loss 39331.981805014	
    Loss 32007.720371852	
    Loss 26068.754660625	
    Loss 21263.023267231	
    Loss 17364.726641947	
    Loss 14212.409925525	
    Loss 11655.957896604	
    Loss 9584.85863403	
    Loss 7908.1077927354	
    Loss 6545.4719659961	
    Loss 5446.4407562453	
    Loss 4555.0859153023	
    Loss 3831.2312283986	
    Loss 3249.5762879634	
    Loss 2772.2261214268	
    Loss 2383.0747149812	
    Loss 2075.457665231	
    Loss 1827.9833567911	
    Loss 1622.620010594	
    Loss 1458.1283361664	
    Loss 1324.6966365197	
    Loss 1211.4332202681	
    Loss 1122.3969752259	
    Loss 1047.1124482669	
    Loss 996.03866739599	
    Loss 942.20687676711	
    Loss 910.7478586592	
    Loss 883.6264725782	
    Loss 861.06075749246	
    Loss 837.94679936397	
    Loss 822.28786542461	
    Loss 813.37574597048	
    Loss 805.33110103454	
    Loss 791.436833953	
    Loss 785.24944210864	
    Loss 780.36681603427	
    Loss 770.06396094838	
    Loss 767.74373221209	
    Loss 762.87185548849	
    Loss 759.65994934584	
    Loss 760.40812605977	
    Loss 751.10685627616	
    Loss 752.7704088736	
    Loss 751.97145221151	
    Loss 751.37720960941	
    Loss 749.66762935559	
    Loss 750.39992019873	
    Loss 752.10493276512	
    Loss 750.92137566752	
    Loss 750.56422341136	
    Loss 750.46024552952	
    Loss 750.48626810442	
    Loss 758.23741361947	
    Loss 751.67192022028	
    Loss 752.92131339252	
    Loss 745.85328288569	
    Loss 749.98768736034	
    Loss 759.23806926941	
    Loss 749.22509494833	
    Loss 746.44753143586	
    Loss 749.24784067362	
    Loss 747.74037722084	
    Loss 750.38646330637	
    Loss 753.77833579931	
    Loss 750.69078182186	
    Loss 754.08253019992	
    Loss 753.20123226383	
    Loss 744.98224392977	
    Loss 743.99529634197	
    Loss 743.4385199289	
    Loss 738.02991442332	
    Loss 742.35170434427	
    Loss 744.55334286854	
    Loss 747.06310640343	
    Loss 748.88294436061	
    Loss 747.56375976586	
    Loss 751.02277117802	
    Loss 753.48898522668	
    Loss 757.19944969928	
    Loss 752.97544079278	
    Loss 750.72035123825	
    Loss 752.37510872036	
    Loss 742.73405919822	
    Loss 747.47921706865	
    Loss 745.99926811878	
    Loss 743.63058924909	
    Loss 747.20296001181	
    Loss 750.93993360624	
    Loss 759.43654728986	
    Loss 753.5167081375	
    Loss 755.65570049693	
    Loss 756.0914192783	
    Loss 752.39593587587	
    Loss 753.55332080973	
    Loss 754.53242218965	
    Loss 753.9632582049	
    Loss 754.62984251355	
    Loss 750.51678185581	
    Loss 748.93672404529	
    Loss 747.74991637901	
    Loss 748.75824120949	
    Loss 754.05757331174	
    Loss 750.94168580382	
    Loss 747.65610657957	
    Loss 746.80428143532	
    Loss 744.82955969212	
    Loss 741.3349677349	
    Loss 748.19802984286	
    Loss 746.80354830569	
    Loss 748.05610095863	
    Loss 748.61459203433	
    Loss 752.47375018515	
    Loss 753.14916282993	
    Loss 749.46313340954	
    Loss 753.4756295476	
    Loss 750.63634813079	
Epoch 2	
 124311
      0
      0
      0
    752
      0
      0
     36
   6709
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006979849478	
Grad norm	6.4673144031197	
    Loss 742.8257298051	
    Loss 745.57149233603	
    Loss 754.26664980049	
    Loss 751.11628019058	
    Loss 753.03210471191	
    Loss 750.2111435368	
    Loss 752.52571183902	
    Loss 746.612223276	
    Loss 755.09044683802	
    Loss 754.37760120305	
    Loss 760.756559345	
    Loss 757.2427431488	
    Loss 749.41181170671	
    Loss 747.51700552344	
    Loss 750.55686944098	
    Loss 747.57145693442	
    Loss 751.89616250352	
    Loss 755.7620961054	
    Loss 755.18019279811	
    Loss 754.23483457491	
    Loss 752.30525353027	
    Loss 754.69885172702	
    Loss 755.4770205565	
    Loss 755.66838813635	
    Loss 750.38707961939	
    Loss 754.52389482339	
    Loss 751.46475525718	
    Loss 750.61530420687	
    Loss 752.44201585536	
    Loss 749.95572291855	
    Loss 752.5000645213	
    Loss 748.54547591577	
    Loss 752.09208138735	
    Loss 751.89457760346	
    Loss 751.72472785861	
    Loss 753.20063348792	
    Loss 749.67170147098	
    Loss 751.71124591933	
    Loss 751.50972943668	
    Loss 750.22138573607	
    Loss 753.85334479364	
    Loss 750.65407050164	
    Loss 745.53567446851	
    Loss 748.58445977007	
    Loss 753.03215928122	
    Loss 751.97381896225	
    Loss 752.74790972905	
    Loss 753.23757001566	
    Loss 748.34319757989	
    Loss 747.20697627708	
    Loss 743.07889471808	
    Loss 749.67114591521	
    Loss 742.61141430787	
    Loss 748.93148909965	
    Loss 752.50143114731	
    Loss 754.84903538088	
    Loss 751.96332752278	
    Loss 752.63312716791	
    Loss 756.98209807729	
    Loss 759.680200686	
    Loss 754.42873286996	
    Loss 755.28968062846	
    Loss 756.13406027239	
    Loss 750.41484825655	
    Loss 751.84960601729	
    Loss 749.98622324433	
    Loss 749.18716318889	
    Loss 751.93433166869	
    Loss 744.22046937228	
    Loss 747.19775226383	
    Loss 747.46213836313	
    Loss 747.7314601494	
    Loss 746.72513368111	
    Loss 748.01049783983	
    Loss 750.17952828764	
    Loss 749.36378022209	
    Loss 749.30076093548	
    Loss 749.44010722744	
    Loss 749.66047786149	
    Loss 757.57263813374	
    Loss 751.13342328907	
    Loss 752.49025697607	
    Loss 745.50664097609	
    Loss 749.70352827246	
    Loss 759.00978021787	
    Loss 749.04025123003	
    Loss 746.29817620734	
    Loss 749.12778097855	
    Loss 747.64210649936	
    Loss 750.30525146999	
    Loss 753.71416179255	
    Loss 750.63855537848	
    Loss 754.04101305325	
    Loss 753.16740807103	
    Loss 744.95445854531	
    Loss 743.97214523617	
    Loss 743.41990732457	
    Loss 738.01338114104	
    Loss 742.33855147285	
    Loss 744.54281894558	
    Loss 747.05484527082	
    Loss 748.87631718271	
    Loss 747.55826660899	
    Loss 751.01885918508	
    Loss 753.48580937766	
    Loss 757.19702452069	
    Loss 752.97359382334	
    Loss 750.71892788456	
    Loss 752.37368879196	
    Loss 742.73290162872	
    Loss 747.47854621067	
    Loss 745.99869326361	
    Loss 743.63028573792	
    Loss 747.20268831922	
    Loss 750.93980960093	
    Loss 759.43645649557	
    Loss 753.51666925373	
    Loss 755.65563155966	
    Loss 756.09136091646	
    Loss 752.39597374131	
    Loss 753.5534016992	
    Loss 754.53246566787	
    Loss 753.96325565866	
    Loss 754.62986245915	
    Loss 750.5168356724	
    Loss 748.93679014354	
    Loss 747.75001610383	
    Loss 748.75834164881	
    Loss 754.05766369693	
    Loss 750.94181826885	
    Loss 747.65621773153	
    Loss 746.80437359463	
    Loss 744.8296400242	
    Loss 741.33506427522	
    Loss 748.19813997656	
    Loss 746.80366580865	
    Loss 748.0562030965	
    Loss 748.61468266393	
    Loss 752.47384191017	
    Loss 753.14924156562	
    Loss 749.46320804967	
    Loss 753.4757077652	
    Loss 750.63642674546	
Epoch 3	
 124311
      0
      0
      0
    752
      0
      0
     36
   6709
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006979849478	
Grad norm	6.4671854055808	
    Loss 742.82580717362	
    Loss 745.57155073823	
    Loss 754.26670041737	
    Loss 751.11632262094	
    Loss 753.03213864008	
    Loss 750.21116967983	
    Loss 752.52573203053	
    Loss 746.61223840617	
    Loss 755.09045885554	
    Loss 754.37761006438	
    Loss 760.75656549465	
    Loss 757.24274514968	
    Loss 749.41181206482	
    Loss 747.51700609045	
    Loss 750.55686956804	
    Loss 747.57145684137	
    Loss 751.89616207164	
    Loss 755.76209567102	
    Loss 755.18019280164	
    Loss 754.23483338886	
    Loss 752.30525258955	
    Loss 754.69885029586	
    Loss 755.47701973679	
    Loss 755.66838821324	
    Loss 750.38707916942	
    Loss 754.52389402885	
    Loss 751.46475486636	
    Loss 750.61530392248	
    Loss 752.44201531453	
    Loss 749.95572256891	
    Loss 752.5000641142	
    Loss 748.5454753989	
    Loss 752.09208093703	
    Loss 751.89457721932	
    Loss 751.72472750731	
    Loss 753.20063333041	
    Loss 749.67170138631	
    Loss 751.71124594703	
    Loss 751.50972938999	
    Loss 750.22138574351	
    Loss 753.85334485539	
    Loss 750.65407058439	
    Loss 745.53567456179	
    Loss 748.58445977018	
    Loss 753.03215924673	
    Loss 751.97381897752	
    Loss 752.74790973648	
    Loss 753.23757001408	
    Loss 748.34319754775	
    Loss 747.20697625221	
    Loss 743.07889466929	
    Loss 749.67114586089	
    Loss 742.611414268	
    Loss 748.93148903235	
    Loss 752.50143107842	
    Loss 754.84903532068	
    Loss 751.96332749278	
    Loss 752.63312715052	
    Loss 756.98209807022	
    Loss 759.68020068733	
    Loss 754.42873286806	
    Loss 755.28968063059	
    Loss 756.13406028477	
    Loss 750.41484826073	
    Loss 751.84960602534	
    Loss 749.98622324896	
    Loss 749.18716318404	
    Loss 751.93433166815	
    Loss 744.22046936535	
    Loss 747.19775225969	
    Loss 747.46213836202	
    Loss 747.73146014955	
    Loss 746.72513368395	
    Loss 748.01049784064	
    Loss 750.17952829083	
    Loss 749.36378022584	
    Loss 749.3007609381	
    Loss 749.44010723031	
    Loss 749.66047786413	
    Loss 757.57263813713	
    Loss 751.13342329152	
    Loss 752.49025697919	
    Loss 745.50664097913	
    Loss 749.70352827394	
    Loss 759.00978021984	
    Loss 749.0402512318	
    Loss 746.29817620895	
    Loss 749.12778098035	
    Loss 747.6421065005	
    Loss 750.30525147039	
    Loss 753.71416179322	
    Loss 750.63855537893	
    Loss 754.04101305383	
    Loss 753.16740807142	
    Loss 744.95445854556	
    Loss 743.97214523624	
    Loss 743.41990732462	
    Loss 738.01338114074	
    Loss 742.33855147266	
    Loss 744.5428189455	
    Loss 747.0548452708	
    Loss 748.8763171827	
    Loss 747.55826660897	
    Loss 751.01885918519	
    Loss 753.48580937772	
    Loss 757.19702452077	
    Loss 752.97359382344	
    Loss 750.71892788468	
    Loss 752.37368879195	
    Loss 742.73290162871	
    Loss 747.47854621076	
    Loss 745.99869326368	
    Loss 743.63028573797	
    Loss 747.20268831925	
    Loss 750.939809601	
    Loss 759.43645649563	
    Loss 753.51666925379	
    Loss 755.65563155968	
    Loss 756.0913609165	
    Loss 752.39597374135	
    Loss 753.55340169923	
    Loss 754.53246566789	
    Loss 753.96325565867	
    Loss 754.62986245914	
    Loss 750.51683567241	
    Loss 748.93679014358	
    Loss 747.75001610386	
    Loss 748.75834164885	
    Loss 754.05766369695	
    Loss 750.94181826889	
    Loss 747.65621773158	
    Loss 746.80437359466	
    Loss 744.82964002422	
    Loss 741.33506427524	
    Loss 748.19813997659	
    Loss 746.80366580867	
    Loss 748.05620309652	
    Loss 748.61468266395	
    Loss 752.47384191019	
    Loss 753.14924156565	
    Loss 749.46320804969	
    Loss 753.47570776523	
    Loss 750.63642674548	
Epoch 4	
 124311
      0
      0
      0
    752
      0
      0
     36
   6709
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006979849478	
Grad norm	6.4671854055805	
    Loss 742.82580717363	
    Loss 745.57155073824	
    Loss 754.26670041738	
    Loss 751.11632262095	
    Loss 753.03213864009	
    Loss 750.21116967984	
    Loss 752.52573203054	
    Loss 746.61223840618	
    Loss 755.09045885554	
    Loss 754.37761006439	
    Loss 760.75656549465	
    Loss 757.24274514968	
    Loss 749.41181206481	
    Loss 747.51700609045	
    Loss 750.55686956804	
    Loss 747.57145684137	
    Loss 751.89616207164	
    Loss 755.76209567102	
    Loss 755.18019280163	
    Loss 754.23483338885	
    Loss 752.30525258955	
    Loss 754.69885029586	
    Loss 755.47701973679	
    Loss 755.66838821324	
    Loss 750.38707916942	
    Loss 754.52389402885	
    Loss 751.46475486636	
    Loss 750.61530392248	
    Loss 752.44201531453	
    Loss 749.95572256891	
    Loss 752.50006411419	
    Loss 748.5454753989	
    Loss 752.09208093703	
    Loss 751.89457721932	
    Loss 751.7247275073	
    Loss 753.20063333041	
    Loss 749.67170138631	
    Loss 751.71124594703	
    Loss 751.50972938999	
    Loss 750.22138574351	
    Loss 753.85334485538	
    Loss 750.65407058438	
    Loss 745.53567456179	
    Loss 748.58445977018	
    Loss 753.03215924673	
    Loss 751.97381897752	
    Loss 752.74790973648	
    Loss 753.23757001408	
    Loss 748.34319754775	
    Loss 747.20697625221	
    Loss 743.07889466929	
    Loss 749.67114586089	
    Loss 742.611414268	
    Loss 748.93148903235	
    Loss 752.50143107842	
    Loss 754.84903532068	
    Loss 751.96332749278	
    Loss 752.63312715052	
    Loss 756.98209807022	
    Loss 759.68020068733	
    Loss 754.42873286806	
    Loss 755.28968063059	
    Loss 756.13406028477	
    Loss 750.41484826073	
    Loss 751.84960602534	
    Loss 749.98622324896	
    Loss 749.18716318404	
    Loss 751.93433166815	
    Loss 744.22046936535	
    Loss 747.19775225969	
    Loss 747.46213836202	
    Loss 747.73146014955	
    Loss 746.72513368395	
    Loss 748.01049784064	
    Loss 750.17952829083	
    Loss 749.36378022584	
    Loss 749.3007609381	
    Loss 749.44010723031	
    Loss 749.66047786413	
    Loss 757.57263813713	
    Loss 751.13342329152	
    Loss 752.49025697919	
    Loss 745.50664097913	
    Loss 749.70352827394	
    Loss 759.00978021984	
    Loss 749.0402512318	
    Loss 746.29817620895	
    Loss 749.12778098035	
    Loss 747.6421065005	
    Loss 750.30525147039	
    Loss 753.71416179322	
    Loss 750.63855537893	
    Loss 754.04101305383	
    Loss 753.16740807142	
    Loss 744.95445854556	
    Loss 743.97214523624	
    Loss 743.41990732462	
    Loss 738.01338114074	
    Loss 742.33855147266	
    Loss 744.5428189455	
    Loss 747.0548452708	
    Loss 748.8763171827	
    Loss 747.55826660897	
    Loss 751.01885918519	
    Loss 753.48580937772	
    Loss 757.19702452077	
    Loss 752.97359382344	
    Loss 750.71892788468	
    Loss 752.37368879195	
    Loss 742.73290162871	
    Loss 747.47854621076	
    Loss 745.99869326368	
    Loss 743.63028573797	
    Loss 747.20268831925	
    Loss 750.939809601	
    Loss 759.43645649563	
    Loss 753.51666925379	
    Loss 755.65563155968	
    Loss 756.0913609165	
    Loss 752.39597374135	
    Loss 753.55340169923	
    Loss 754.53246566789	
    Loss 753.96325565867	
    Loss 754.62986245914	
    Loss 750.51683567241	
    Loss 748.93679014358	
    Loss 747.75001610386	
    Loss 748.75834164885	
    Loss 754.05766369695	
    Loss 750.94181826889	
    Loss 747.65621773158	
    Loss 746.80437359466	
    Loss 744.82964002422	
    Loss 741.33506427524	
    Loss 748.19813997659	
    Loss 746.80366580867	
    Loss 748.05620309652	
    Loss 748.61468266395	
    Loss 752.47384191019	
    Loss 753.14924156565	
    Loss 749.46320804969	
    Loss 753.47570776523	
    Loss 750.63642674548	
Epoch 5	
 124311
      0
      0
      0
    752
      0
      0
     36
   6709
[torch.DoubleTensor of size 9]

Validation accuracy:	0.10006979849478	
Grad norm	6.4671854055805	
    Loss 742.82580717363	
    Loss 745.57155073824	
    Loss 754.26670041738	
    Loss 751.11632262094	
    Loss 753.03213864009	
    Loss 750.21116967984	
    Loss 752.52573203054	
    Loss 746.61223840618	
    Loss 755.09045885554	
    Loss 754.37761006439	
    Loss 760.75656549465	
    Loss 757.24274514968	
    Loss 749.41181206481	
    Loss 747.51700609045	
    Loss 750.55686956804	
    Loss 747.57145684137	
    Loss 751.89616207164	
    Loss 755.76209567102	
    Loss 755.18019280163	
    Loss 754.23483338885	
    Loss 752.30525258955	
    Loss 754.69885029586	
    Loss 755.47701973679	
    Loss 755.66838821324	
    Loss 750.38707916942	
    Loss 754.52389402885	
    Loss 751.46475486636	
    Loss 750.61530392248	
    Loss 752.44201531453	
    Loss 749.95572256891	
    Loss 752.50006411419	
    Loss 748.5454753989	
    Loss 752.09208093703	
    Loss 751.89457721932	
    Loss 751.7247275073	
    Loss 753.20063333041	
    Loss 749.67170138631	
    Loss 751.71124594703	
    Loss 751.50972938999	
    Loss 750.22138574351	
    Loss 753.85334485538	
    Loss 750.65407058438	
    Loss 745.53567456179	
    Loss 748.58445977018	
    Loss 753.03215924673	
    Loss 751.97381897752	
    Loss 752.74790973648	
    Loss 753.23757001408	
    Loss 748.34319754775	
    Loss 747.20697625221	
    Loss 743.07889466929	
    Loss 749.67114586089	
    Loss 742.611414268	
    Loss 748.93148903235	
    Loss 752.50143107842	
    Loss 754.84903532068	
    Loss 751.96332749277	
    Loss 752.63312715052	
    Loss 756.98209807022	
