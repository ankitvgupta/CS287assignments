[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	1	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 4602
 3932
  984
 1647
 3552
 2324
 1486
 1674
 5923
 3731
 3335
 8131
 4458
 1846
 2427
 2850
 3489
 2639
 3259
 3003
 2104
 1682
 1580
 3461
 1980
 3300
 1345
 2714
 2190
 2670
  878
 1991
 4427
 4089
 5303
 1601
 1929
 3429
 1709
  649
 4599
 1573
 5516
 2277
 3520
[torch.DoubleTensor of size 45]

Validation accuracy:	0.051666059723234	
Grad norm	0	
    Loss 11392211.603619	
    Loss 9900139.6664772	
    Loss 8603525.9409601	
    Loss 7476759.5788738	
    Loss 6497560.7642287	
    Loss 5646617.6805288	
    Loss 4907137.0702433	
    Loss 4264517.1687813	
    Loss 3706062.1346992	
    Loss 3220740.9711381	
    Loss 2798976.7209422	
    Loss 2432453.2161577	
    Loss 2113934.6509974	
    Loss 1837133.7635291	
    Loss 1596583.3232648	
    Loss 1387535.9181359	
    Loss 1205861.2376038	
    Loss 1047983.2031173	
    Loss 910781.06198717	
    Loss 791551.67031692	
    Loss 687936.90440585	
    Loss 597890.23755377	
    Loss 519636.59222666	
    Loss 451628.49264406	
    Loss 392529.37740732	
    Loss 341169.70789765	
    Loss 296535.38819857	
    Loss 257747.78316238	
    Loss 224040.49686534	
    Loss 194746.76120321	
    Loss 169289.60991958	
    Loss 147165.23005205	
    Loss 127940.3747491	
    Loss 111232.1911576	
    Loss 96711.030477572	
    Loss 84091.690634822	
    Loss 73125.118825069	
    Loss 63594.052799378	
    Loss 55312.294272083	
    Loss 48115.329470691	
    Loss 41860.192778107	
    Loss 36423.193470881	
    Loss 31699.920707142	
    Loss 27593.624933968	
    Loss 24025.83008746	
    Loss 20926.808373354	
    Loss 18232.417861017	
    Loss 15889.623249726	
    Loss 13854.71770433	
    Loss 12086.21743477	
    Loss 10549.710663475	
    Loss 9214.5586181862	
    Loss 8053.9882772572	
    Loss 7045.9593968834	
    Loss 6169.7661836746	
    Loss 5406.7843825354	
    Loss 4744.5674504285	
    Loss 4171.0258032338	
    Loss 3671.4893726178	
    Loss 3236.4678148959	
    Loss 2859.6361472946	
    Loss 2530.636009124	
    Loss 2245.5334580405	
    Loss 1996.9531081211	
    Loss 1781.8373387817	
    Loss 1593.7120090332	
    Loss 1430.5047345339	
    Loss 1289.3444298934	
    Loss 1166.6580984225	
    Loss 1059.5603895476	
    Loss 966.79499141949	
    Loss 886.33621171825	
Epoch 2	
 99740
     0
     0
     0
     0
     0
     0
     0
 32068
[torch.DoubleTensor of size 9]

Validation accuracy:	0.077711519786356	
Grad norm	22.651809838991	
    Loss 863.362590193	
    Loss 796.7676360949	
    Loss 739.02188245699	
    Loss 688.22220597981	
    Loss 644.36578035571	
    Loss 606.61417755855	
    Loss 572.60970879175	
    Loss 542.40896807239	
    Loss 518.1824801611	
    Loss 497.74311128789	
    Loss 477.75378079645	
    Loss 461.95941848019	
    Loss 447.3629175636	
    Loss 433.72784171395	
    Loss 422.97661052075	
    Loss 413.79429611379	
    Loss 406.03217280614	
    Loss 398.70562660112	
    Loss 391.65457867325	
    Loss 387.89379893227	
    Loss 382.6103202266	
    Loss 379.12992690008	
    Loss 376.06941385869	
    Loss 372.71391070341	
    Loss 370.11744320793	
    Loss 366.66176401796	
    Loss 363.11513565359	
    Loss 362.89051970797	
    Loss 362.70776136786	
    Loss 361.00280547395	
    Loss 359.78268939241	
    Loss 358.17614400869	
    Loss 358.59202755019	
    Loss 357.50498250628	
    Loss 356.51995318819	
    Loss 355.18544897784	
    Loss 354.5007865164	
    Loss 353.51637346808	
    Loss 353.13036835467	
    Loss 353.64194953486	
    Loss 353.48761117038	
    Loss 352.45550124313	
    Loss 352.94732742772	
    Loss 351.72649347935	
    Loss 351.61207485532	
    Loss 353.09526265213	
    Loss 352.87905091863	
    Loss 351.36548913789	
    Loss 351.32740532432	
    Loss 351.02580902353	
    Loss 351.21935124817	
    Loss 351.31254236823	
    Loss 351.46957182817	
    Loss 352.18605938153	
    Loss 352.63223936399	
    Loss 351.2390957777	
    Loss 350.9180812712	
    Loss 352.83847405411	
    Loss 353.30388958967	
    Loss 352.75282617069	
    Loss 353.54838995539	
    Loss 352.75662334061	
    Loss 352.85541053466	
    Loss 352.15249980479	
    Loss 352.47196532395	
    Loss 351.57540628587	
    Loss 351.04385369712	
    Loss 351.32980857626	
    Loss 351.46073443438	
    Loss 351.21300784668	
    Loss 351.23859927279	
    Loss 351.40379249189	
Epoch 3	
 99798
     0
     0
     0
     0
     0
     0
     0
 32010
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076262442340374	
Grad norm	6.3744669486624	
    Loss 350.51229656242	
    Loss 351.1029359014	
    Loss 351.74647704016	
    Loss 351.62896893919	
    Loss 351.86175242007	
    Loss 352.43045476365	
    Loss 351.72054036194	
    Loss 350.43480914011	
    Loss 351.31587684819	
    Loss 352.74138833096	
    Loss 351.75333784594	
    Loss 352.46067536575	
    Loss 352.19545821932	
    Loss 351.02850948137	
    Loss 351.10527182368	
    Loss 351.33992630419	
    Loss 351.77849660748	
    Loss 351.56561912657	
    Loss 350.69014399092	
    Loss 352.29146007268	
    Loss 351.66268164504	
    Loss 352.23646140794	
    Loss 352.70012515864	
    Loss 352.41482889808	
    Loss 352.48080913552	
    Loss 351.33579308205	
    Loss 349.79394137595	
    Loss 351.31752013552	
    Loss 352.65424718336	
    Loss 352.26690521723	
    Loss 352.18938253599	
    Loss 351.57689320632	
    Loss 352.85642587479	
    Loss 352.51691379808	
    Loss 352.1862626989	
    Loss 351.41908870472	
    Loss 351.22965949312	
    Loss 350.67654995559	
    Loss 350.66067697816	
    Loss 351.49717735828	
    Loss 351.6258545115	
    Loss 350.84001921645	
    Loss 351.54439982397	
    Loss 350.50830906605	
    Loss 350.55468041946	
    Loss 352.17774896834	
    Loss 352.08182140812	
    Loss 350.6727916833	
    Loss 350.72626550361	
    Loss 350.50334591098	
    Loss 350.76500899109	
    Loss 350.91641456765	
    Loss 351.12577317601	
    Loss 351.88792335345	
    Loss 352.37380621801	
    Loss 351.01347295715	
    Loss 350.72131133043	
    Loss 352.66806729943	
    Loss 353.15585369642	
    Loss 352.6238270882	
    Loss 353.43642205488	
    Loss 352.65952725518	
    Loss 352.77097665254	
    Loss 352.07932974694	
    Loss 352.40859405284	
    Loss 351.52061156102	
    Loss 350.99635272053	
    Loss 351.2890616816	
    Loss 351.42527456419	
    Loss 351.18277800277	
    Loss 351.21250659033	
    Loss 351.38122259749	
Epoch 4	
 99798
     0
     0
     0
     0
     0
     0
     0
 32010
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076285202719107	
Grad norm	6.3118952322436	
    Loss 350.49083544048	
    Loss 351.08447679285	
    Loss 351.73063792658	
    Loss 351.6150115802	
    Loss 351.849640104	
    Loss 352.42006958266	
    Loss 351.71156228256	
    Loss 350.42694417105	
    Loss 351.30885347089	
    Loss 352.73539082515	
    Loss 351.74820561081	
    Loss 352.45626707575	
    Loss 352.19157843578	
    Loss 351.02517550164	
    Loss 351.1023908637	
    Loss 351.33746306499	
    Loss 351.77649495113	
    Loss 351.56393416025	
    Loss 350.688703266	
    Loss 352.29019705772	
    Loss 351.66154815998	
    Loss 352.23548367119	
    Loss 352.69929426179	
    Loss 352.41416405419	
    Loss 352.48025673064	
    Loss 351.33531749402	
    Loss 349.79351417056	
    Loss 351.31717023086	
    Loss 352.65396916392	
    Loss 352.26668135567	
    Loss 352.18917807299	
    Loss 351.57670488901	
    Loss 352.85625602262	
    Loss 352.51674384625	
    Loss 352.1861236301	
    Loss 351.41896891829	
    Loss 351.22956884228	
    Loss 350.6764897582	
    Loss 350.66061341704	
    Loss 351.49713037366	
    Loss 351.62582624201	
    Loss 350.84000908256	
    Loss 351.54439745229	
    Loss 350.50831348075	
    Loss 350.55469040795	
    Loss 352.1777662213	
    Loss 352.08183776521	
    Loss 350.67280775002	
    Loss 350.72628516052	
    Loss 350.50336441803	
    Loss 350.76502374893	
    Loss 350.91642070543	
    Loss 351.12578116966	
    Loss 351.8879343586	
    Loss 352.37382070728	
    Loss 351.01347869231	
    Loss 350.72131212534	
    Loss 352.6680713236	
    Loss 353.15585756066	
    Loss 352.62382798975	
    Loss 353.43642428146	
    Loss 352.65953040135	
    Loss 352.77097857776	
    Loss 352.07933269203	
    Loss 352.40859773801	
    Loss 351.52061630816	
    Loss 350.99635760787	
    Loss 351.28906907949	
    Loss 351.42528097081	
    Loss 351.18278716739	
    Loss 351.21251556485	
    Loss 351.38123101865	
Epoch 5	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118449546818	
    Loss 350.49084467621	
    Loss 351.08448604525	
    Loss 351.73064730443	
    Loss 351.61501846281	
    Loss 351.84964600519	
    Loss 352.42007570848	
    Loss 351.71156781789	
    Loss 350.4269485662	
    Loss 351.30885610157	
    Loss 352.73539387063	
    Loss 351.74820872608	
    Loss 352.45627024908	
    Loss 352.1915808725	
    Loss 351.02517782368	
    Loss 351.10239307609	
    Loss 351.33746523976	
    Loss 351.77649771302	
    Loss 351.56393689453	
    Loss 350.68870583738	
    Loss 352.29019924486	
    Loss 351.66154987253	
    Loss 352.23548518175	
    Loss 352.69929568355	
    Loss 352.41416563946	
    Loss 352.48025826255	
    Loss 351.33531883138	
    Loss 349.79351524984	
    Loss 351.3171712938	
    Loss 352.65397024654	
    Loss 352.26668244767	
    Loss 352.18917895241	
    Loss 351.57670554988	
    Loss 352.85625654178	
    Loss 352.51674415799	
    Loss 352.18612396763	
    Loss 351.41896922784	
    Loss 351.22956919777	
    Loss 350.67649018543	
    Loss 350.66061371579	
    Loss 351.49713067947	
    Loss 351.6258265866	
    Loss 350.84000947158	
    Loss 351.54439783184	
    Loss 350.5083138522	
    Loss 350.55469076076	
    Loss 352.17776658056	
    Loss 352.08183808962	
    Loss 350.67280804722	
    Loss 350.7262854553	
    Loss 350.50336469107	
    Loss 350.7650239805	
    Loss 350.91642086944	
    Loss 351.12578132736	
    Loss 351.8879345212	
    Loss 352.37382088336	
    Loss 351.0134787996	
    Loss 350.72131219237	
    Loss 352.66807140028	
    Loss 353.15585762965	
    Loss 352.62382803269	
    Loss 353.43642433135	
    Loss 352.65953045173	
    Loss 352.77097861406	
    Loss 352.07933273143	
    Loss 352.40859777823	
    Loss 351.52061635235	
    Loss 350.99635765131	
    Loss 351.28906913608	
    Loss 351.42528102136	
    Loss 351.18278723393	
    Loss 351.2125156287	
    Loss 351.38123107793	
Epoch 6	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448747789	
    Loss 350.49084474058	
    Loss 351.08448610899	
    Loss 351.73064736858	
    Loss 351.61501851008	
    Loss 351.84964604504	
    Loss 352.42007574995	
    Loss 351.71156785476	
    Loss 350.42694859552	
    Loss 351.30885611941	
    Loss 352.73539389144	
    Loss 351.74820874698	
    Loss 352.45627027079	
    Loss 352.19158088927	
    Loss 351.02517783936	
    Loss 351.1023930913	
    Loss 351.33746525445	
    Loss 351.7764977313	
    Loss 351.56393691251	
    Loss 350.6887058545	
    Loss 352.2901992596	
    Loss 351.66154988432	
    Loss 352.23548519199	
    Loss 352.69929569305	
    Loss 352.4141656499	
    Loss 352.48025827259	
    Loss 351.33531884002	
    Loss 349.79351525686	
    Loss 351.31717130064	
    Loss 352.65397025345	
    Loss 352.26668245481	
    Loss 352.18917895813	
    Loss 351.57670555404	
    Loss 352.85625654498	
    Loss 352.5167441599	
    Loss 352.18612396979	
    Loss 351.41896922986	
    Loss 351.22956920009	
    Loss 350.67649018821	
    Loss 350.66061371773	
    Loss 351.49713068142	
    Loss 351.62582658882	
    Loss 350.84000947407	
    Loss 351.54439783427	
    Loss 350.50831385459	
    Loss 350.55469076299	
    Loss 352.17776658283	
    Loss 352.08183809168	
    Loss 350.67280804912	
    Loss 350.72628545717	
    Loss 350.50336469284	
    Loss 350.76502398203	
    Loss 350.91642087054	
    Loss 351.12578132842	
    Loss 351.88793452229	
    Loss 352.37382088453	
    Loss 351.01347880031	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.77097861429	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913642	
    Loss 351.42528102168	
    Loss 351.18278723434	
    Loss 351.2125156291	
    Loss 351.38123107831	
Epoch 7	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742376	
    Loss 350.49084474099	
    Loss 351.08448610939	
    Loss 351.730647369	
    Loss 351.61501851039	
    Loss 351.84964604529	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.1023930914	
    Loss 351.33746525455	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777849	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 8	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525455	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 9	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 10	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 11	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 12	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 13	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 14	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 15	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 16	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 17	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
    Loss 352.19158088938	
    Loss 351.02517783946	
    Loss 351.10239309141	
    Loss 351.33746525454	
    Loss 351.77649773142	
    Loss 351.56393691263	
    Loss 350.68870585462	
    Loss 352.29019925969	
    Loss 351.6615498844	
    Loss 352.23548519206	
    Loss 352.69929569311	
    Loss 352.41416564996	
    Loss 352.48025827266	
    Loss 351.33531884008	
    Loss 349.79351525691	
    Loss 351.31717130068	
    Loss 352.65397025349	
    Loss 352.26668245485	
    Loss 352.18917895817	
    Loss 351.57670555406	
    Loss 352.85625654499	
    Loss 352.51674415991	
    Loss 352.1861239698	
    Loss 351.41896922987	
    Loss 351.22956920011	
    Loss 350.67649018823	
    Loss 350.66061371775	
    Loss 351.49713068144	
    Loss 351.62582658883	
    Loss 350.84000947409	
    Loss 351.54439783428	
    Loss 350.5083138546	
    Loss 350.554690763	
    Loss 352.17776658285	
    Loss 352.08183809169	
    Loss 350.67280804914	
    Loss 350.72628545719	
    Loss 350.50336469285	
    Loss 350.76502398203	
    Loss 350.91642087055	
    Loss 351.12578132843	
    Loss 351.8879345223	
    Loss 352.37382088454	
    Loss 351.01347880032	
    Loss 350.72131219283	
    Loss 352.66807140079	
    Loss 353.15585763009	
    Loss 352.62382803297	
    Loss 353.43642433169	
    Loss 352.65953045206	
    Loss 352.7709786143	
    Loss 352.07933273168	
    Loss 352.40859777848	
    Loss 351.52061635263	
    Loss 350.99635765159	
    Loss 351.28906913643	
    Loss 351.42528102169	
    Loss 351.18278723434	
    Loss 351.21251562911	
    Loss 351.38123107831	
Epoch 18	
 99800
     0
     0
     0
     0
     0
     0
     0
 32008
[torch.DoubleTensor of size 9]

Validation accuracy:	0.076300376304928	
Grad norm	6.3118448742385	
    Loss 350.49084474099	
    Loss 351.0844861094	
    Loss 351.73064736901	
    Loss 351.61501851039	
    Loss 351.8496460453	
    Loss 352.42007575021	
    Loss 351.71156785499	
    Loss 350.4269485957	
    Loss 351.30885611952	
    Loss 352.73539389158	
    Loss 351.74820874711	
    Loss 352.45627027093	
