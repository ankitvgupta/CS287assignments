[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	0.5	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 3513
 2888
  779
 5974
 5348
 3967
  846
 5875
 1074
 1558
 1070
  852
 6528
 1822
 3702
 7121
 2734
 1482
  954
 1435
 3750
 1465
 1056
 2648
 4477
 2773
 1971
 6634
 3234
 3583
 4942
 4258
 3682
 2423
  597
 2181
 3130
 2173
 1072
 2791
 1727
 1522
 2730
 3375
 4092
[torch.DoubleTensor of size 45]

Validation accuracy:	0.020029133284778	
Grad norm	0	
    Loss 22772508.039066	
    Loss 19790705.099401	
    Loss 17199405.081797	
    Loss 14947409.075613	
    Loss 12990292.382691	
    Loss 11289429.871126	
    Loss 9811292.810904	
    Loss 8526694.1613557	
    Loss 7410299.5238402	
    Loss 6440078.3776961	
    Loss 5596897.1020689	
    Loss 4864123.2170756	
    Loss 4227296.7287169	
    Loss 3673857.3447735	
    Loss 3192883.7303801	
    Loss 2774887.2547433	
    Loss 2411622.1046917	
    Loss 2095917.2210143	
    Loss 1821546.9577481	
    Loss 1583108.4720966	
    Loss 1375888.0447049	
    Loss 1195803.9267765	
    Loss 1039293.2215725	
    Loss 903276.66986563	
    Loss 785074.80116181	
    Loss 682347.99180712	
    Loss 593069.12451419	
    Loss 515482.60584653	
    Loss 448055.56822708	
    Loss 389453.99238374	
    Loss 338528.1109474	
    Loss 294267.85058374	
    Loss 255805.89867715	
    Loss 222377.7672744	
    Loss 193325.58372287	
    Loss 168076.00849805	
    Loss 146134.40938836	
    Loss 127065.72100446	
    Loss 110493.1128554	
    Loss 96091.548247691	
    Loss 83575.540756063	
    Loss 72697.460464725	
    Loss 63244.594158888	
    Loss 55027.456002331	
    Loss 47887.4915705	
    Loss 41684.586412317	
    Loss 36292.197329631	
    Loss 31604.255545459	
    Loss 27531.127102247	
    Loss 23990.710492929	
    Loss 20914.657230542	
    Loss 18241.354434092	
    Loss 15917.740723061	
    Loss 13898.917849122	
    Loss 12144.96990369	
    Loss 10618.524093	
    Loss 9292.6687446242	
    Loss 8143.4482205882	
    Loss 7143.0990499356	
    Loss 6271.9454376901	
    Loss 5516.7543770183	
    Loss 4858.8864054169	
    Loss 4287.4726421021	
    Loss 3789.9744119374	
    Loss 3359.0855079907	
    Loss 2982.6789828334	
    Loss 2656.887102705	
    Loss 2374.4198347277	
    Loss 2128.2947524643	
    Loss 1913.9464357086	
    Loss 1728.1207689739	
    Loss 1566.5119736528	
Epoch 2	
 111941
      0
      0
      0
      0
      0
      0
      0
  19867
[torch.DoubleTensor of size 9]

Validation accuracy:	0.07976754066521	
Grad norm	39.654852239572	
    Loss 1521.3806498015	
    Loss 1387.2858072037	
    Loss 1271.2552090442	
    Loss 1169.7346521463	
    Loss 1081.7234758711	
    Loss 1005.8987899069	
    Loss 938.05034320846	
    Loss 878.57475920985	
    Loss 828.91602138421	
    Loss 786.4229306445	
    Loss 747.77821066004	
    Loss 715.69221049851	
    Loss 686.64897920113	
    Loss 660.32570094553	
    Loss 638.58541674513	
    Loss 620.21998399275	
    Loss 604.56863408236	
    Loss 590.00040620937	
    Loss 575.95347867886	
    Loss 566.40047698486	
    Loss 556.61296174439	
    Loss 549.40486060148	
    Loss 542.70029526113	
    Loss 536.33174247422	
    Loss 530.82153412027	
    Loss 525.18685856106	
    Loss 520.0047068173	
    Loss 517.74863242916	
    Loss 516.43054585009	
    Loss 513.32970241649	
    Loss 511.28914498789	
    Loss 508.30468911541	
    Loss 507.5723455546	
    Loss 506.25713416285	
    Loss 504.41702799702	
    Loss 502.12711755319	
    Loss 501.07750871489	
    Loss 499.66416210473	
    Loss 499.09015928289	
    Loss 499.30728893648	
    Loss 499.29991941808	
    Loss 497.83158859454	
    Loss 497.72564952339	
    Loss 495.90130886024	
    Loss 495.5392222926	
    Loss 497.49485311624	
    Loss 497.67692676446	
    Loss 495.82424483436	
    Loss 495.59607767045	
    Loss 494.87166493019	
    Loss 495.28195872484	
    Loss 495.21976260871	
    Loss 495.17534974072	
    Loss 495.5419502745	
    Loss 496.22017889268	
    Loss 494.89863577386	
    Loss 494.46674224626	
    Loss 497.16094760107	
    Loss 497.79521427087	
    Loss 496.73264182079	
    Loss 497.66030839989	
    Loss 496.84397761872	
    Loss 496.58634409627	
    Loss 495.50126891456	
    Loss 495.93796347843	
    Loss 494.40169312636	
    Loss 494.46132418763	
    Loss 495.1447654174	
    Loss 495.23620900379	
    Loss 494.79120668736	
    Loss 494.7727055725	
    Loss 494.65355935624	
Epoch 3	
 112936
      0
      0
      0
      0
      0
      0
      0
  18872
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079274399126001	
Grad norm	6.493577850082	
    Loss 493.71481029135	
    Loss 494.13613338926	
    Loss 495.06423074358	
    Loss 495.14501255088	
    Loss 495.43731875998	
    Loss 496.41255251814	
    Loss 495.25365353385	
    Loss 493.74199498476	
    Loss 494.46949849349	
    Loss 495.78736011608	
    Loss 495.21359889976	
    Loss 496.18277844893	
    Loss 495.88714906036	
    Loss 494.53544400764	
    Loss 494.49564693785	
    Loss 495.00129241775	
    Loss 495.74027681655	
    Loss 495.4327014145	
    Loss 493.78650896086	
    Loss 494.99274299587	
    Loss 494.56342610665	
    Loss 495.47014967391	
    Loss 495.84665228797	
    Loss 495.62345211788	
    Loss 495.42859932877	
    Loss 494.41862662848	
    Loss 493.26480279422	
    Loss 494.50685731424	
    Loss 496.22871357485	
    Loss 495.77782993666	
    Loss 496.02887835557	
    Loss 495.04262333665	
    Loss 496.03875621804	
    Loss 496.23383477338	
    Loss 495.70688206001	
    Loss 494.56134662637	
    Loss 494.50155888709	
    Loss 493.94539163395	
    Loss 494.12441394653	
    Loss 494.99221174845	
    Loss 495.54976577582	
    Loss 494.56866003347	
    Loss 494.88787051451	
    Loss 493.43425077726	
    Loss 493.39367736173	
    Loss 495.62944373274	
    Loss 496.05545582858	
    Loss 494.41212951655	
    Loss 494.36774201675	
    Loss 493.80396082322	
    Loss 494.35468609558	
    Loss 494.41235796434	
    Loss 494.47411609578	
    Loss 494.93202272269	
    Loss 495.68856561884	
    Loss 494.43615664766	
    Loss 494.06452494244	
    Loss 496.81124573586	
    Loss 497.49053071003	
    Loss 496.46798831495	
    Loss 497.43013909439	
    Loss 496.64340937466	
    Loss 496.4123205984	
    Loss 495.35050580382	
    Loss 495.80678822534	
    Loss 494.28768799136	
    Loss 494.36267699159	
    Loss 495.0592154016	
    Loss 495.16285612611	
    Loss 494.7280450915	
    Loss 494.71778692454	
    Loss 494.60584743757	
Epoch 4	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.354810354418	
    Loss 493.66908721392	
    Loss 494.09616851349	
    Loss 495.02961650119	
    Loss 495.11477721557	
    Loss 495.41089238016	
    Loss 496.3898413213	
    Loss 495.23379202706	
    Loss 493.72463391416	
    Loss 494.45445393071	
    Loss 495.77441820396	
    Loss 495.20246981634	
    Loss 496.1730383158	
    Loss 495.87874825576	
    Loss 494.52812805765	
    Loss 494.48924813281	
    Loss 494.99578403833	
    Loss 495.73547817312	
    Loss 495.42859593977	
    Loss 493.78305713828	
    Loss 494.98975466254	
    Loss 494.56088359915	
    Loss 495.46788267775	
    Loss 495.84480765928	
    Loss 495.62191814307	
    Loss 495.42717673444	
    Loss 494.41733430804	
    Loss 493.2636771963	
    Loss 494.50586084168	
    Loss 496.22782875919	
    Loss 495.77709360225	
    Loss 496.02819975584	
    Loss 495.04203547607	
    Loss 496.03819800428	
    Loss 496.23335255215	
    Loss 495.70646788833	
    Loss 494.56100845765	
    Loss 494.50125964634	
    Loss 493.94510869388	
    Loss 494.12419891654	
    Loss 494.99202799001	
    Loss 495.54960609785	
    Loss 494.56849744824	
    Loss 494.88771775925	
    Loss 493.43411215789	
    Loss 493.3935481416	
    Loss 495.6293283422	
    Loss 496.05535418672	
    Loss 494.41202220994	
    Loss 494.36764063493	
    Loss 493.80387185888	
    Loss 494.35461190383	
    Loss 494.41228397913	
    Loss 494.47405444624	
    Loss 494.93196618302	
    Loss 495.68850690526	
    Loss 494.43610205956	
    Loss 494.06447568454	
    Loss 496.81120199372	
    Loss 497.4904884174	
    Loss 496.46795230896	
    Loss 497.43010649625	
    Loss 496.64337786035	
    Loss 496.41229518121	
    Loss 495.35048654544	
    Loss 495.80677063614	
    Loss 494.28767262687	
    Loss 494.36266635361	
    Loss 495.05920738719	
    Loss 495.16285525507	
    Loss 494.72804824497	
    Loss 494.71778944618	
    Loss 494.60584962382	
Epoch 5	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546968312656	
    Loss 493.66908944172	
    Loss 494.09616890809	
    Loss 495.02961759675	
    Loss 495.1147771522	
    Loss 495.41089124215	
    Loss 496.38984202425	
    Loss 495.23379174417	
    Loss 493.72463288176	
    Loss 494.45445343787	
    Loss 495.77441860663	
    Loss 495.20247092492	
    Loss 496.17303886199	
    Loss 495.87874919683	
    Loss 494.52812883555	
    Loss 494.48924854883	
    Loss 494.99578480245	
    Loss 495.73547881148	
    Loss 495.42859687338	
    Loss 493.78305867915	
    Loss 494.98975608804	
    Loss 494.5608851705	
    Loss 495.46788366934	
    Loss 495.8448093463	
    Loss 495.62192006156	
    Loss 495.42717783859	
    Loss 494.41733491734	
    Loss 493.26367771112	
    Loss 494.50586117274	
    Loss 496.22782893083	
    Loss 495.77709396368	
    Loss 496.02819982655	
    Loss 495.04203555327	
    Loss 496.03819777984	
    Loss 496.23335237932	
    Loss 495.70646776937	
    Loss 494.56100847645	
    Loss 494.50125962472	
    Loss 493.9451085345	
    Loss 494.1241989918	
    Loss 494.99202807541	
    Loss 495.54960617308	
    Loss 494.56849736083	
    Loss 494.88771761836	
    Loss 493.43411199438	
    Loss 493.39354794815	
    Loss 495.6293281598	
    Loss 496.05535402216	
    Loss 494.41202194405	
    Loss 494.36764034711	
    Loss 493.80387160498	
    Loss 494.35461169871	
    Loss 494.41228373964	
    Loss 494.47405425356	
    Loss 494.93196599832	
    Loss 495.68850668282	
    Loss 494.43610184127	
    Loss 494.06447548287	
    Loss 496.81120181195	
    Loss 497.49048823482	
    Loss 496.46795215432	
    Loss 497.43010635215	
    Loss 496.64337771566	
    Loss 496.41229506898	
    Loss 495.35048646502	
    Loss 495.80677056185	
    Loss 494.28767256191	
    Loss 494.36266631406	
    Loss 495.05920736113	
    Loss 495.1628552702	
    Loss 494.72804828402	
    Loss 494.71778947866	
    Loss 494.60584965163	
Epoch 6	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966760446	
    Loss 493.66908946929	
    Loss 494.09616892162	
    Loss 495.02961761337	
    Loss 495.1147771599	
    Loss 495.41089124114	
    Loss 496.38984203458	
    Loss 495.23379174688	
    Loss 493.7246328784	
    Loss 494.45445343807	
    Loss 495.7744186121	
    Loss 495.20247093457	
    Loss 496.17303886771	
    Loss 495.87874920501	
    Loss 494.5281288427	
    Loss 494.48924855343	
    Loss 494.99578480935	
    Loss 495.73547881748	
    Loss 495.42859688088	
    Loss 493.78305869032	
    Loss 494.98975609836	
    Loss 494.56088518152	
    Loss 495.46788367646	
    Loss 495.84480935795	
    Loss 495.62192007471	
    Loss 495.42717784644	
    Loss 494.41733492194	
    Loss 493.26367771505	
    Loss 494.50586117543	
    Loss 496.22782893244	
    Loss 495.77709396646	
    Loss 496.0281998274	
    Loss 495.04203555414	
    Loss 496.03819777876	
    Loss 496.23335237853	
    Loss 495.70646776889	
    Loss 494.56100847672	
    Loss 494.5012596247	
    Loss 493.9451085336	
    Loss 494.12419899246	
    Loss 494.99202807614	
    Loss 495.5496061737	
    Loss 494.5684973604	
    Loss 494.88771761761	
    Loss 493.43411199345	
    Loss 493.39354794704	
    Loss 495.62932815876	
    Loss 496.05535402124	
    Loss 494.41202194245	
    Loss 494.36764034533	
    Loss 493.80387160342	
    Loss 494.35461169742	
    Loss 494.41228373813	
    Loss 494.47405425233	
    Loss 494.93196599716	
    Loss 495.6885066814	
    Loss 494.43610183986	
    Loss 494.06447548157	
    Loss 496.81120181077	
    Loss 497.49048823367	
    Loss 496.46795215334	
    Loss 497.43010635122	
    Loss 496.64337771473	
    Loss 496.41229506827	
    Loss 495.35048646451	
    Loss 495.80677056137	
    Loss 494.28767256149	
    Loss 494.36266631381	
    Loss 495.05920736098	
    Loss 495.1628552703	
    Loss 494.72804828428	
    Loss 494.71778947887	
    Loss 494.60584965181	
Epoch 7	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966749944	
    Loss 493.66908946947	
    Loss 494.09616892171	
    Loss 495.02961761347	
    Loss 495.11477715995	
    Loss 495.41089124113	
    Loss 496.38984203464	
    Loss 495.23379174689	
    Loss 493.72463287836	
    Loss 494.45445343807	
    Loss 495.77441861212	
    Loss 495.20247093464	
    Loss 496.17303886774	
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.56088518159	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	
    Loss 496.03819777876	
    Loss 496.23335237853	
    Loss 495.70646776889	
    Loss 494.56100847672	
    Loss 494.5012596247	
    Loss 493.9451085336	
    Loss 494.12419899247	
    Loss 494.99202807614	
    Loss 495.5496061737	
    Loss 494.56849736039	
    Loss 494.88771761761	
    Loss 493.43411199344	
    Loss 493.39354794704	
    Loss 495.62932815876	
    Loss 496.05535402123	
    Loss 494.41202194244	
    Loss 494.36764034531	
    Loss 493.80387160341	
    Loss 494.35461169742	
    Loss 494.41228373811	
    Loss 494.47405425232	
    Loss 494.93196599716	
    Loss 495.68850668139	
    Loss 494.43610183986	
    Loss 494.06447548157	
    Loss 496.81120181077	
    Loss 497.49048823366	
    Loss 496.46795215333	
    Loss 497.43010635121	
    Loss 496.64337771473	
    Loss 496.41229506826	
    Loss 495.35048646451	
    Loss 495.80677056137	
    Loss 494.28767256149	
    Loss 494.36266631381	
    Loss 495.05920736097	
    Loss 495.1628552703	
    Loss 494.72804828428	
    Loss 494.71778947887	
    Loss 494.60584965181	
Epoch 8	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966749924	
    Loss 493.66908946947	
    Loss 494.09616892171	
    Loss 495.02961761348	
    Loss 495.11477715995	
    Loss 495.41089124113	
    Loss 496.38984203464	
    Loss 495.23379174689	
    Loss 493.72463287836	
    Loss 494.45445343807	
    Loss 495.77441861212	
    Loss 495.20247093463	
    Loss 496.17303886774	
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.5608851816	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	
    Loss 496.03819777876	
    Loss 496.23335237853	
    Loss 495.70646776889	
    Loss 494.56100847672	
    Loss 494.5012596247	
    Loss 493.9451085336	
    Loss 494.12419899247	
    Loss 494.99202807614	
    Loss 495.5496061737	
    Loss 494.56849736039	
    Loss 494.88771761761	
    Loss 493.43411199344	
    Loss 493.39354794704	
    Loss 495.62932815876	
    Loss 496.05535402123	
    Loss 494.41202194244	
    Loss 494.36764034531	
    Loss 493.80387160341	
    Loss 494.35461169742	
    Loss 494.41228373811	
    Loss 494.47405425232	
    Loss 494.93196599716	
    Loss 495.68850668139	
    Loss 494.43610183986	
    Loss 494.06447548157	
    Loss 496.81120181077	
    Loss 497.49048823366	
    Loss 496.46795215333	
    Loss 497.43010635121	
    Loss 496.64337771473	
    Loss 496.41229506826	
    Loss 495.35048646451	
    Loss 495.80677056137	
    Loss 494.28767256149	
    Loss 494.36266631381	
    Loss 495.05920736097	
    Loss 495.1628552703	
    Loss 494.72804828428	
    Loss 494.71778947887	
    Loss 494.60584965181	
Epoch 9	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966749923	
    Loss 493.66908946947	
    Loss 494.09616892171	
    Loss 495.02961761348	
    Loss 495.11477715995	
    Loss 495.41089124113	
    Loss 496.38984203464	
    Loss 495.23379174689	
    Loss 493.72463287836	
    Loss 494.45445343807	
    Loss 495.77441861212	
    Loss 495.20247093463	
    Loss 496.17303886774	
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.5608851816	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	
    Loss 496.03819777876	
    Loss 496.23335237853	
    Loss 495.70646776889	
    Loss 494.56100847672	
    Loss 494.5012596247	
    Loss 493.9451085336	
    Loss 494.12419899247	
    Loss 494.99202807614	
    Loss 495.5496061737	
    Loss 494.56849736039	
    Loss 494.88771761761	
    Loss 493.43411199344	
    Loss 493.39354794704	
    Loss 495.62932815876	
    Loss 496.05535402123	
    Loss 494.41202194244	
    Loss 494.36764034531	
    Loss 493.80387160341	
    Loss 494.35461169742	
    Loss 494.41228373811	
    Loss 494.47405425232	
    Loss 494.93196599716	
    Loss 495.68850668139	
    Loss 494.43610183986	
    Loss 494.06447548157	
    Loss 496.81120181077	
    Loss 497.49048823366	
    Loss 496.46795215333	
    Loss 497.43010635121	
    Loss 496.64337771473	
    Loss 496.41229506826	
    Loss 495.35048646451	
    Loss 495.80677056137	
    Loss 494.28767256149	
    Loss 494.36266631381	
    Loss 495.05920736097	
    Loss 495.1628552703	
    Loss 494.72804828428	
    Loss 494.71778947887	
    Loss 494.60584965181	
Epoch 10	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966749923	
    Loss 493.66908946947	
    Loss 494.09616892171	
    Loss 495.02961761348	
    Loss 495.11477715995	
    Loss 495.41089124113	
    Loss 496.38984203464	
    Loss 495.23379174689	
    Loss 493.72463287836	
    Loss 494.45445343807	
    Loss 495.77441861212	
    Loss 495.20247093463	
    Loss 496.17303886774	
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.5608851816	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	
    Loss 496.03819777876	
    Loss 496.23335237853	
    Loss 495.70646776889	
    Loss 494.56100847672	
    Loss 494.5012596247	
    Loss 493.9451085336	
    Loss 494.12419899247	
    Loss 494.99202807614	
    Loss 495.5496061737	
    Loss 494.56849736039	
    Loss 494.88771761761	
    Loss 493.43411199344	
    Loss 493.39354794704	
    Loss 495.62932815876	
    Loss 496.05535402123	
    Loss 494.41202194244	
    Loss 494.36764034531	
    Loss 493.80387160341	
    Loss 494.35461169742	
    Loss 494.41228373811	
    Loss 494.47405425232	
    Loss 494.93196599716	
    Loss 495.68850668139	
    Loss 494.43610183986	
    Loss 494.06447548157	
    Loss 496.81120181077	
    Loss 497.49048823366	
    Loss 496.46795215333	
    Loss 497.43010635121	
    Loss 496.64337771473	
    Loss 496.41229506826	
    Loss 495.35048646451	
    Loss 495.80677056137	
    Loss 494.28767256149	
    Loss 494.36266631381	
    Loss 495.05920736097	
    Loss 495.1628552703	
    Loss 494.72804828428	
    Loss 494.71778947887	
    Loss 494.60584965181	
Epoch 11	
 112937
      0
      0
      0
      0
      0
      0
      0
  18871
[torch.DoubleTensor of size 9]

Validation accuracy:	0.079281985918912	
Grad norm	6.3546966749923	
    Loss 493.66908946947	
    Loss 494.09616892171	
    Loss 495.02961761348	
    Loss 495.11477715995	
    Loss 495.41089124113	
    Loss 496.38984203464	
    Loss 495.23379174689	
    Loss 493.72463287836	
    Loss 494.45445343807	
    Loss 495.77441861212	
    Loss 495.20247093463	
    Loss 496.17303886774	
    Loss 495.87874920507	
    Loss 494.52812884275	
    Loss 494.48924855346	
    Loss 494.99578480939	
    Loss 495.73547881752	
    Loss 495.42859688093	
    Loss 493.78305869038	
    Loss 494.98975609842	
    Loss 494.5608851816	
    Loss 495.46788367651	
    Loss 495.84480935803	
    Loss 495.6219200748	
    Loss 495.42717784649	
    Loss 494.41733492197	
    Loss 493.26367771508	
    Loss 494.50586117545	
    Loss 496.22782893245	
    Loss 495.77709396648	
    Loss 496.0281998274	
    Loss 495.04203555414	
