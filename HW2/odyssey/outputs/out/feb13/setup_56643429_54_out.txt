[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	3	Lambda:	10	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 5684
 1234
 1504
 4085
 2429
 4657
 1400
 4529
 3154
 1365
 1655
 4288
 1445
  649
 3533
 2584
 2781
 2205
 2769
  874
 3655
 1516
 3231
 1258
 1297
 8899
  944
 1379
 1996
 6633
 2267
 2197
 8042
 6856
 2448
  682
 1995
 3501
 2054
  943
 3400
 1649
 2756
 3293
 6093
[torch.DoubleTensor of size 45]

Validation accuracy:	0.020529861616897	
Grad norm	0	
    Loss 22785227.140865	
    Loss 14950036.707307	
    Loss 9809426.6437873	
    Loss 6436557.1497184	
    Loss 4223553.4768868	
    Loss 2771569.9803766	
    Loss 1818908.0605444	
    Loss 1193796.6418758	
    Loss 783677.41531221	
    Loss 514556.99545796	
    Loss 337992.36828392	
    Loss 222130.64130851	
    Loss 146095.26089801	
    Loss 96213.383482265	
    Loss 63491.823931234	
    Loss 42011.955277984	
    Loss 27924.098103344	
    Loss 18686.245277967	
    Loss 12617.055726651	
    Loss 8633.7276094239	
    Loss 6018.9126314718	
    Loss 4309.5471354432	
    Loss 3185.6096284265	
    Loss 2447.2102448867	
    Loss 1955.1502121703	
    Loss 1645.2453511075	
    Loss 1431.4609481498	
    Loss 1293.6450516052	
    Loss 1207.7645807379	
    Loss 1144.6318230908	
    Loss 1109.7355062031	
    Loss 1077.8827501171	
    Loss 1069.4306429845	
    Loss 1057.6243054148	
    Loss 1049.7210791667	
    Loss 1045.943795688	
    Loss 1035.2301924956	
    Loss 1036.865709109	
    Loss 1036.4708212319	
    Loss 1033.8632852884	
    Loss 1040.1180830889	
    Loss 1034.2063499364	
    Loss 1025.927584384	
    Loss 1031.1240638576	
    Loss 1039.2009804044	
    Loss 1033.9950509274	
    Loss 1037.6481418008	
    Loss 1037.1047593281	
    Loss 1028.0317482228	
    Loss 1027.9077177512	
    Loss 1019.0581987177	
    Loss 1034.030627365	
    Loss 1017.3476861853	
    Loss 1032.4508933481	
    Loss 1038.8299980643	
    Loss 1040.8141330718	
    Loss 1035.3408505369	
    Loss 1035.9851172327	
    Loss 1042.0964574935	
    Loss 1044.8871668349	
    Loss 1034.7974492845	
    Loss 1036.2749841639	
    Loss 1038.058095129	
    Loss 1030.2896702991	
    Loss 1035.151721952	
    Loss 1031.8687556755	
    Loss 1030.1159326783	
    Loss 1036.520939606	
    Loss 1023.2348321595	
    Loss 1027.909591117	
    Loss 1027.4844057229	
    Loss 1027.3081249006	
    Loss 1024.9415384028	
    Loss 1028.2065324659	
    Loss 1032.2510263864	
    Loss 1030.1429404892	
    Loss 1033.423141134	
    Loss 1034.1780968515	
    Loss 1034.592542652	
    Loss 1046.5730844084	
    Loss 1033.8668234626	
    Loss 1036.7564691292	
    Loss 1021.1185200043	
    Loss 1032.682257541	
    Loss 1046.9703167281	
    Loss 1031.1715276502	
    Loss 1023.4475985208	
    Loss 1029.6831749752	
    Loss 1029.1851154558	
    Loss 1034.5127851065	
    Loss 1039.1465315728	
    Loss 1031.6712500213	
    Loss 1038.0661354799	
    Loss 1037.8179835425	
    Loss 1021.2486934518	
    Loss 1022.8511793222	
    Loss 1023.7478025181	
    Loss 1015.1894570855	
    Loss 1023.5296632018	
    Loss 1029.2136098583	
    Loss 1031.9705764687	
    Loss 1032.4052231521	
    Loss 1029.238251041	
    Loss 1036.8649262719	
    Loss 1036.7943766851	
    Loss 1044.387703534	
    Loss 1037.0683926315	
    Loss 1032.3541194546	
    Loss 1037.3290833489	
    Loss 1019.7373541706	
    Loss 1030.5755186956	
    Loss 1026.6356885035	
    Loss 1022.4131621359	
    Loss 1029.3131526777	
    Loss 1036.2575174371	
    Loss 1048.2769870726	
    Loss 1035.4452684707	
    Loss 1039.1094843257	
    Loss 1041.3514899923	
    Loss 1035.9851761385	
    Loss 1039.3946954717	
    Loss 1039.3280130876	
    Loss 1038.3795787222	
    Loss 1039.643308536	
    Loss 1030.8598764776	
    Loss 1027.7337930895	
    Loss 1027.3144547116	
    Loss 1031.9678627653	
    Loss 1039.8061471192	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054346	
    Loss 1030.2238501103	
    Loss 1032.7392788079	
    Loss 1033.8194230308	
    Loss 1037.18678543	
    Loss 1037.8949459719	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.294869931	
Epoch 2	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508355	
    Loss 1015.3272968333	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581406	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 3	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581406	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 4	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581406	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826032	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 5	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581407	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.9705764359	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 6	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581407	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 7	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581407	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 8	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581407	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 9	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581406	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826033	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 10	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581406	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
    Loss 1017.3402151886	
    Loss 1032.4457362021	
    Loss 1038.8267765678	
    Loss 1040.8118192104	
    Loss 1035.3391421171	
    Loss 1035.9841621356	
    Loss 1042.0955426113	
    Loss 1044.8867900827	
    Loss 1034.7974141814	
    Loss 1036.2749986524	
    Loss 1038.0582152768	
    Loss 1030.289751983	
    Loss 1035.1517775399	
    Loss 1031.8688299576	
    Loss 1030.1160218426	
    Loss 1036.5210336681	
    Loss 1023.2348717534	
    Loss 1027.9096235203	
    Loss 1027.4844333785	
    Loss 1027.3081528638	
    Loss 1024.9415575484	
    Loss 1028.2065455043	
    Loss 1032.2510335136	
    Loss 1030.1429429019	
    Loss 1033.4231396805	
    Loss 1034.1780943646	
    Loss 1034.5925393497	
    Loss 1046.5730826032	
    Loss 1033.8668241	
    Loss 1036.7564687205	
    Loss 1021.1185202645	
    Loss 1032.682258075	
    Loss 1046.9703170557	
    Loss 1031.1715279212	
    Loss 1023.4475985428	
    Loss 1029.683175337	
    Loss 1029.185115276	
    Loss 1034.5127850933	
    Loss 1039.1465311925	
    Loss 1031.6712498547	
    Loss 1038.0661352547	
    Loss 1037.8179833414	
    Loss 1021.2486933804	
    Loss 1022.8511793387	
    Loss 1023.7478025401	
    Loss 1015.1894570745	
    Loss 1023.529663173	
    Loss 1029.2136098484	
    Loss 1031.970576436	
    Loss 1032.4052231209	
    Loss 1029.2382509948	
    Loss 1036.8649262471	
    Loss 1036.7943766741	
    Loss 1044.3877035268	
    Loss 1037.0683926249	
    Loss 1032.3541194512	
    Loss 1037.3290833455	
    Loss 1019.7373541683	
    Loss 1030.5755186973	
    Loss 1026.6356885031	
    Loss 1022.4131621335	
    Loss 1029.313152676	
    Loss 1036.2575174364	
    Loss 1048.2769870716	
    Loss 1035.4452684702	
    Loss 1039.1094843257	
    Loss 1041.3514899925	
    Loss 1035.985176139	
    Loss 1039.3946954721	
    Loss 1039.3280130877	
    Loss 1038.3795787222	
    Loss 1039.6433085358	
    Loss 1030.8598764776	
    Loss 1027.7337930894	
    Loss 1027.3144547114	
    Loss 1031.9678627652	
    Loss 1039.8061471191	
    Loss 1032.1359776388	
    Loss 1026.4674938362	
    Loss 1026.6290944428	
    Loss 1023.92460491	
    Loss 1019.5363977083	
    Loss 1034.2343054347	
    Loss 1030.2238501103	
    Loss 1032.739278808	
    Loss 1033.8194230309	
    Loss 1037.18678543	
    Loss 1037.894945972	
    Loss 1027.4724595615	
    Loss 1037.6182355779	
    Loss 1030.2948699311	
Epoch 11	
 126647
      0
      0
      0
    335
      0
      0
     16
   4810
[torch.DoubleTensor of size 9]

Validation accuracy:	0.099910475843651	
Grad norm	6.7346240508302	
    Loss 1015.3272968334	
    Loss 1021.5016467684	
    Loss 1039.3975427246	
    Loss 1033.4430095979	
    Loss 1036.4633718573	
    Loss 1030.1632874891	
    Loss 1035.8154706704	
    Loss 1021.8734034312	
    Loss 1041.1576285677	
    Loss 1037.9528870715	
    Loss 1051.2548780061	
    Loss 1041.9173071816	
    Loss 1026.9800976652	
    Loss 1024.9962581407	
    Loss 1033.0881888049	
    Loss 1029.8732175887	
    Loss 1032.8713745674	
    Loss 1040.4904166725	
    Loss 1039.7140433734	
    Loss 1037.3894568152	
    Loss 1033.7239228309	
    Loss 1038.7126976429	
    Loss 1039.1594229818	
    Loss 1038.9531677428	
    Loss 1030.8008229613	
    Loss 1038.6760145798	
    Loss 1033.5092883289	
    Loss 1032.5992830096	
    Loss 1036.5852416635	
    Loss 1032.4299918291	
    Loss 1036.0756557582	
    Loss 1029.5826069632	
    Loss 1037.7859095065	
    Loss 1036.8224274214	
    Loss 1036.0463901592	
    Loss 1036.9295133448	
    Loss 1029.2987317974	
    Loss 1032.9764731343	
    Loss 1033.9210096205	
    Loss 1032.1773006295	
    Loss 1038.9986607111	
    Loss 1033.4786158786	
    Loss 1025.4509785807	
    Loss 1030.806395012	
    Loss 1038.9924665375	
    Loss 1033.8642229913	
    Loss 1037.5617877194	
    Loss 1037.0487322094	
    Loss 1027.9969227902	
    Loss 1027.8846870809	
    Loss 1019.0421208824	
    Loss 1034.0198802241	
