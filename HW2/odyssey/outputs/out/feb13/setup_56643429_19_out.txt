[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	0	Eta:	3	Lambda:	1	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2112
 1411
 3008
 1856
  733
 2640
 6983
 2250
 3223
 2989
 1933
 5508
 2297
 1993
 2040
 1656
 3924
 1122
 3782
 1441
 3607
 2162
 4152
 3241
 2251
 2814
 3772
 1993
 1195
 4272
 5951
 2746
 2248
 3485
 3627
 3363
 3520
 2956
 1390
 3070
  957
 7291
 1640
  900
 6304
[torch.DoubleTensor of size 45]

Validation accuracy:	0.01853453508133	
Grad norm	0	
    Loss 2277734.0181828	
    Loss 2093381.8997629	
    Loss 1924001.0208263	
    Loss 1768345.771845	
    Loss 1625291.7144593	
    Loss 1493810.3706576	
    Loss 1372987.6320105	
    Loss 1261942.5023373	
    Loss 1159879.5045737	
    Loss 1066076.2884718	
    Loss 979856.42408957	
    Loss 900610.32297242	
    Loss 827781.34038686	
    Loss 760854.03701971	
    Loss 699332.49786217	
    Loss 642788.99065929	
    Loss 590816.61455823	
    Loss 543045.90424532	
    Loss 499141.60712351	
    Loss 458790.9208283	
    Loss 421702.8009193	
    Loss 387612.9016373	
    Loss 356280.93254759	
    Loss 327483.809411	
    Loss 301016.69670897	
    Loss 276692.25988298	
    Loss 254331.27386907	
    Loss 233780.88075042	
    Loss 214890.2433571	
    Loss 197527.92511467	
    Loss 181569.9171961	
    Loss 166902.75816879	
    Loss 153423.07915887	
    Loss 141032.99750095	
    Loss 129643.85479311	
    Loss 119175.90086797	
    Loss 109554.29294025	
    Loss 100712.06546246	
    Loss 92584.733294761	
    Loss 85114.41374479	
    Loss 78246.559119145	
    Loss 71933.980270756	
    Loss 66133.379902692	
    Loss 60801.314156131	
    Loss 55900.172091341	
    Loss 51394.897021813	
    Loss 47254.470714399	
    Loss 43448.219352123	
    Loss 39950.771196541	
    Loss 36735.911253362	
    Loss 33780.100095693	
    Loss 31064.204298214	
    Loss 28567.889824437	
    Loss 26273.509952063	
    Loss 24164.411359598	
    Loss 22224.722388579	
    Loss 20442.323028479	
    Loss 18805.200402673	
    Loss 17300.279627822	
    Loss 15916.592612279	
    Loss 14645.189165138	
    Loss 13476.106846186	
    Loss 12401.818758069	
    Loss 11414.370169145	
    Loss 10506.585224706	
    Loss 9672.0408378711	
    Loss 8904.0432564185	
    Loss 8199.3888989034	
    Loss 7551.2890143351	
    Loss 6955.7192836725	
    Loss 6408.3588904042	
    Loss 5905.5807592487	
Epoch 2	
 72396
  1185
     0
  2395
   167
     0
     0
  1437
 49824
  4404
[torch.DoubleTensor of size 10]

Validation accuracy:	0.11932507890265	
Grad norm	30.618970354164	
    Loss 5761.9268349093	
    Loss 5311.8467273882	
    Loss 4898.3961725526	
    Loss 4517.1607359088	
    Loss 4167.4775661964	
    Loss 3845.797659232	
    Loss 3549.8184583724	
    Loss 3277.4306597809	
    Loss 3028.1577210035	
    Loss 2799.213443125	
    Loss 2587.6657449581	
    Loss 2393.826541441	
    Loss 2215.4142040337	
    Loss 2051.0265247789	
    Loss 1900.4382242846	
    Loss 1762.102643391	
    Loss 1635.1874460923	
    Loss 1518.273655906	
    Loss 1410.8570652598	
    Loss 1312.8343487893	
    Loss 1221.4934407625	
    Loss 1137.7391032174	
    Loss 1061.3753341961	
    Loss 990.54159054588	
    Loss 925.78272440363	
    Loss 865.61750168287	
    Loss 809.44936128295	
    Loss 760.25510654848	
    Loss 714.11426599399	
    Loss 671.57268559629	
    Loss 632.00878813889	
    Loss 596.03430683857	
    Loss 563.89537608924	
    Loss 533.19121268961	
    Loss 505.08450331991	
    Loss 479.41236197539	
    Loss 455.32077729822	
    Loss 433.04787608605	
    Loss 413.05175828912	
    Loss 395.01046010582	
    Loss 378.15761693317	
    Loss 362.43062286342	
    Loss 349.23686442664	
    Loss 335.49021618338	
    Loss 323.3831759098	
    Loss 312.82161509114	
    Loss 302.46786909618	
    Loss 292.13428583608	
    Loss 284.02308958446	
    Loss 276.16440487894	
    Loss 268.77930030374	
    Loss 262.41622435236	
    Loss 257.00790094679	
    Loss 251.8185291067	
    Loss 246.66913806102	
    Loss 240.79924824585	
    Loss 236.03206656806	
    Loss 233.02089175255	
    Loss 229.68066961824	
    Loss 226.31448280999	
    Loss 223.3586267317	
    Loss 220.24759091334	
    Loss 217.94792796576	
    Loss 215.4276572252	
    Loss 213.2390369356	
    Loss 210.99253424467	
    Loss 207.99584122178	
    Loss 206.42824848068	
    Loss 204.9630334766	
    Loss 203.5889560592	
    Loss 202.36328629168	
    Loss 201.52467971744	
Epoch 3	
 71728
  1016
     0
  1878
   112
     0
     0
  1531
 51292
  4251
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12150248846807	
Grad norm	7.1612104756744	
    Loss 200.43867815212	
    Loss 199.71080890657	
    Loss 199.26173189942	
    Loss 197.74544872173	
    Loss 197.2320825155	
    Loss 196.58652236974	
    Loss 195.5121766044	
    Loss 194.29934945833	
    Loss 194.16205480561	
    Loss 194.21600913229	
    Loss 193.20636484337	
    Loss 192.98407965572	
    Loss 192.50980315162	
    Loss 191.41739718891	
    Loss 191.14373014928	
    Loss 190.92313535368	
    Loss 191.05381852596	
    Loss 190.89301208125	
    Loss 190.80104261716	
    Loss 191.44068769609	
    Loss 190.72433042766	
    Loss 190.35538133102	
    Loss 190.59616163423	
    Loss 190.13711566862	
    Loss 190.09724192377	
    Loss 189.38480812669	
    Loss 187.88578819064	
    Loss 188.9611756811	
    Loss 189.03814027993	
    Loss 188.97555358248	
    Loss 188.45753405638	
    Loss 188.36544701603	
    Loss 189.18683192442	
    Loss 188.72930830527	
    Loss 188.47911422565	
    Loss 188.41611625094	
    Loss 187.88202944823	
    Loss 187.19506435729	
    Loss 187.04554541084	
    Loss 187.26242940739	
    Loss 187.23924076603	
    Loss 186.99045154121	
    Loss 188.01471976189	
    Loss 187.30462682913	
    Loss 187.18394552509	
    Loss 187.67078102362	
    Loss 187.45249774739	
    Loss 186.42403458329	
    Loss 186.86631220365	
    Loss 186.87387354539	
    Loss 186.73108467334	
    Loss 187.00670956856	
    Loss 187.72222906546	
    Loss 188.15109582578	
    Loss 188.15804466356	
    Loss 187.03051426471	
    Loss 186.6300213278	
    Loss 187.63552354965	
    Loss 187.96717344999	
    Loss 187.98501348355	
    Loss 188.12950711848	
    Loss 187.8751185536	
    Loss 188.20983924015	
    Loss 188.09227823926	
    Loss 188.12266986972	
    Loss 187.91709177337	
    Loss 186.79680900921	
    Loss 186.94678477311	
    Loss 187.07689209676	
    Loss 187.16008636459	
    Loss 187.27600321584	
    Loss 187.66817034469	
Epoch 4	
 71715
  1014
     0
  1872
   110
     0
     0
  1547
 51337
  4213
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12160111677592	
Grad norm	6.185849783029	
    Loss 186.93483352185	
    Loss 187.2895882422	
    Loss 187.83207373846	
    Loss 187.23530816415	
    Loss 187.57142502023	
    Loss 187.71181750431	
    Loss 187.3536708754	
    Loss 186.8050943025	
    Loss 187.27104208049	
    Loss 187.87834062727	
    Loss 187.3799306009	
    Loss 187.6313322584	
    Loss 187.59197920323	
    Loss 186.88804414553	
    Loss 186.98121105787	
    Loss 187.09620289945	
    Loss 187.53871523899	
    Loss 187.66278990038	
    Loss 187.8334378066	
    Loss 188.71572458553	
    Loss 188.21927181979	
    Loss 188.05664638772	
    Loss 188.48431673286	
    Loss 188.19607771454	
    Loss 188.31435120127	
    Loss 187.7463584752	
    Loss 186.3801159985	
    Loss 187.57909975307	
    Loss 187.76984016579	
    Loss 187.81199066249	
    Loss 187.39005053488	
    Loss 187.38557167471	
    Loss 188.28651309402	
    Loss 187.89993672238	
    Loss 187.7176698902	
    Loss 187.71710623577	
    Loss 187.2413436076	
    Loss 186.60516478954	
    Loss 186.5023056834	
    Loss 186.76284114643	
    Loss 186.78172676842	
    Loss 186.57194538135	
    Loss 187.63182452131	
    Loss 186.95292158258	
    Loss 186.8611024481	
    Loss 187.3758087306	
    Loss 187.18235548335	
    Loss 186.17613262369	
    Loss 186.63885996945	
    Loss 186.66546243828	
    Loss 186.54065965682	
    Loss 186.8320159396	
    Loss 187.56294887766	
    Loss 188.00552959495	
    Loss 188.02471432571	
    Loss 186.90856849629	
    Loss 186.51885971309	
    Loss 187.53443363248	
    Loss 187.87436669953	
    Loss 187.90025594738	
    Loss 188.05167479877	
    Loss 187.80406276628	
    Loss 188.14533855533	
    Loss 188.03292155646	
    Loss 188.06860405763	
    Loss 187.86787387116	
    Loss 186.7520648917	
    Loss 186.90581381765	
    Loss 187.0401483683	
    Loss 187.12680634232	
    Loss 187.24598630281	
    Loss 187.64108369269	
Epoch 5	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1554222488175	
    Loss 186.9087212464	
    Loss 187.26521062224	
    Loss 187.80910201498	
    Loss 187.21401349215	
    Loss 187.55185632596	
    Loss 187.69407276983	
    Loss 187.3373292423	
    Loss 186.79035270491	
    Loss 187.25741555866	
    Loss 187.86564297465	
    Loss 187.36823167881	
    Loss 187.62071755989	
    Loss 187.58233293382	
    Loss 186.87875540353	
    Loss 186.97271073152	
    Loss 187.08838958139	
    Loss 187.53165326288	
    Loss 187.6563336867	
    Loss 187.82757310738	
    Loss 188.71047949293	
    Loss 188.21444795563	
    Loss 188.05241061576	
    Loss 188.48046822359	
    Loss 188.19255967987	
    Loss 188.31117498216	
    Loss 187.74346901628	
    Loss 186.37746993521	
    Loss 187.57676838586	
    Loss 187.76779841389	
    Loss 187.81023155916	
    Loss 187.3885370758	
    Loss 187.38423704578	
    Loss 188.28529829944	
    Loss 187.89873662611	
    Loss 187.71661648509	
    Loss 187.71617717627	
    Loss 187.24057258042	
    Loss 186.60441994414	
    Loss 186.50157653199	
    Loss 186.76216284012	
    Loss 186.7811822531	
    Loss 186.57153508252	
    Loss 187.63153194122	
    Loss 186.95265955878	
    Loss 186.86088258485	
    Loss 187.37568865199	
    Loss 187.18229061949	
    Loss 186.17608976606	
    Loss 186.63883875135	
    Loss 186.66547346925	
    Loss 186.54072131699	
    Loss 186.83208903858	
    Loss 187.56307480003	
    Loss 188.00568383697	
    Loss 188.02487751863	
    Loss 186.90874665017	
    Loss 186.5190647111	
    Loss 187.53467174668	
    Loss 187.87458882956	
    Loss 187.90048473942	
    Loss 188.0518867494	
    Loss 187.80428159257	
    Loss 188.14557653644	
    Loss 188.03313625629	
    Loss 188.06882684006	
    Loss 187.86809957372	
    Loss 186.75229387093	
    Loss 186.90603010589	
    Loss 187.0403881764	
    Loss 187.12704852366	
    Loss 187.24623323524	
    Loss 187.64133348304	
Epoch 6	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548066830306	
    Loss 186.90897829157	
    Loss 187.26543021122	
    Loss 187.80927781655	
    Loss 187.21416691393	
    Loss 187.55199749122	
    Loss 187.69421396345	
    Loss 187.33745793582	
    Loss 186.79048559315	
    Loss 187.25753507096	
    Loss 187.86574443073	
    Loss 187.368324119	
    Loss 187.62080948311	
    Loss 187.58242275246	
    Loss 186.87881768897	
    Loss 186.97276959332	
    Loss 187.08844493397	
    Loss 187.53170958891	
    Loss 187.65638706676	
    Loss 187.82762522537	
    Loss 188.71053464664	
    Loss 188.21449887568	
    Loss 188.05246729629	
    Loss 188.48052223236	
    Loss 188.19261050904	
    Loss 188.31122428953	
    Loss 187.74351615751	
    Loss 186.37751348745	
    Loss 187.57681350206	
    Loss 187.76784466838	
    Loss 187.81028006672	
    Loss 187.38858687426	
    Loss 187.38428528124	
    Loss 188.28534305169	
    Loss 187.8987739257	
    Loss 187.7166534032	
    Loss 187.71621283023	
    Loss 187.24060912306	
    Loss 186.60445219251	
    Loss 186.50160419115	
    Loss 186.76218798717	
    Loss 186.78120912715	
    Loss 186.57156392557	
    Loss 187.63156251044	
    Loss 186.95268779846	
    Loss 186.86090955672	
    Loss 187.37571728464	
    Loss 187.18231900934	
    Loss 186.1761165749	
    Loss 186.63886421429	
    Loss 186.66549831778	
    Loss 186.5407464825	
    Loss 186.83211296716	
    Loss 187.56309949178	
    Loss 188.00570830765	
    Loss 188.0249010057	
    Loss 186.90876956335	
    Loss 186.51908763768	
    Loss 187.53469510855	
    Loss 187.87461038557	
    Loss 187.90050568616	
    Loss 188.0519060234	
    Loss 187.80430048366	
    Loss 188.14559558988	
    Loss 188.03315357481	
    Loss 188.06884406449	
    Loss 187.86811633451	
    Loss 186.7523102253	
    Loss 186.90604535154	
    Loss 187.04040405009	
    Loss 187.12706409593	
    Loss 187.24624860582	
    Loss 187.64134866414	
Epoch 7	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548035913719	
    Loss 186.90899371689	
    Loss 187.26544363678	
    Loss 187.80928894256	
    Loss 187.21417675854	
    Loss 187.55200653683	
    Loss 187.69422282359	
    Loss 187.33746604408	
    Loss 186.79049378637	
    Loss 187.25754250217	
    Loss 187.8657508528	
    Loss 187.36832999714	
    Loss 187.62081523597	
    Loss 187.58242830167	
    Loss 186.8788218117	
    Loss 186.97277344885	
    Loss 187.08844858213	
    Loss 187.53171318527	
    Loss 187.65639044751	
    Loss 187.82762846618	
    Loss 188.71053797938	
    Loss 188.21450196121	
    Loss 188.05247061979	
    Loss 188.4805253717	
    Loss 188.19261345474	
    Loss 188.31122711521	
    Loss 187.7435188553	
    Loss 186.37751596801	
    Loss 187.57681603423	
    Loss 187.76784721926	
    Loss 187.81028271207	
    Loss 187.38858956694	
    Loss 187.38428786648	
    Loss 188.28534544449	
    Loss 187.89877594958	
    Loss 187.71665539958	
    Loss 187.71621474179	
    Loss 187.24061105298	
    Loss 186.60445391684	
    Loss 186.50160568695	
    Loss 186.7621893521	
    Loss 186.78121056307	
    Loss 186.57156543983	
    Loss 187.63156409875	
    Loss 186.95268925776	
    Loss 186.86091094741	
    Loss 187.37571874462	
    Loss 187.18232044541	
    Loss 186.17611792661	
    Loss 186.63886549466	
    Loss 186.66549956394	
    Loss 186.54074773317	
    Loss 186.83211415578	
    Loss 187.56310070925	
    Loss 188.00570950907	
    Loss 188.02490215685	
    Loss 186.90877068432	
    Loss 186.51908875371	
    Loss 187.53469624059	
    Loss 187.8746114274	
    Loss 187.90050669685	
    Loss 188.05190695184	
    Loss 187.80430139435	
    Loss 188.14559650517	
    Loss 188.0331544077	
    Loss 188.06884489668	
    Loss 187.86811714106	
    Loss 186.75231100896	
    Loss 186.90604607942	
    Loss 187.04040480447	
    Loss 187.12706483437	
    Loss 187.2462493314	
    Loss 187.64134938032	
Epoch 8	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034885983	
    Loss 186.9089944444	
    Loss 187.26544427115	
    Loss 187.80928946848	
    Loss 187.21417722375	
    Loss 187.55200696369	
    Loss 187.69422324215	
    Loss 187.33746642789	
    Loss 186.79049417619	
    Loss 187.25754285634	
    Loss 187.8657511585	
    Loss 187.36833027737	
    Loss 187.62081551117	
    Loss 187.58242856757	
    Loss 186.87882200889	
    Loss 186.97277363272	
    Loss 187.08844875785	
    Loss 187.53171335749	
    Loss 187.65639060932	
    Loss 187.82762862075	
    Loss 188.71053813854	
    Loss 188.21450210896	
    Loss 188.05247077961	
    Loss 188.48052552244	
    Loss 188.19261359613	
    Loss 188.31122725064	
    Loss 187.7435189851	
    Loss 186.37751608695	
    Loss 187.57681615596	
    Loss 187.76784734147	
    Loss 187.81028283938	
    Loss 187.388589697	
    Loss 187.38428799105	
    Loss 188.28534555975	
    Loss 187.89877604752	
    Loss 187.71665549652	
    Loss 187.71621483437	
    Loss 187.24061114603	
    Loss 186.6044540006	
    Loss 186.50160575987	
    Loss 186.76218941873	
    Loss 186.78121063315	
    Loss 186.57156551347	
    Loss 187.63156417603	
    Loss 186.95268932845	
    Loss 186.8609110148	
    Loss 187.37571881522	
    Loss 187.1823205146	
    Loss 186.17611799165	
    Loss 186.63886555622	
    Loss 186.6654996239	
    Loss 186.54074779301	
    Loss 186.83211421274	
    Loss 187.56310076741	
    Loss 188.00570956638	
    Loss 188.02490221177	
    Loss 186.90877073778	
    Loss 186.51908880685	
    Loss 187.5346962944	
    Loss 187.8746114768	
    Loss 187.90050674477	
    Loss 188.05190699582	
    Loss 187.8043014376	
    Loss 188.14559654855	
    Loss 188.03315444722	
    Loss 188.06884493641	
    Loss 187.86811717946	
    Loss 186.75231104616	
    Loss 186.90604611386	
    Loss 187.04040484009	
    Loss 187.12706486917	
    Loss 187.24624936551	
    Loss 187.64134941402	
Epoch 9	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034834729	
    Loss 186.90899447862	
    Loss 187.26544430101	
    Loss 187.80928949321	
    Loss 187.21417724563	
    Loss 187.55200698373	
    Loss 187.69422326184	
    Loss 187.33746644595	
    Loss 186.79049419464	
    Loss 187.25754287311	
    Loss 187.86575117297	
    Loss 187.36833029065	
    Loss 187.62081552425	
    Loss 187.58242858024	
    Loss 186.87882201825	
    Loss 186.97277364144	
    Loss 187.08844876624	
    Loss 187.53171336567	
    Loss 187.65639061702	
    Loss 187.82762862808	
    Loss 188.71053814611	
    Loss 188.214502116	
    Loss 188.05247078724	
    Loss 188.48052552963	
    Loss 188.19261360288	
    Loss 188.31122725708	
    Loss 187.74351899131	
    Loss 186.37751609262	
    Loss 187.57681616177	
    Loss 187.76784734731	
    Loss 187.81028284549	
    Loss 187.38858970325	
    Loss 187.38428799702	
    Loss 188.28534556529	
    Loss 187.89877605227	
    Loss 187.71665550121	
    Loss 187.71621483884	
    Loss 187.24061115048	
    Loss 186.60445400465	
    Loss 186.50160576342	
    Loss 186.76218942196	
    Loss 186.78121063655	
    Loss 186.57156551705	
    Loss 187.63156417979	
    Loss 186.95268933188	
    Loss 186.86091101807	
    Loss 187.37571881864	
    Loss 187.18232051792	
    Loss 186.17611799478	
    Loss 186.63886555917	
    Loss 186.6654996268	
    Loss 186.54074779589	
    Loss 186.83211421548	
    Loss 187.5631007702	
    Loss 188.00570956911	
    Loss 188.0249022144	
    Loss 186.90877074034	
    Loss 186.51908880939	
    Loss 187.53469629698	
    Loss 187.87461147916	
    Loss 187.90050674706	
    Loss 188.05190699791	
    Loss 187.80430143966	
    Loss 188.14559655061	
    Loss 188.03315444911	
    Loss 188.06884493832	
    Loss 187.86811718129	
    Loss 186.75231104793	
    Loss 186.9060461155	
    Loss 187.04040484178	
    Loss 187.12706487083	
    Loss 187.24624936713	
    Loss 187.64134941562	
Epoch 10	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832116	
    Loss 186.90899448022	
    Loss 187.26544430243	
    Loss 187.80928949438	
    Loss 187.21417724667	
    Loss 187.55200698468	
    Loss 187.69422326276	
    Loss 187.33746644681	
    Loss 186.79049419553	
    Loss 187.25754287391	
    Loss 187.86575117364	
    Loss 187.36833029127	
    Loss 187.62081552487	
    Loss 187.58242858085	
    Loss 186.8788220187	
    Loss 186.97277364186	
    Loss 187.08844876665	
    Loss 187.53171336606	
    Loss 187.65639061739	
    Loss 187.82762862843	
    Loss 188.71053814647	
    Loss 188.21450211633	
    Loss 188.05247078761	
    Loss 188.48052552997	
    Loss 188.1926136032	
    Loss 188.31122725739	
    Loss 187.7435189916	
    Loss 186.37751609288	
    Loss 187.57681616204	
    Loss 187.76784734758	
    Loss 187.81028284578	
    Loss 187.38858970354	
    Loss 187.38428799731	
    Loss 188.28534556555	
    Loss 187.8987760525	
    Loss 187.71665550143	
    Loss 187.71621483905	
    Loss 187.24061115071	
    Loss 186.60445400484	
    Loss 186.5016057636	
    Loss 186.76218942211	
    Loss 186.78121063673	
    Loss 186.57156551723	
    Loss 187.63156417997	
    Loss 186.95268933204	
    Loss 186.86091101823	
    Loss 187.37571881881	
    Loss 187.18232051809	
    Loss 186.17611799493	
    Loss 186.63886555932	
    Loss 186.66549962693	
    Loss 186.54074779603	
    Loss 186.83211421561	
    Loss 187.56310077032	
    Loss 188.00570956924	
    Loss 188.02490221451	
    Loss 186.90877074046	
    Loss 186.51908880952	
    Loss 187.53469629711	
    Loss 187.87461147927	
    Loss 187.90050674717	
    Loss 188.05190699801	
    Loss 187.80430143975	
    Loss 188.14559655071	
    Loss 188.0331544492	
    Loss 188.06884493841	
    Loss 187.86811718138	
    Loss 186.75231104801	
    Loss 186.90604611557	
    Loss 187.04040484186	
    Loss 187.1270648709	
    Loss 187.2462493672	
    Loss 187.6413494157	
Epoch 11	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832035	
    Loss 186.9089944803	
    Loss 187.2654443025	
    Loss 187.80928949443	
    Loss 187.21417724671	
    Loss 187.55200698472	
    Loss 187.69422326281	
    Loss 187.33746644685	
    Loss 186.79049419557	
    Loss 187.25754287395	
    Loss 187.86575117368	
    Loss 187.36833029129	
    Loss 187.6208155249	
    Loss 187.58242858088	
    Loss 186.87882201872	
    Loss 186.97277364189	
    Loss 187.08844876667	
    Loss 187.53171336608	
    Loss 187.65639061742	
    Loss 187.82762862845	
    Loss 188.71053814649	
    Loss 188.21450211635	
    Loss 188.05247078762	
    Loss 188.48052552999	
    Loss 188.19261360322	
    Loss 188.31122725741	
    Loss 187.74351899162	
    Loss 186.37751609289	
    Loss 187.57681616206	
    Loss 187.7678473476	
    Loss 187.8102828458	
    Loss 187.38858970355	
    Loss 187.38428799732	
    Loss 188.28534556556	
    Loss 187.89877605251	
    Loss 187.71665550144	
    Loss 187.71621483906	
    Loss 187.24061115072	
    Loss 186.60445400486	
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555932	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	
    Loss 187.87461147928	
    Loss 187.90050674718	
    Loss 188.05190699801	
    Loss 187.80430143976	
    Loss 188.14559655071	
    Loss 188.03315444921	
    Loss 188.06884493842	
    Loss 187.86811718138	
    Loss 186.75231104802	
    Loss 186.90604611558	
    Loss 187.04040484186	
    Loss 187.12706487091	
    Loss 187.24624936721	
    Loss 187.6413494157	
Epoch 12	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832038	
    Loss 186.9089944803	
    Loss 187.2654443025	
    Loss 187.80928949444	
    Loss 187.21417724672	
    Loss 187.55200698472	
    Loss 187.69422326281	
    Loss 187.33746644685	
    Loss 186.79049419557	
    Loss 187.25754287395	
    Loss 187.86575117368	
    Loss 187.36833029129	
    Loss 187.6208155249	
    Loss 187.58242858088	
    Loss 186.87882201873	
    Loss 186.97277364189	
    Loss 187.08844876667	
    Loss 187.53171336609	
    Loss 187.65639061742	
    Loss 187.82762862845	
    Loss 188.71053814649	
    Loss 188.21450211635	
    Loss 188.05247078762	
    Loss 188.48052552999	
    Loss 188.19261360322	
    Loss 188.31122725741	
    Loss 187.74351899162	
    Loss 186.37751609289	
    Loss 187.57681616206	
    Loss 187.7678473476	
    Loss 187.8102828458	
    Loss 187.38858970356	
    Loss 187.38428799732	
    Loss 188.28534556557	
    Loss 187.89877605251	
    Loss 187.71665550144	
    Loss 187.71621483906	
    Loss 187.24061115072	
    Loss 186.60445400486	
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555933	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	
    Loss 187.87461147928	
    Loss 187.90050674718	
    Loss 188.05190699801	
    Loss 187.80430143976	
    Loss 188.14559655071	
    Loss 188.03315444921	
    Loss 188.06884493842	
    Loss 187.86811718138	
    Loss 186.75231104802	
    Loss 186.90604611558	
    Loss 187.04040484186	
    Loss 187.12706487091	
    Loss 187.24624936721	
    Loss 187.6413494157	
Epoch 13	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832038	
    Loss 186.9089944803	
    Loss 187.2654443025	
    Loss 187.80928949444	
    Loss 187.21417724672	
    Loss 187.55200698472	
    Loss 187.69422326281	
    Loss 187.33746644685	
    Loss 186.79049419557	
    Loss 187.25754287395	
    Loss 187.86575117368	
    Loss 187.36833029129	
    Loss 187.6208155249	
    Loss 187.58242858088	
    Loss 186.87882201873	
    Loss 186.97277364189	
    Loss 187.08844876667	
    Loss 187.53171336609	
    Loss 187.65639061742	
    Loss 187.82762862845	
    Loss 188.71053814649	
    Loss 188.21450211635	
    Loss 188.05247078762	
    Loss 188.48052552999	
    Loss 188.19261360321	
    Loss 188.31122725741	
    Loss 187.74351899162	
    Loss 186.37751609289	
    Loss 187.57681616206	
    Loss 187.7678473476	
    Loss 187.8102828458	
    Loss 187.38858970356	
    Loss 187.38428799732	
    Loss 188.28534556557	
    Loss 187.89877605251	
    Loss 187.71665550144	
    Loss 187.71621483906	
    Loss 187.24061115072	
    Loss 186.60445400486	
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555933	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	
    Loss 187.87461147928	
    Loss 187.90050674718	
    Loss 188.05190699801	
    Loss 187.80430143976	
    Loss 188.14559655071	
    Loss 188.03315444921	
    Loss 188.06884493842	
    Loss 187.86811718138	
    Loss 186.75231104802	
    Loss 186.90604611558	
    Loss 187.04040484186	
    Loss 187.12706487091	
    Loss 187.24624936721	
    Loss 187.6413494157	
Epoch 14	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832038	
    Loss 186.9089944803	
    Loss 187.2654443025	
    Loss 187.80928949444	
    Loss 187.21417724672	
    Loss 187.55200698472	
    Loss 187.69422326281	
    Loss 187.33746644685	
    Loss 186.79049419557	
    Loss 187.25754287395	
    Loss 187.86575117368	
    Loss 187.36833029129	
    Loss 187.6208155249	
    Loss 187.58242858088	
    Loss 186.87882201873	
    Loss 186.97277364189	
    Loss 187.08844876667	
    Loss 187.53171336609	
    Loss 187.65639061742	
    Loss 187.82762862845	
    Loss 188.71053814649	
    Loss 188.21450211635	
    Loss 188.05247078762	
    Loss 188.48052552999	
    Loss 188.19261360321	
    Loss 188.31122725741	
    Loss 187.74351899162	
    Loss 186.37751609289	
    Loss 187.57681616206	
    Loss 187.76784734759	
    Loss 187.8102828458	
    Loss 187.38858970356	
    Loss 187.38428799732	
    Loss 188.28534556557	
    Loss 187.89877605251	
    Loss 187.71665550144	
    Loss 187.71621483906	
    Loss 187.24061115072	
    Loss 186.60445400486	
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555933	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	
    Loss 187.87461147928	
    Loss 187.90050674718	
    Loss 188.05190699801	
    Loss 187.80430143976	
    Loss 188.14559655071	
    Loss 188.03315444921	
    Loss 188.06884493842	
    Loss 187.86811718138	
    Loss 186.75231104802	
    Loss 186.90604611558	
    Loss 187.04040484186	
    Loss 187.12706487091	
    Loss 187.24624936721	
    Loss 187.6413494157	
Epoch 15	
 71705
  1013
     0
  1876
   110
     0
     0
  1548
 51342
  4214
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12162387715465	
Grad norm	6.1548034832038	
    Loss 186.9089944803	
    Loss 187.2654443025	
    Loss 187.80928949444	
    Loss 187.21417724672	
    Loss 187.55200698472	
    Loss 187.69422326281	
    Loss 187.33746644685	
    Loss 186.79049419557	
    Loss 187.25754287395	
    Loss 187.86575117368	
    Loss 187.36833029129	
    Loss 187.6208155249	
    Loss 187.58242858088	
    Loss 186.87882201873	
    Loss 186.97277364189	
    Loss 187.08844876667	
    Loss 187.53171336609	
    Loss 187.65639061742	
    Loss 187.82762862845	
    Loss 188.71053814649	
    Loss 188.21450211635	
    Loss 188.05247078762	
    Loss 188.48052552999	
    Loss 188.19261360321	
    Loss 188.31122725741	
    Loss 187.74351899162	
    Loss 186.37751609289	
    Loss 187.57681616206	
    Loss 187.76784734759	
    Loss 187.8102828458	
    Loss 187.38858970356	
    Loss 187.38428799732	
    Loss 188.28534556557	
    Loss 187.89877605251	
    Loss 187.71665550144	
    Loss 187.71621483906	
    Loss 187.24061115072	
    Loss 186.60445400486	
    Loss 186.5016057636	
    Loss 186.76218942212	
    Loss 186.78121063674	
    Loss 186.57156551724	
    Loss 187.63156417998	
    Loss 186.95268933205	
    Loss 186.86091101824	
    Loss 187.37571881882	
    Loss 187.1823205181	
    Loss 186.17611799494	
    Loss 186.63886555933	
    Loss 186.66549962694	
    Loss 186.54074779604	
    Loss 186.83211421562	
    Loss 187.56310077033	
    Loss 188.00570956924	
    Loss 188.02490221452	
    Loss 186.90877074047	
    Loss 186.51908880952	
    Loss 187.53469629711	
