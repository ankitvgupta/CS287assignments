[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	2	Eta:	3	Lambda:	5	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 2788
 1770
 2368
 1093
 2928
 3198
 1681
 1572
 1491
 3943
 2352
 2595
 2939
 8263
 2081
 1554
 3479
 5126
 2276
 2305
 4583
 4231
 1760
 3230
 4618
 6476
 2227
  904
 1165
  863
 2860
  761
 4106
 4863
 1803
 6747
  874
 4234
 2760
 2643
 4811
 1222
 5246
 1663
 1356
[torch.DoubleTensor of size 45]

Validation accuracy:	0.022047220199077	
Grad norm	0	
    Loss 11383047.689376	
    Loss 7468552.5828225	
    Loss 4900308.5179584	
    Loss 3215308.4803466	
    Loss 2109767.4065355	
    Loss 1384402.0119725	
    Loss 908486.66637986	
    Loss 596225.78066636	
    Loss 391344.70285228	
    Loss 256917.64108099	
    Loss 168711.60084907	
    Loss 110838.55281139	
    Loss 72864.930899226	
    Loss 47947.24093063	
    Loss 31599.851007536	
    Loss 20874.008393114	
    Loss 13838.075511918	
    Loss 9220.5439476506	
    Loss 6188.7415116462	
    Loss 4204.5569327532	
    Loss 2896.8768950999	
    Loss 2040.2487779255	
    Loss 1480.0365986087	
    Loss 1110.5169815327	
    Loss 869.86912372487	
    Loss 708.44350836507	
    Loss 599.64348294846	
    Loss 537.76852969274	
    Loss 494.25955914461	
    Loss 464.55575872082	
    Loss 443.23021896005	
    Loss 429.92434537997	
    Loss 425.21155751546	
    Loss 417.27571634288	
    Loss 412.63752176952	
    Loss 410.37122875458	
    Loss 408.30957420803	
    Loss 406.13079936254	
    Loss 405.92234565762	
    Loss 408.18195101601	
    Loss 406.7874629452	
    Loss 404.33684109188	
    Loss 408.75445942542	
    Loss 405.66636245082	
    Loss 404.66048373645	
    Loss 408.47549727678	
    Loss 407.06403656637	
    Loss 401.92949267321	
    Loss 404.67629453115	
    Loss 405.29540116603	
    Loss 405.02939652062	
    Loss 405.63342952195	
    Loss 407.87881290683	
    Loss 409.61392295047	
    Loss 409.56914282844	
    Loss 405.84873730563	
    Loss 404.16678154637	
    Loss 409.81340268962	
    Loss 409.67171304036	
    Loss 409.06940065122	
    Loss 410.07345568634	
    Loss 407.66239426293	
    Loss 407.64876836844	
    Loss 406.88182150666	
    Loss 408.42605182053	
    Loss 406.59579808723	
    Loss 403.62265512784	
    Loss 405.62305629684	
    Loss 405.80114017418	
    Loss 406.24395929013	
    Loss 404.88139146477	
    Loss 406.23273330594	
Epoch 2	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6799385406643	
    Loss 402.84935056045	
    Loss 405.86353548369	
    Loss 408.16488509143	
    Loss 406.27892971739	
    Loss 407.28756520733	
    Loss 408.58584300488	
    Loss 405.4931911731	
    Loss 403.97554762374	
    Loss 406.53257786026	
    Loss 409.40933244336	
    Loss 406.42181661727	
    Loss 408.19380186314	
    Loss 408.02384353338	
    Loss 404.8893602415	
    Loss 405.5881827383	
    Loss 405.79697979383	
    Loss 407.45384007427	
    Loss 407.47543189405	
    Loss 406.23335257835	
    Loss 409.90503654217	
    Loss 407.16561162846	
    Loss 406.85107121332	
    Loss 408.43876550321	
    Loss 407.41018400398	
    Loss 408.5484626513	
    Loss 405.70081205837	
    Loss 400.94681233549	
    Loss 407.36856732781	
    Loss 408.70230884829	
    Loss 408.44721176544	
    Loss 406.39246376285	
    Loss 405.7873264322	
    Loss 409.38570876286	
    Loss 406.89821554296	
    Loss 405.81264987121	
    Loss 405.8978817682	
    Loss 405.37766134416	
    Loss 404.2049939742	
    Loss 404.66341380841	
    Loss 407.35089541031	
    Loss 406.2381642742	
    Loss 403.97898989028	
    Loss 408.51858816321	
    Loss 405.51358825097	
    Loss 404.56088543512	
    Loss 408.41008674884	
    Loss 407.0220139349	
    Loss 401.90184891478	
    Loss 404.6588343285	
    Loss 405.28458413744	
    Loss 405.02199974646	
    Loss 405.62870454696	
    Loss 407.87564367491	
    Loss 409.61173696441	
    Loss 409.56782693053	
    Loss 405.84779703048	
    Loss 404.16621408274	
    Loss 409.81306423574	
    Loss 409.67154734573	
    Loss 409.06940768911	
    Loss 410.07345882491	
    Loss 407.66236616356	
    Loss 407.64877097417	
    Loss 406.88182351139	
    Loss 408.42607253433	
    Loss 406.595840086	
    Loss 403.62268396996	
    Loss 405.62309465611	
    Loss 405.80117559101	
    Loss 406.24400136708	
    Loss 404.88142597534	
    Loss 406.23276956348	
Epoch 3	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6796838273359	
    Loss 402.84938627989	
    Loss 405.86355529667	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.40933316229	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153282	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 4	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 5	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6796838273349	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 6	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 7	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6796838273349	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 8	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 9	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6796838273349	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 10	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 11	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 12	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 13	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 14	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.6796838273349	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.2049939751	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
    Loss 406.24400136709	
    Loss 404.88142597535	
    Loss 406.23276956349	
Epoch 15	
 79265
   747
     0
   569
     7
     0
     0
   881
 47728
  2611
[torch.DoubleTensor of size 10]

Validation accuracy:	0.12010651857247	
Grad norm	6.679683827335	
    Loss 402.8493862799	
    Loss 405.86355529668	
    Loss 408.16490075909	
    Loss 406.27893898075	
    Loss 407.2875711145	
    Loss 408.58584753397	
    Loss 405.49319382974	
    Loss 403.97554957345	
    Loss 406.5325792437	
    Loss 409.4093331623	
    Loss 406.42181685162	
    Loss 408.19380225023	
    Loss 408.02384389794	
    Loss 404.88936039258	
    Loss 405.58818296119	
    Loss 405.796979902	
    Loss 407.45383998208	
    Loss 407.47543170656	
    Loss 406.23335247073	
    Loss 409.90503636963	
    Loss 407.16561153281	
    Loss 406.85107121788	
    Loss 408.43876555725	
    Loss 407.41018404139	
    Loss 408.54846269565	
    Loss 405.70081207701	
    Loss 400.94681233376	
    Loss 407.36856732359	
    Loss 408.7023088453	
    Loss 408.44721176986	
    Loss 406.39246376102	
    Loss 405.78732643785	
    Loss 409.38570876867	
    Loss 406.89821554802	
    Loss 405.81264986968	
    Loss 405.89788176987	
    Loss 405.37766134612	
    Loss 404.20499397509	
    Loss 404.66341381043	
    Loss 407.35089541067	
    Loss 406.23816427314	
    Loss 403.97898989022	
    Loss 408.51858816276	
    Loss 405.5135882511	
    Loss 404.56088543539	
    Loss 408.41008674899	
    Loss 407.02201393531	
    Loss 401.90184891507	
    Loss 404.65883432886	
    Loss 405.28458413786	
    Loss 405.02199974667	
    Loss 405.62870454713	
    Loss 407.87564367499	
    Loss 409.61173696445	
    Loss 409.56782693059	
    Loss 405.84779703051	
    Loss 404.16621408277	
    Loss 409.81306423576	
    Loss 409.67154734577	
    Loss 409.06940768917	
    Loss 410.07345882494	
    Loss 407.66236616358	
    Loss 407.64877097418	
    Loss 406.8818235114	
    Loss 408.42607253434	
    Loss 406.59584008602	
    Loss 403.62268396997	
    Loss 405.62309465612	
    Loss 405.80117559102	
