[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	10	Lambda:	10	Minibatch size:	64	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 1017
 2693
 1892
 5413
 5425
 1324
 2672
 1883
 2615
 4369
 5890
 4311
 2378
 2623
 4832
 1612
 2449
 1224
 3737
 1038
 1159
 2605
 2095
 1290
 4351
 4432
 1440
 1866
 4912
 2787
 1797
 3409
 4736
 2525
 5273
 1589
 1714
 2287
 1185
 2587
 5749
 2533
 5325
 3054
 1711
[torch.DoubleTensor of size 45]

Validation accuracy:	0.022752791939791	
Grad norm	0	
    Loss 22776119.165508	
    Loss 5572462.8334285	
    Loss 1364524.1351774	
    Loss 335134.11472346	
    Loss 83314.000095087	
    Loss 21693.842950111	
    Loss 6644.4469366744	
    Loss 2902.9583657017	
    Loss 2077.046063417	
    Loss 1827.4387500513	
    Loss 1812.6614854808	
    Loss 1772.0643129018	
    Loss 1709.7923895316	
    Loss 1708.4220570972	
    Loss 1764.8632138193	
    Loss 1741.7990741725	
    Loss 1743.1032244315	
    Loss 1794.5031476678	
    Loss 1762.0114191168	
    Loss 1744.3350720438	
    Loss 1776.9653795949	
    Loss 1771.3975735464	
    Loss 1763.9682444064	
    Loss 1777.2960177405	
    Loss 1742.7365105294	
    Loss 1773.8387255565	
    Loss 1743.5023669683	
    Loss 1777.8986873927	
    Loss 1770.7004558014	
    Loss 1748.7852884702	
    Loss 1789.4813525312	
    Loss 1751.9950186063	
    Loss 1787.6137747994	
    Loss 1780.5615936592	
    Loss 1760.3610424052	
    Loss 1781.0238528304	
    Loss 1755.7513447752	
    Loss 1759.7394345178	
    Loss 1770.5938397809	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897397	
    Loss 1743.2969738839	
    Loss 1811.9922667596	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 2	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 3	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 4	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097586	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 5	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 6	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 7	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097587	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 8	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
    Loss 1789.0181833404	
    Loss 1723.8230372385	
    Loss 1790.096719359	
    Loss 1801.2244018961	
    Loss 1777.5915097586	
    Loss 1762.0868473106	
    Loss 1787.1666534865	
    Loss 1787.4566739322	
    Loss 1779.9880444553	
    Loss 1747.9193091497	
    Loss 1756.2969160152	
    Loss 1779.4142325656	
    Loss 1767.5715591441	
    Loss 1769.3070162084	
    Loss 1773.6097155937	
    Loss 1768.2841611404	
    Loss 1769.4912515774	
    Loss 1746.8491921571	
    Loss 1782.391629935	
    Loss 1763.2504368035	
    Loss 1739.189982537	
    Loss 1738.8841713177	
    Loss 1745.1704255308	
    Loss 1766.6809041426	
    Loss 1747.6336615181	
    Loss 1785.4457947963	
    Loss 1741.9395634597	
    Loss 1767.6989216315	
    Loss 1797.357280134	
    Loss 1748.8540705422	
    Loss 1792.4029465043	
    Loss 1683.7446574743	
    Loss 1763.916675338	
    Loss 1783.5536937608	
    Loss 1747.0187853461	
    Loss 1689.8325016549	
    Loss 1738.9723553803	
    Loss 1780.0231150769	
    Loss 1790.0057035993	
    Loss 1762.2768576639	
    Loss 1751.3881384037	
    Loss 1756.9024571196	
    Loss 1756.8524023186	
    Loss 1697.6159559617	
    Loss 1733.1539945207	
    Loss 1750.1142721072	
    Loss 1707.4938428828	
    Loss 1746.483468279	
    Loss 1778.8149970631	
    Loss 1765.6724073126	
    Loss 1754.0052017105	
    Loss 1744.9393653757	
    Loss 1778.4335426716	
    Loss 1760.1263558997	
    Loss 1801.7557209927	
    Loss 1752.5717789941	
    Loss 1722.6360571293	
    Loss 1785.4440508019	
    Loss 1704.1973492712	
    Loss 1740.6516349891	
    Loss 1754.6612469931	
    Loss 1744.3603656967	
    Loss 1750.0250998919	
    Loss 1756.076055567	
    Loss 1801.6295719047	
    Loss 1736.5155419956	
    Loss 1776.844220017	
    Loss 1772.0559532748	
    Loss 1764.8232384108	
    Loss 1764.6266447813	
    Loss 1778.9460368816	
    Loss 1745.6891892464	
    Loss 1769.2137995154	
    Loss 1745.7202910206	
    Loss 1745.5304387957	
    Loss 1765.1039510538	
    Loss 1795.0518670682	
    Loss 1779.949494713	
    Loss 1754.1740524877	
    Loss 1733.9836685291	
    Loss 1730.7009290635	
    Loss 1719.5697959962	
    Loss 1711.9144510218	
    Loss 1745.9419082357	
    Loss 1768.8812142113	
    Loss 1781.0628458474	
    Loss 1782.2868952295	
    Loss 1780.7008351294	
    Loss 1770.429119493	
    Loss 1712.8572455356	
    Loss 1766.0224812423	
    Loss 1768.9007457139	
Epoch 9	
  7372
  1412
     2
     0
 42787
     0
     0
 14729
 62196
  3308
     2
[torch.DoubleTensor of size 11]

Validation accuracy:	0.088204054382132	
Grad norm	7.2473787928625	
    Loss 1693.8764682592	
    Loss 1755.9113709692	
    Loss 1801.4562675016	
    Loss 1749.0627442169	
    Loss 1749.2037373125	
    Loss 1744.6159668868	
    Loss 1765.4312917484	
    Loss 1710.7191766711	
    Loss 1785.0689073267	
    Loss 1756.2183295714	
    Loss 1795.1482146247	
    Loss 1767.7716375436	
    Loss 1708.7783046464	
    Loss 1708.1274518512	
    Loss 1764.816053975	
    Loss 1741.7894581582	
    Loss 1743.0979610936	
    Loss 1794.5013192609	
    Loss 1762.0103026314	
    Loss 1744.3349916272	
    Loss 1776.9651399714	
    Loss 1771.3974587798	
    Loss 1763.9683089936	
    Loss 1777.2960119846	
    Loss 1742.7365140835	
    Loss 1773.8387287386	
    Loss 1743.5023665683	
    Loss 1777.8986882315	
    Loss 1770.7004567263	
    Loss 1748.7852886005	
    Loss 1789.4813525576	
    Loss 1751.9950186083	
    Loss 1787.6137747942	
    Loss 1780.5615936572	
    Loss 1760.36104241	
    Loss 1781.0238528274	
    Loss 1755.7513447762	
    Loss 1759.7394345186	
    Loss 1770.593839781	
    Loss 1758.9774845604	
    Loss 1789.8165218733	
    Loss 1771.1407857774	
    Loss 1768.3738897398	
    Loss 1743.2969738839	
    Loss 1811.9922667595	
    Loss 1748.5921289085	
    Loss 1776.4994132701	
    Loss 1765.1609713994	
    Loss 1734.3902322636	
    Loss 1735.6753501381	
    Loss 1722.7197276867	
