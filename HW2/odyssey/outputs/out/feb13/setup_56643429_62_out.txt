[?1034hDatafile:	/n/home09/ankitgupta/CS287/CS287assignments/HW2/PTB.hdf5	Classifier:	lr	Alpha:	5	Eta:	10	Lambda:	10	Minibatch size:	128	Num Epochs:	20	
nclasses:	45	nsparsefeatures:	101214	ndensefeatures:	3	
Imported all data	
Began logistic regression	
Got parameters	101214	3	45	
Set up ParallelTable	
Set up model	
Set up criterion	
Got params and grads	
Epoch 1	
 6213
 4456
 2960
 2357
 2538
  955
 2054
 6911
 3313
 1181
 1214
  524
 2364
 2864
 2491
 5887
 4816
 3472
 1755
 2188
 3040
 2971
 1780
  994
  692
 4291
 2797
 2693
 2098
  875
 1665
 1638
 2137
 3794
 1197
 1187
 5238
 3454
 3317
 3996
 3656
 4350
 5568
 6099
 1768
[torch.DoubleTensor of size 45]

Validation accuracy:	0.025195739257101	
Grad norm	0	
    Loss 22778843.671193	
    Loss 1350863.6850493	
    Loss 80814.167752903	
    Loss 5528.4356262732	
    Loss 1078.978564816	
    Loss 814.8867304646	
    Loss 770.8528001108	
    Loss 773.96526117729	
    Loss 776.84583866314	
    Loss 786.36852591775	
    Loss 789.479674564	
    Loss 794.13280717776	
    Loss 785.95982077898	
    Loss 778.36013433792	
    Loss 787.71637060516	
    Loss 784.18146021832	
    Loss 790.98644700687	
    Loss 787.29541598943	
    Loss 783.7687375852	
    Loss 786.25465880652	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 2	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 3	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 4	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 5	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328029	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.0680422183	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 6	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 7	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328029	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.0680422183	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 8	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.0680422183	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 9	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328029	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.0680422183	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
    Loss 783.48554107193	
    Loss 782.87628359631	
    Loss 776.44131918428	
    Loss 792.50011008714	
    Loss 788.60720401489	
    Loss 795.7043705111	
    Loss 788.92542081161	
    Loss 775.33584552997	
    Loss 795.05517768103	
    Loss 772.26863068818	
    Loss 787.35497863576	
    Loss 790.36122422836	
    Loss 781.82098264282	
    Loss 778.78710498095	
    Loss 789.54905525584	
    Loss 791.41930910795	
    Loss 786.1628153998	
    Loss 769.3440710195	
    Loss 780.16090264639	
    Loss 783.34381236222	
    Loss 785.88725201025	
    Loss 763.46177948952	
    Loss 783.72273947248	
Epoch 10	
 93232
  4962
     0
 10702
  3687
     0
     0
  1363
 15505
  2357
[torch.DoubleTensor of size 10]

Validation accuracy:	0.078948167030833	
Grad norm	7.376895368318	
    Loss 756.40916320602	
    Loss 789.00302174451	
    Loss 789.28643328028	
    Loss 784.52553997384	
    Loss 798.09056204498	
    Loss 798.25920949098	
    Loss 769.88200363507	
    Loss 773.90965788101	
    Loss 776.84309556468	
    Loss 786.36846312095	
    Loss 789.47960313372	
    Loss 794.13280751996	
    Loss 785.95981672179	
    Loss 778.36013389669	
    Loss 787.71637046239	
    Loss 784.18146023278	
    Loss 790.98644699867	
    Loss 787.29541598676	
    Loss 783.7687375847	
    Loss 786.25465880645	
    Loss 791.23281031176	
    Loss 777.2628807486	
    Loss 798.10328783201	
    Loss 786.22402710636	
    Loss 791.90880698445	
    Loss 777.44456283846	
    Loss 757.34406297072	
    Loss 793.70416376141	
    Loss 784.06804221829	
    Loss 782.87683080465	
    Loss 773.52428161011	
    Loss 780.55643767768	
    Loss 792.72779749901	
    Loss 784.78520513867	
    Loss 777.30804221826	
    Loss 778.73894628434	
    Loss 776.92222088654	
    Loss 781.65982721087	
    Loss 785.72935153877	
    Loss 792.08507449564	
    Loss 778.20253859856	
    Loss 763.1976322726	
    Loss 791.65626223304	
    Loss 770.21511811552	
    Loss 785.6805173967	
    Loss 792.35251245551	
    Loss 787.34616590392	
    Loss 762.39892329321	
    Loss 794.76346572791	
