[?1034hUsing cuda	
datafile:	/n/home09/ankitgupta/CS287/CS287assignments/finalproject/PRINC_CB513_1.hdf5	classifier:	rnn	b:	128	alpha:	1	sequence_length:	50	embedding_size	200	optimizer:	sgd	epochs:	200	hidden	200	eta:	0.05	rnn_unit1	lstm	rnn_unit2	lstm	dropout	0.5	
Num classes:	10	
Vocab size:	37	
Start class:	1	
Test size	     1
 83350
[torch.LongStorage of size 2]

Using cuda	
 1183318
       1
[torch.LongStorage of size 2]

  128
 9244
[torch.LongStorage of size 2]

  128
 9244
[torch.LongStorage of size 2]

Converted LSTM to CUDA	
Converted crit to CUDA	
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> output]
  (1): nn.LookupTable
  (2): nn.Transpose
  (3): nn.SplitTable
  (4): nn.BiSequencer @ nn.Sequential {
    [input -> (1) -> (2) -> (3) -> output]
    (1): nn.ConcatTable {
      input
        |`-> (1): nn.Sequencer @ nn.FastLSTM(200 -> 200)
        |`-> (2): nn.Sequential {
        |      [input -> (1) -> (2) -> (3) -> output]
        |      (1): nn.ReverseTable
        |      (2): nn.Sequencer @ nn.FastLSTM(200 -> 200)
        |      (3): nn.ReverseTable
        |    }
         ... -> output
    }
    (2): nn.ZipTable
    (3): nn.Sequencer @ nn.Recursor @ nn.JoinTable
  }
  (5): nn.Sequencer @ nn.Recursor @ nn.Dropout(0.5, busy)
  (6): nn.Sequencer @ nn.Recursor @ nn.Linear(400 -> 200)
  (7): nn.Sequencer @ nn.Recursor @ nn.ReLU
  (8): nn.Sequencer @ nn.Recursor @ nn.Dropout(0.5, busy)
  (9): nn.Sequencer @ nn.Recursor @ nn.Linear(200 -> 10)
  (10): nn.Sequencer @ nn.Recursor @ nn.LogSoftMax
}
Input size	9244	
Max train index	23	
Num samples	9244	
Epoch	1	114.02966928482	
Epoch	2	71.525266289711	
Epoch	3	67.947460532188	
Epoch	4	68.282545685768	
Epoch	5	67.636548042297	
Epoch	6	67.004031538963	
Epoch	7	66.569802880287	
Epoch	8	66.775857567787	
Epoch	9	65.86001098156	
Epoch	10	65.462061047554	
Epoch	11	64.011212468147	
Epoch	12	63.285876274109	
Epoch	13	63.489227771759	
Epoch	14	62.995388746262	
Epoch	15	62.945347189903	
Epoch	16	62.63026714325	
Epoch	17	62.024324774742	
Epoch	18	61.670998692513	
Epoch	19	61.64637029171	
Epoch	20	61.565861463547	
Epoch	21	61.144163608551	
Epoch	22	61.268583893776	
Epoch	23	60.669407963753	
Epoch	24	60.910650372505	
Epoch	25	60.645785212517	
Epoch	26	60.700557231903	
